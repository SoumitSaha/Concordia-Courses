{"cells":[{"cell_type":"markdown","metadata":{"id":"AI6a5KiMQWma"},"source":["# **Tutorial on Response Generation with SpeechBrain**\n","\n","\n","This tutorial will guide you through the process of **fine-tuning the pretrained GPT2 model** that is available in the HuggingFace Transformers library for response generation.\n","\n","\n","**What is a dialogue system?**\n","\n","In previous labs, we have implemented machine translation, which is used to read the source language (input) and generate the desired language (output). Similarly, in a dialogue system, we will implement a model to generate a response given a context. This is also known as Natural Language Generation (NLG).\n","\n","<img src=\"https://miro.medium.com/v2/resize:fit:1400/format:webp/1*yef6QgRpT1ktP6BpHelPmA.png\" alt=\"drawing\" width=\"700\" align=\"center\"/>\n","\n","\n","**Transformers for Language Modeling**\n","\n","As we’ve seen in the previous labs, the original transformer model is made up of an encoder and decoder – each is a stack of what we can call transformer blocks. A lot of the subsequent research works try to focus only on either the encoder or decoder, and use just one stack of transformer blocks – stacking them up as high as practically possible and feeding them massive amounts of training text.\n","\n","<img src=\"https://jalammar.github.io/images/gpt2/gpt-2-transformer-xl-bert-3.png\" alt=\"drawing\" width=\"700\" align=\"center\"/>\n","\n","How high can we stack up these blocks? It turns out that’s one of the main distinguishing factors between the different GPT2 model sizes:\n","\n","<img src=\"https://jalammar.github.io/images/gpt2/gpt2-sizes-hyperparameters-3.png\" alt=\"drawing\" width=\"700\" align=\"center\"/>\n","\n","\n","\n","\n","\n","**Architectures of interest for this tutorial**\n","\n","We will consider the smallest pre-trained GPT2 model : GPT-2.\n","\n","GPT-2 is a large transformer-based language model with 1.5 billion parameters, trained on a dataset of 8 million web pages. GPT-2 is trained with a simple objective: **predict the next word, given all of the previous words within some text**. The diversity of the dataset causes this simple goal to contain naturally occurring demonstrations of many tasks across diverse domains. GPT-2 is a direct scale-up of GPT, with more than 10X the parameters and trained on more than 10X the amount of data. Please refer to the official paper to obtain more details: [Language Models are Unsupervised Multitask Learners](https://d4mucfpksywv.cloudfront.net/better-language-models/language-models.pdf).\n","\n","\n","You could find some helpful resources here:\n","\n","*   [The Illustrated GPT-2](https://jalammar.github.io/illustrated-gpt2/)\n","*   [How to build a State-of-the-Art Conversational AI with Transfer Learning](https://medium.com/huggingface/how-to-build-a-state-of-the-art-conversational-ai-with-transfer-learning-2d818ac26313)\n","*   [Fun Article about Fine-Tuning for Superhero Descriptions](https://towardsdatascience.com/unleashing-the-power-of-gpt-how-to-fine-tune-your-model-da35c90766c4#:~:text=By%20fine-tuning%20GPT-3%2C%20creating%20a%20highly%20customized%20and,code%20and%20without%20assuming%20prior%20knowledge%20about%20GPT-3.)\n","\n","*  [GPT vs Bert](https://medium.com/@10shubhamkedar10/gpt-vs-bert-12d108956260)\n","\n","\n","\n","\n","**With this tutorial, you will learn how to:**\n","\n","1. Instantiate a pretrained GPT2.\n","2. Fine-tuning GPT2 on MultiWOZ with SpeechBrain for response generation task.\n","\n","\n","\n","Let's first install all the needed packages:"]},{"cell_type":"code","execution_count":1,"metadata":{"id":"12izOP1EZjfU","executionInfo":{"status":"ok","timestamp":1711987489343,"user_tz":240,"elapsed":152275,"user":{"displayName":"pooneh mousavi","userId":"07980475777627459411"}}},"outputs":[],"source":["%%capture\n","!git clone https://github.com/speechbrain/speechbrain.git\n","%cd speechbrain\n","!pip install -r requirements.txt\n","!pip install sacrebleu\n","!pip install .\n","%cd .."]},{"cell_type":"markdown","metadata":{"id":"rMWLjuaNMItz"},"source":["## **1. Generate Texts  with GPT-2**\n","\n","\n","\n","\n","\n"]},{"cell_type":"code","source":["from transformers import pipeline, set_seed\n","generator = pipeline('text-generation', model='gpt2')\n","set_seed(42)\n","generator(\"Hello, I am Speech Brain,\", max_length=30, num_return_sequences=5)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":498,"referenced_widgets":["20da3d8065b74ede8716220e59cb07f7","e4704b95bb8349acb04b27c4af999c5b","6121c7d4f1a8480a8d2ddef053fca228","6daf60831ce848bca1c4f8b7a6669484","61dbaed269a046828b32eb7059f3cbb9","e0e63e59ba0e4a2ab5bf54d6207b462b","da2fc61e45de4fa5831c625d7594cc07","d72f15aaa40e4d8ba518a9f2f1c11e41","68e9ac63395a4f4b8666f2474b56bec6","adad2ae6407d48f2a8a31050d439d92b","1d9b65821da1420587c38eb0994ae7e4","f841c313b7f24ff0833a7975fbb34fe7","f951c371c47e4ce28eb00f15f312d6cc","0a10985c24ba403f913500896641c3c3","b37d6582c06c404eb06c0d47aea4ee13","a0e4a2f282704cb0ae3fdbd01a199166","76ac4042a5a54b00a9417995c17012a8","8db732949b614047918ab5efa2ab8c8a","450394b1b0254de4bb492bfce2ef07ec","8997d71e6bb243cb8a5bc0a4c78e6de2","d7378c4cc7454ba998aeb7c106d97bcc","7393b6cc95b744aba89c47b1830e9e82","089777b581d44dc5906c1f4ddd3a9d0d","8ec838df7fc149edbca79f37cc6bc3a5","cb4e377006004b2db63c89dbeda500ca","e34e231b08c34ddabb774ea5f32267dc","358ad6ab63d8484f9bfd6cfe34a2b6db","1d59a1c156ca473780bb921491529707","f35017f3c86d4a7f84cb49315ca8e0a6","ded0f928dd094454b4b82614bcb65586","64b756ea2cc5401291124f0d4af04554","9e472fab69644572af50d5057d2a0af5","ecf2a7cf461746fc8089a403a036715c","2f78c0e702d54b3a96a9bdcbdd20422e","621fbaa2beb845b3809f696afbbac794","cb848f587e7b4eba9ae597319a94c25a","541fc23b9e3f496fa10d39487dfa5476","15fa4e99cc644c3db2643ea76431cca2","91d6e20a21eb4257ab000004fe8fb011","154ec8478f214f6d9d68eaf27d5fb9e3","aef6c7f15ac141759cba088b77b0cc93","415554f78eb949d7ab375a351c574ca7","2b5167edbc3b4309817b2500f6ac307d","6e01afb1e0ee4384945867fbdcffb203","7e6aeae67ece43438b026b00584ab333","4c89484d51dc471fbc84ca8818cc3482","80cdfa75fe1f430ba932e4a9752395a8","5f8f791413694498aa7d280ba13cffe7","2263769cbe6f40a5acfa9b630c1fe3ce","d0e4730a60fa4b72aa44e00cc64f4e50","4211317ecaea4a6f9fa4d7e8324c4367","89bf1fefe38d4545a71142e705fbbeef","fc6d954b0cea424683caaf5d681a811c","d76186287aa4425e9b05dc6450d25d02","643550e06e234cbbba976e37e6f86d2f","5f3faae1ba1c4a4b8027cc2d75730f11","e6d6ad2f4abe4b379fee77ddbf2775d4","b8a4d67ed7924b0286645066b9438313","f01a7f79746f4aa4a081518980e9e28b","821a9731ef16419783af46afa3a9efa2","8956d05ab6244100b1921e4538445af7","84bf0c771cbd48abb3a9b84a305f28be","7a0b5f8b635b4778b82f105a71c87575","4d980d77e3a1495f9ec9ea1e7c295325","a4c2bb321a69439ca6668cadf94062ff","462f217c245646868b4f07e4c8789faa","84515f2df6ba4c06ba112e777cf88324","1a3c2347b69f41b39580b280ef76497a","7c7f6799f6e648e8b5aa376914176e5a","91fd048c13164740be0530b1a07cbfdb","02d34838586446279ba5344a7b16a2b7","20a399879f1045adbc7d49e6bf472cb9","c8e6d0a0f8274873ae42a83f0084be44","e976167922ae4343ad1b4b70cd7b2720","c5d00235382b489c9477fdf87d9f5367","33abc0f470ae4fdf9ad04b4d05878ede","de7fe412b3e44f1ba2be319b44582c7f"]},"id":"VVNw9XPPmB2v","outputId":"c1e101f2-b7d9-412f-8f57-26dddea40112","executionInfo":{"status":"ok","timestamp":1711987521968,"user_tz":240,"elapsed":32651,"user":{"displayName":"pooneh mousavi","userId":"07980475777627459411"}}},"execution_count":2,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:88: UserWarning: \n","The secret `HF_TOKEN` does not exist in your Colab secrets.\n","To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n","You will be able to reuse this secret in all of your notebooks.\n","Please note that authentication is recommended but still optional to access public models or datasets.\n","  warnings.warn(\n"]},{"output_type":"display_data","data":{"text/plain":["config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"20da3d8065b74ede8716220e59cb07f7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["model.safetensors:   0%|          | 0.00/548M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f841c313b7f24ff0833a7975fbb34fe7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"089777b581d44dc5906c1f4ddd3a9d0d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2f78c0e702d54b3a96a9bdcbdd20422e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7e6aeae67ece43438b026b00584ab333"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5f3faae1ba1c4a4b8027cc2d75730f11"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"84515f2df6ba4c06ba112e777cf88324"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"execute_result","data":{"text/plain":["[{'generated_text': \"Hello, I am Speech Brain, but what I do in that role is different—I've been here for nearly 12 years, and I've heard\"},\n"," {'generated_text': 'Hello, I am Speech Brain, this is my first public speech. I understand that it is not a formal presentation, but I am talking about something'},\n"," {'generated_text': 'Hello, I am Speech Brain, and I will guide you in your journey!\\n\\nAs we approach the end of our journey, I want you'},\n"," {'generated_text': 'Hello, I am Speech Brain, you know.\"\\n\\n\"I\\'m not!\"\\n\\n\"I don\\'t think so, so you\\'re not'},\n"," {'generated_text': 'Hello, I am Speech Brain, and I am here to teach you an easy and effective way to get rid of Brain from your home. This easy'}]"]},"metadata":{},"execution_count":2}]},{"cell_type":"markdown","metadata":{"id":"EGUWY37XUb9t"},"source":["Here, we can explore the model with:"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FUOO9DAaUbPW","outputId":"bf21f65b-7a07-44a1-e1cb-c34511dc1de4","executionInfo":{"status":"ok","timestamp":1711987521968,"user_tz":240,"elapsed":6,"user":{"displayName":"pooneh mousavi","userId":"07980475777627459411"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["GPT2LMHeadModel(\n","  (transformer): GPT2Model(\n","    (wte): Embedding(50257, 768)\n","    (wpe): Embedding(1024, 768)\n","    (drop): Dropout(p=0.1, inplace=False)\n","    (h): ModuleList(\n","      (0-11): 12 x GPT2Block(\n","        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","        (attn): GPT2Attention(\n","          (c_attn): Conv1D()\n","          (c_proj): Conv1D()\n","          (attn_dropout): Dropout(p=0.1, inplace=False)\n","          (resid_dropout): Dropout(p=0.1, inplace=False)\n","        )\n","        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","        (mlp): GPT2MLP(\n","          (c_fc): Conv1D()\n","          (c_proj): Conv1D()\n","          (act): NewGELUActivation()\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","      )\n","    )\n","    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","  )\n","  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",")\n"]}],"source":["print(generator.model)"]},{"cell_type":"markdown","metadata":{"id":"psdnsUwfsGgf"},"source":["\n","## **2. Pretrain GPT-2 and Fine-tune**\n","Until now, we only saw how to use pre-trained GPT-2 to continue our own sentences.\n","As we have learned in previous labs, the suggested way to use SpeechBrain is to directly plug your pre-trained model into your pipeline to fine-tune it while training our final model.\n","\n","\n","Remember in \"Week 7: Pretrained and fine-tune\" lab, Wav2vec2 is offered as a **lobe** in SpeechBrain. Its implementation can be found in `speechbrain.lobes.models.huggingface_transformers.wav2vec2.py`.  We need to have a similar interface for GPT. Then, GPT can simply be added as a block to your hyper-params file:\n","\n","For GPT-2:\n","```yaml\n","GPT2: !new:speechbrain.lobes.models.huggingface_transformers.gpt.GPT\n","    source: !ref <gpt_hub>\n","    freeze: !ref <freeze_gptmodel>\n","    save_path: !ref <gpt_folder>\n","    max_new_tokens: !ref <max_new_tokens>\n","    num_beams: !ref <num_beams>\n","    top_k: !ref  <top_k>\n","    top_p: !ref <top_p>\n","```\n","\n","- *freeze* enables you to fine-tune (False) or freeze (True) the neural parameters while training your final model.\n","\n","\n","The GPT model is just a neural network that can be applied to your input data and can be jointly trained with the downstream task of interest. GPT interface has been already implemented in Speechbrain. Its implementation can be found in `speechbrain.lobes.models.huggingface_transformers.gpt.py`.\n","\n","```\n","\"\"\"This lobe enables the integration of huggingface pretrained GPT2LMHeadModel model.\n","\n","Transformer from HuggingFace needs to be installed:\n","https://huggingface.co/transformers/installation.html\n","\n","Authors\n"," * Pooneh Mousavi 2023\n"," * Simone Alghisi 2023\n","\"\"\"\n","\n","import logging\n","import torch\n","\n","from speechbrain.lobes.models.huggingface_transformers.huggingface import (\n","    HFTransformersInterface,\n",")\n","\n","\n","logger = logging.getLogger(__name__)\n","\n","\n","class GPT(HFTransformersInterface):\n","    \"\"\"This lobe enables the integration of HuggingFace pretrained GPT model.\n","     Source paper whisper:\n","        https://life-extension.github.io/2020/05/27/GPT%E6%8A%80%E6%9C%AF%E5%88%9D%E6%8E%A2/language-models.pdf\n","    Transformer from HuggingFace needs to be installed:\n","        https://huggingface.co/transformers/installation.html\n","\n","    The model can be finetuned. It will download automatically the model from\n","    HuggingFace or use a local path.\n","\n","    Arguments\n","    ---------\n","    source : str\n","        HuggingFace hub name: e.g \"gpt2\"\n","    save_path : str\n","        Path (dir) of the downloaded model.\n","    freeze : bool (default: False)\n","        If True, the model is frozen. If False, the model will be trained\n","        alongside with the rest of the pipeline.\n","    max_new_tokens : int\n","        Maximum count of new tokens allowed.\n","    min_length : int\n","        Minimum count of input tokens\n","    top_k : int\n","        Top results count to keep\n","    top_p : float\n","        Proportion of top results to keep\n","    num_beams : int\n","        Number of decoder beams\n","    eos_token_id : int\n","        Index of end-of-sentence token.\n","    early_stopping : int\n","        Whether to stop training early.\n","\n","    Example\n","    -------\n","    >>> model_hub = \"gpt2\"\n","    >>> save_path = \"savedir\"\n","    >>> model = GPT(model_hub, save_path)\n","    >>> tokens = torch.tensor([[1, 1]])\n","    >>> tokens_type = torch.tensor([[1, 1]])\n","    >>> attention_mask = torch.tensor([[1, 1]])\n","    >>> outputs = model(tokens, tokens_type, attention_mask)\n","    \"\"\"\n","\n","    def __init__(\n","        self,\n","        source,\n","        save_path,\n","        freeze=False,\n","        max_new_tokens=200,\n","        min_length=1,\n","        top_k=45,\n","        top_p=0.9,\n","        num_beams=8,\n","        eos_token_id=50258,\n","        early_stopping=True,\n","    ) -> None:\n","        super().__init__(\n","            source=source, save_path=save_path, freeze=freeze, with_lm_head=True\n","        )\n","        self.max_new_tokens = max_new_tokens\n","        self.min_length = min_length\n","        self.top_k = top_k\n","        self.top_p = top_p\n","        self.num_beams = num_beams\n","        self.early_stopping = early_stopping\n","        self.eos_token_id = eos_token_id\n","\n","        self.load_tokenizer(source=source, pad_token=None, use_fast=False)\n","\n","        if self.freeze:\n","            logger.warning(\"huggingface_GPT - GPT  is frozen.\")\n","            self.model.train()  # we keep it to train to have dropout and LN computed adequately\n","            for param in self.model.parameters():\n","                param.requires_grad = False\n","\n","    def forward(\n","        self,\n","        input_ids: torch.Tensor,\n","        token_type_ids: torch.Tensor,\n","        attention_mask: torch.Tensor,\n","    ):\n","        \"\"\"Takes an input a history of conversation and returns its corresponding reply.\n","\n","        Arguments\n","        ---------\n","        input_ids : torch.Tensor\n","            A batch of input-id to transform to features.\n","        token_type_ids : torch.Tensor\n","            Token Type(Speaker) for each token in input_ids.\n","        attention_mask : torch.Tensor\n","            A batch of attention_mask.\n","\n","        Returns\n","        -------\n","        output : torch.Tensor\n","            Reply to conversation\n","        \"\"\"\n","        with torch.set_grad_enabled(not self.freeze):\n","            output = self.model.forward(\n","                input_ids,\n","                token_type_ids=token_type_ids,\n","                attention_mask=attention_mask,\n","            )\n","        return output\n","\n","    def generate(\n","        self,\n","        input_ids: torch.Tensor,\n","        token_type_ids,\n","        attention_mask: torch.Tensor,\n","        decoder_type=\"greedy\",\n","    ):\n","        \"\"\"Takes an input a history of conversation and returns its corresponding reply.\n","\n","        Arguments\n","        ---------\n","        input_ids : torch.Tensor\n","            A batch of input-id which are dialogue context tokens\n","        token_type_ids : torch.Tensor\n","        attention_mask : torch.Tensor\n","            A batch of attention_mask.\n","        decoder_type : str\n","            It shows strategy for autoregressive decoding either beam search or greedy.\n","\n","        Returns\n","        -------\n","        hyp : torch.Tensor\n","            Conversation reply.\n","        \"\"\"\n","\n","        with torch.no_grad():\n","            if decoder_type == \"beam\":\n","                # beam decoding based on the input_ids which are dialogue context tokens (here only history)\n","                hyp = self.model.generate(\n","                    input_ids=input_ids,\n","                    token_type_ids=token_type_ids,\n","                    attention_mask=attention_mask,\n","                    do_sample=True,\n","                    max_new_tokens=self.max_new_tokens,\n","                    min_length=self.min_length,\n","                    top_k=self.top_k,\n","                    top_p=self.top_p,\n","                    num_beams=self.num_beams,\n","                    num_return_sequences=1,\n","                    eos_token_id=self.eos_token_id,\n","                    early_stopping=self.early_stopping,\n","                )\n","            else:\n","                # greedy decoding based on the input_ids which are dialogue context tokens (here only history)\n","                hyp = self.model.generate(\n","                    input_ids,\n","                    token_type_ids=token_type_ids,\n","                    max_new_tokens=self.max_new_tokens,\n","                    eos_token_id=self.eos_token_id,\n","                    attention_mask=attention_mask,\n","                )\n","        return hyp\n","```\n","\n","\n","\n","\n","\n","\n"]},{"cell_type":"markdown","source":["The inputs of GPT2 model are input_ids and token_type_ids. Input_ids are a concatenation of all tokenized histories with the <speaker_token> of each sentence added before it. The token_type_ids has the same length as the input_ids and indicates who is the speaker of each token.\n","For example:\n","```\n","history: 'Hi how are you', 'I am fine and you', 'I am good']\n","input_ids : <speaker_1> Hi how are you <speaker_2> I am fine and you? <speaker_1> I am good>\n","token_type_ids : [[<speaker_1>,<speaker_1>,<speaker_1>,<speaker_1>],\n","                  [<speaker_2>,<speaker_2>,<speaker_2>,<speaker_2>,<speaker_2>],\n","                  [<speaker_1> <speaker_1>,<speaker_1>]]\n","\n","```"],"metadata":{"id":"6C3pPu-665xm"}},{"cell_type":"markdown","source":["**Note:** It is just an illustrating example. The real input has the token_id instead of the words."],"metadata":{"id":"VUnDyhuc_DrL"}},{"cell_type":"markdown","source":["## 3 Fine-tuning GPT-2 on Resonse Generation (with MultiWOZ)\n","Now we will discuss how to fine-tune the GPT-2  model for response generation. To achieve this, we will be using a smaller version of the MultiWOZ 2.1 dataset. Multi-Domain Wizard-of-Oz dataset (MultiWOZ), a fully-labeled collection of human-human written conversations spanning over multiple domains and topics. Instead of using all the data, we will set a parameter that identifies the percentage of data to be sampled. For this experiment, we will just sample 1000, 100, 200 of training , valid and test enties.\n"],"metadata":{"id":"tayhoDdV6SNw"}},{"cell_type":"markdown","source":["### **Step 1: Prepare your data**\n","The goal of data preparation is to create the data manifest files.\n","These files tell SpeechBrain where to find the dialogue history and the system reply. They are text files written in the popular CSV and JSON formats.\n","\n","#### **Data manifest files**\n","Let's take a look into how a data manifest file in JSON format looks like:\n","\n","\n","```json\n","{\n","    \"SNG01919.json_1\": {\n","        \"history\": [\n","            \"i need a taxi from the missing sock and i need to get to my destination by 08:30 . can you help ?\"\n","        ],\n","        \"reply\": \"i can help you with that . where are you going ?\",\n","        \"length\": 145\n","    },\n","    \"SNG01919.json_3\": {\n","        \"history\": [\n","            \"i need a taxi from the missing sock and i need to get to my destination by 08:30 . can you help ?\",\n","            \"i can help you with that . where are you going ?\",\n","            \"i am going to el shaddai\"\n","        ],\n","        \"reply\": \"okay your booking is complete . be on the lookout for a white volkswagen\",\n","        \"length\": 128.33333333333334\n","    },\n","    \"SNG01919.json_5\": {\n","        \"history\": [\n","            \"i need a taxi from the missing sock and i need to get to my destination by 08:30 . can you help ?\",\n","            \"i can help you with that . where are you going ?\",\n","            \"i am going to el shaddai\",\n","            \"okay your booking is complete . be on the lookout for a white volkswagen\",\n","            \"i will also need the contact number please .\"\n","        ],\n","        \"reply\": \"their contact number is 07053289961 . do you need any further assistance ?\",\n","        \"length\": 131\n","    }\n","}\n","```\n","As you can see, we have a hierarchical structure in which the first key is a **unique identifier** of the name_of the dialouge+the turn number.\n","\n","You can specify here the entries with the name you prefer. However, there must be a matching between the name of these entries and what the experiment script (e.g, train.py) expects. We will elaborate more on this later.\n","\n","\n","#### **Preparation Script**\n","Every dataset is formatted in a different way. The script that parses your own dataset and creates the JSON or the CSV files is something that you are supposed to write. Most of the time, this is very straightforward.\n","\n","For the MultiWOZ dataset, for instance, we wrote this data preparation script called multiwoz_prepare.py.\n","The function automatically downloads the data. We search for all the dialogues and split them based on the turns. Our goal is to train a model that could produce reasonable system-generated responses. Therefore, as our gold labels, we extract the system turns (replies uttered by the system). It will be our reply field in manifest files. Then, we will extract the history for each reply. The history is a list of sentences prior to that response. Each sentence is uttered either by a system or a user. You could see as we go further in the dialogue, the history become bigger since it contains all previous histories + new one. Even rows in history are uttered by the user and odd rows by the system. For the length, we take an average over the length of all sentences( number of words in each sentence). This field is only used by the data loader to sort data to avoid any unnecessary padded_tokens. It is not the actual length. The actual length of the inputs is varied depending on the tokenizer and token_type that we will use.\n","\n","You can use this script as a good base for your custom preparation on your target dataset. As you can see, we create three separate data manifest files to manage the training, validation, and test phases.\n","\n"],"metadata":{"id":"fUYeE7JcDfJW"}},{"cell_type":"markdown","source":["It is a good practice to make your data as clean as possible before feeding it to the model. To prepare the text data for the model building, we perform text preprocessing. Some of the preprocessing steps are:\n","Removing punctuations like. , ! $( ) * % @\n","Removing URLs\n","Lower/upper casing\n","Mapping abbreviations and short forms to their full forms (e.g. \"it's\" to \"it is\")\n","We will apply these preprocessing steps using multiwoz_prepare.py and mapping.pair files."],"metadata":{"id":"LT_xFZPcUf_T"}},{"cell_type":"code","source":["%%file mapping.pair\n","it's\tit is\n","don't\tdo not\n","doesn't\tdoes not\n","didn't\tdid not\n","you'd\tyou would\n","you're\tyou are\n","you'll\tyou will\n","i'm\ti am\n","they're\tthey are\n","that's\tthat is\n","what's\twhat is\n","couldn't\tcould not\n","i've\ti have\n","we've\twe have\n","can't\tcannot\n","i'd\ti would\n","i'd\ti would\n","aren't\tare not\n","isn't\tis not\n","wasn't\twas not\n","weren't\twere not\n","won't\twill not\n","there's\tthere is\n","there're\tthere are\n",". .\t.\n","restaurants\trestaurant -s\n","hotels\thotel -s\n","laptops\tlaptop -s\n","cheaper\tcheap -er\n","dinners\tdinner -s\n","lunches\tlunch -s\n","breakfasts\tbreakfast -s\n","expensively\texpensive -ly\n","moderately\tmoderate -ly\n","cheaply\tcheap -ly\n","prices\tprice -s\n","places\tplace -s\n","venues\tvenue -s\n","ranges\trange -s\n","meals\tmeal -s\n","locations\tlocation -s\n","areas\tarea -s\n","policies\tpolicy -s\n","children\tchild -s\n","kids\tkid -s\n","kidfriendly\tkid friendly\n","cards\tcard -s\n","upmarket\texpensive\n","inpricey\tcheap\n","inches\tinch -s\n","uses\tuse -s\n","dimensions\tdimension -s\n","driverange\tdrive range\n","includes\tinclude -s\n","computers\tcomputer -s\n","machines\tmachine -s\n","families\tfamily -s\n","ratings\trating -s\n","constraints\tconstraint -s\n","pricerange\tprice range\n","batteryrating\tbattery rating\n","requirements\trequirement -s\n","drives\tdrive -s\n","specifications\tspecification -s\n","weightrange\tweight range\n","harddrive\thard drive\n","batterylife\tbattery life\n","businesses\tbusiness -s\n","hours\thour -s\n","one\t1\n","two\t2\n","three\t3\n","four\t4\n","five\t5\n","six\t6\n","seven\t7\n","eight\t8\n","nine\t9\n","ten\t10\n","eleven\t11\n","twelve\t12\n","anywhere\tany where\n","good bye\tgoodbye\n"],"metadata":{"id":"1_QAROTCW0s3","colab":{"base_uri":"https://localhost:8080/"},"outputId":"9c53239c-381d-4d36-b28d-540faaefe56a","executionInfo":{"status":"ok","timestamp":1711988110086,"user_tz":240,"elapsed":189,"user":{"displayName":"pooneh mousavi","userId":"07980475777627459411"}}},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Writing mapping.pair\n"]}]},{"cell_type":"code","source":["from itertools import product\n","from pathlib import Path\n","from statistics import mean\n","from typing import Any, Dict, List, Optional, Set, Tuple\n","import json\n","import logging\n","import os\n","import random\n","import re\n","import shutil\n","\n","\n","from tqdm import tqdm\n","\n","from speechbrain.utils.data_utils import download_file\n","\n","logger = logging.getLogger(__name__)\n","MULTIWOZ_21_DATASET_URL = (\n","    \"https://github.com/budzianowski/multiwoz/raw/master/data/MultiWOZ_2.1.zip\"\n",")\n","\n","\"\"\"\n","Trade script used for tokenization porposes.\n","\n","The original one can be found at:\n","https://github.com/jasonwu0731/trade-dst/blob/master/create_data.py\n","\"\"\"\n","\n","def prepare_mwoz_21(\n","    output_folder: str,\n","    data_folder: str,\n","    override: bool,\n","    replacements_path: str,\n","    tr_random_dialogues: Optional[int] = None,\n","    dev_random_dialogues: Optional[int] = None,\n","    te_random_dialogues: Optional[int] = None,\n","    seed: int = 42,\n",") -> None:\n","    # set seed\n","    random.seed(seed)\n","\n","    dataset_folder = os.path.join(data_folder, \"MultiWOZ_21\")\n","    if not os.path.isdir(dataset_folder):\n","        download_mwoz_21(data_folder)\n","    else:\n","        logger.info(f\"{dataset_folder} exists, skipping.\")\n","\n","    tr_split, dev_split, te_split = get_splits(dataset_folder)\n","\n","    data_path = os.path.join(dataset_folder, \"data.json\")\n","    build_dialogue_dataset(\n","        data_path,\n","        logger,\n","        tr_split,\n","        \"train.json\",\n","        output_folder,\n","        override,\n","        replacements_path,\n","        tr_random_dialogues,\n","    )\n","\n","    build_dialogue_dataset(\n","        data_path,\n","        logger,\n","        dev_split,\n","        \"valid.json\",\n","        output_folder,\n","        override,\n","        replacements_path,\n","        dev_random_dialogues,\n","    )\n","\n","    build_dialogue_dataset(\n","        data_path,\n","        logger,\n","        te_split,\n","        \"test.json\",\n","        output_folder,\n","        override,\n","        replacements_path,\n","        te_random_dialogues,\n","    )\n","\n","\n","def insertSpace(token, text):\n","    sidx = 0\n","    while True:\n","        sidx = text.find(token, sidx)\n","        if sidx == -1:\n","            break\n","        if (\n","            sidx + 1 < len(text)\n","            and re.match(\"[0-9]\", text[sidx - 1])\n","            and re.match(\"[0-9]\", text[sidx + 1])\n","        ):\n","            sidx += 1\n","            continue\n","        if text[sidx - 1] != \" \":\n","            text = text[:sidx] + \" \" + text[sidx:]\n","            sidx += 1\n","        if sidx + len(token) < len(text) and text[sidx + len(token)] != \" \":\n","            text = text[: sidx + 1] + \" \" + text[sidx + 1 :]\n","        sidx += 1\n","    return text\n","\n","\n","def normalize(text, replacements):\n","    # lower case every word\n","    text = text.lower()\n","\n","    # replace white spaces in front and end\n","    text = re.sub(r\"^\\s*|\\s*$\", \"\", text)\n","\n","    # hotel domain pfb30\n","    text = re.sub(r\"b&b\", \"bed and breakfast\", text)\n","    text = re.sub(r\"b and b\", \"bed and breakfast\", text)\n","\n","    # weird unicode bug\n","    text = re.sub(\"(\\u2018|\\u2019)\", \"'\", text)\n","\n","    # replace st.\n","    text = text.replace(\";\", \",\")\n","    text = re.sub(\"$\\/\", \"\", text)\n","    text = text.replace(\"/\", \" and \")\n","\n","    # replace other special characters\n","    text = text.replace(\"-\", \" \")\n","    text = re.sub('[\"\\<>@\\(\\)]', \"\", text)  # remove\n","\n","    # insert white space before and after tokens:\n","    for token in [\"?\", \".\", \",\", \"!\"]:\n","        text = insertSpace(token, text)\n","\n","    # insert white space for 's\n","    text = insertSpace(\"'s\", text)\n","\n","    # replace it's, does't, you'd ... etc\n","    text = re.sub(\"^'\", \"\", text)\n","    text = re.sub(\"'$\", \"\", text)\n","    text = re.sub(\"'\\s\", \" \", text)\n","    text = re.sub(\"\\s'\", \" \", text)\n","    for fromx, tox in replacements:\n","        text = \" \" + text + \" \"\n","        text = text.replace(fromx, tox)[1:-1]\n","\n","    # remove multiple spaces\n","    text = re.sub(\" +\", \" \", text)\n","\n","    # concatenate numbers\n","    tokens = text.split()\n","    i = 1\n","    while i < len(tokens):\n","        if re.match(\"^\\d+$\", tokens[i]) and re.match(\"\\d+$\", tokens[i - 1]):\n","            tokens[i - 1] += tokens[i]\n","            del tokens[i]\n","        else:\n","            i += 1\n","    text = \" \".join(tokens)\n","    return text\n","\n","\n","def get_replacements(\n","    replacements_path: str = \"trade/utils/mapping.pair\",\n",") -> List[Tuple[str, str]]:\n","    \"\"\"\n","    Get the replacements from a given file. Used by trade preprocessing.\n","\n","    Arguments\n","    ---------\n","    replacements_path: str\n","        File containing from, to pairs, one per line.\n","\n","    Returns\n","    -------\n","    replacements: List of replacements, i.e. pairs of str\n","        Pairs of elements used to substitute the first element with the second.\n","    \"\"\"\n","    replacements = []\n","    with open(replacements_path, \"r\") as fin:\n","        for line in fin.readlines():\n","            tok_from, tok_to = line.replace(\"\\n\", \"\").split(\"\\t\")\n","            replacements.append((\" \" + tok_from + \" \", \" \" + tok_to + \" \"))\n","    return replacements\n","\n","\n","TOKEN_EXCEPTIONS = {\"childs\": \"children\", \"businesss\": \"businesses\", \"inchs\": \"inches\"}\n","\n","PATTERN_EXCEPTIONS = {\"breakfasts\": \"b&bs\"}\n","\n","\n","def invert_trade_subtokenization(\n","    original_seq: str,\n","    trade_seq: str,\n","    token_exceptions: Dict[str, str] = TOKEN_EXCEPTIONS,\n","    pattern_exceptions: Dict[str, str] = PATTERN_EXCEPTIONS,\n","    subtoken_special_chrs: List[str] = [\" -\", \" _\"],\n",") -> str:\n","    \"\"\"\n","    Invert all trade subtokenizations in a string given the original sequence.\n","\n","    Arguments\n","    ---------\n","    original_seq: str\n","        The original sequence.\n","    trade_seq: str\n","        The sequence that has been pre-processed by trade.\n","    token_exceptions: dict, keys are str, values are str\n","        A dictionary to map merged token to their correct counterpart. E.g.\n","        child -s is merged into childs, but the correct token is children.\n","    pattern_exceptions: dict, keys are str, values are str\n","        A dictionary to map patterns to their correct counterpart. E.g.\n","        after the pre-processing \"b&bs\" is mapped to \"bed and breakfast -s\",\n","        making the search of breakfasts impossible if not handled by such\n","        exceptions.\n","    subtoken_special_chrs: list of str\n","        List containing the special characters that are used for subtokens.\n","\n","    Returns\n","    -------\n","    corrected_seq: str\n","        The sequence corrected, i.e. subtokens replaced by tokens.\n","    \"\"\"\n","    regex = \"|\".join(subtoken_special_chrs)\n","    subtoken_pieces = re.split(regex, trade_seq, maxsplit=1)\n","    search_after: int = 0\n","    while len(subtoken_pieces) > 1:\n","        # example: 'the wind is moderate -ly strong'\n","        # split: ['the wind is moderate ', 'ly strong']\n","        # split[0]: 'the wind is moderate' --> split on whitespace ['the', 'wind', 'is', 'moderate']\n","        left_side = subtoken_pieces[0].split()\n","        subtoken_left = left_side[-1]\n","        # split[1]: 'ly strong' --> split on whitespace ['ly', 'strong']\n","        right_side = subtoken_pieces[1].split()\n","        subtoken_right = right_side[0]\n","        # try merging the subtoken parts to form a token, i.e. moderate + ly\n","        token = \"\".join([subtoken_left, subtoken_right])\n","\n","        if token in token_exceptions:\n","            # if you match an exception, replace the token with the exception\n","            token = token_exceptions[token]\n","\n","        # assume there are no tokens on left and right side of the subtokens' pieces\n","        left_token = None  # if token is at the beginnig\n","        right_token = None  # if token is at the end\n","        # try looking for them\n","        if len(left_side) > 1:\n","            left_token = left_side[-2]\n","        if len(right_side) > 1:\n","            right_token = right_side[1]\n","\n","        # start from a complete match, and progressively remove left and right\n","        # tokens to counter TRADE preprocessing of some tokens\n","        # The order is\n","        # 1. True, True\n","        # 2. True, False\n","        # 3. False, True\n","        # 4. False, False\n","        # basically, at the end you try looking only for the merged token\n","        pattern: str = \"\"\n","        idx: int = -1\n","        for use_left, use_right in product((True, False), (True, False)):\n","            pattern = token\n","            if (left_token is not None) and use_left:\n","                pattern = \" \".join([left_token, pattern])\n","            if right_token is not None and use_right:\n","                pattern = \" \".join([pattern, right_token])\n","\n","            # check if the pattern is in the exceptions\n","            if pattern in pattern_exceptions:\n","                pattern = pattern_exceptions[pattern]\n","            # Search the pattern\n","            idx = original_seq[search_after:].lower().find(pattern)\n","            if idx > -1:\n","                break\n","\n","        error: str = f\"\"\"\n","            Pattern search failed in the following case:\n","            PATTERN =  \\t{pattern}\n","            LEFT SIDE = \\t{left_side}\n","            RIGHT SIDE = \\t{right_side}\n","            ORIG SEQ = \\t{original_seq[search_after:]}\n","\n","            This may be due to further TRADE pre-processing, or not correct merging operation.\n","            To solve this, add a special rule for the token that breaks the code either as a\n","            token_exception or a pattern_exception.\n","        \"\"\"\n","\n","        assert idx > -1, error\n","        # move the index to avoid perfect matches with the same token\n","        # TODO is probably better to move it of len(left_token + token) or\n","        # len(token) depending on the match\n","        search_after += idx + 1\n","        # reconstruct the sentence with the matched pattern\n","        trade_seq = \" \".join([*left_side[:-1], token, *right_side[1:]])\n","\n","        # try splitting the sentence again and repeat the process\n","        subtoken_pieces = re.split(regex, trade_seq, maxsplit=1)\n","    # Good, no subtokens found: return trade seq\n","    return trade_seq\n","\n","\n","def get_json_object(data_path: str) -> dict:\n","    \"\"\"\n","    A function to read a json object and return the python\n","    dictionary associated to it.\n","\n","    Arguments\n","    ---------\n","    data_path: str\n","        Path to a json file.\n","\n","    Returns\n","    -------\n","    loaded_json: dict\n","        A loaded json object.\n","    \"\"\"\n","    with open(data_path, \"r\") as data_file:\n","        data = json.load(data_file)\n","\n","    return data\n","\n","\n","def load_dialogues(\n","    data_path: str,\n","    data_split: List[str],\n","    replacements: List[Tuple[str, str]],\n",") -> List[List[Dict[str, Any]]]:\n","    \"\"\"\n","    Load dialogues from data_path, apply trade pre-processing, revert the\n","    subtokenization, and create a dictionary containing the dialogue id,\n","    the turn id, and the corrected sequence.\n","\n","    Arguments\n","    ---------\n","    data_path: str\n","        Path to the json file containing the data.\n","    data_split: list of str\n","        List of string containing MultiWOZ 2.1 keys of the dialogues\n","        associated to a certain split (train, dev, test).\n","    replacements_path: str\n","        File containing (from, to) pairs, one per line.\n","\n","    Returns\n","    -------\n","    dialogues: list of list of dict, keys are str, values could be anything\n","        List of dialogues. Each dialogue is a list of turns. Each turn is a\n","        dict containing dialogue_idx, turn_idx, and the corrected sequence.\n","    \"\"\"\n","\n","    def get_preprocessed_seq(\n","        original_seq: str, replacements: List[Tuple[str, str]]\n","    ) -> str:\n","        # apply trade normalization\n","        trade_seq = normalize(original_seq, replacements)\n","        # merge back subtokens\n","        sequence = invert_trade_subtokenization(original_seq, trade_seq)\n","        return sequence\n","\n","    dialogues: List[List[Dict[str, Any]]] = []\n","\n","    data = get_json_object(data_path)\n","\n","    for dialogue_idx in tqdm(data_split, desc=\"Load Dialogues\"):\n","        dial: List[Dict[str, Any]] = []\n","        original_dialogue: dict = data[dialogue_idx]\n","        turns: dict = original_dialogue[\"log\"]\n","        for i, turn in enumerate(turns):\n","            sequence = get_preprocessed_seq(turn[\"text\"], replacements)\n","            to_save = {\n","                \"sequence\": sequence,\n","                \"turn_idx\": i,\n","                \"dialogue_idx\": dialogue_idx,\n","            }\n","            dial.append(to_save)\n","        dialogues.append(dial)\n","    return dialogues\n","\n","\n","def create_entry_key(turn: Dict[str, Any]) -> str:\n","    \"\"\"\n","    Creates the entry key for a given entry by considering dialogue id\n","    and turn id for the given turn.\n","\n","    Arguments\n","    ---------\n","    turn: dict, keys are str, values could be anything\n","        A dict containing, the dialogue id, the turn id, the sequence,\n","        and the mean length.\n","    kwargs: any\n","        Additional arguments for the current function.\n","\n","    Returns\n","    -------\n","    key: str\n","        The key for the given turn.\n","    \"\"\"\n","    dialogue_idx = turn[\"dialogue_idx\"]\n","    turn_idx = turn[\"turn_idx\"]\n","    return f\"{dialogue_idx}_{turn_idx}\"\n","\n","\n","def create_dialogue_dataset(\n","    dialogues: List[List[Dict[str, Any]]]\n",") -> Dict[str, Dict[str, Any]]:\n","    \"\"\"\n","    Creates a dialogue dataset starting from a set of dialogues. Each\n","    entry of the dataset contains the dialogue history and the system\n","    reply in response to that.\n","\n","    Arguments\n","    ---------\n","    dialogues: list of list of dict, keys are str, values could be anything\n","        List of dialogues. Each dialogue is a list of turns. Each turn is a\n","        dict containing dialogue_idx, turn_idx, and the corrected sequence.\n","    kwargs: any\n","        Additional arguments for the current function.\n","\n","    Returns\n","    -------\n","    dataset: Dict[str, Dict[str, Any]]\n","        Dataset, keys are str, values are dictionaries containing the\n","        dialogue history and the system reply.\n","    \"\"\"\n","\n","    def create_dialogue_dataset_entry(\n","        turn: Dict[str, Any], history: List[str]\n","    ) -> Optional[Dict[str, Any]]:\n","        \"\"\"\n","        Creates an entry if the current turn id is odd. An entry is\n","        composed of the history, which contains the previous turns\n","        of the current dialogue, and the reply of the system.\n","\n","        Arguments\n","        ---------\n","        turn: dict, keys are str, values could be anything\n","            A dict containing, the dialogue id, the turn id, the sequence,\n","            and the mean length.\n","        replacements_path: str\n","            Path to TRADE file containing (from, to) pairs, one per line.\n","        kwargs: any\n","            Additional arguments for the current function.\n","\n","        Returns\n","        -------\n","        entry: optional dict, keys are str, values could be anything\n","            Entry of the dialogue dataset. It is a dict containing the history\n","            of the dialogue, i.e. a list of turns, the reply of the system,\n","            i.e. a turn, and the mean length.\n","        \"\"\"\n","\n","        turn_idx = turn[\"turn_idx\"]\n","        entry: Optional[Dict[str, Any]] = None\n","        if turn_idx % 2 == 0:\n","            # user turn, simply append it to the history\n","            user_seq: str = turn[\"sequence\"]\n","            history.append(user_seq)\n","        elif turn_idx % 2 == 1:\n","            # system turn, create the dataset entry, and the append it to the history\n","            system_seq: str = turn[\"sequence\"]\n","            history_mean_length = mean([len(turn) for turn in history])\n","            entry = {\n","                \"history\": history.copy(),\n","                \"reply\": system_seq,\n","                \"length\": history_mean_length + len(system_seq),\n","            }\n","            history.append(system_seq)\n","        return entry\n","\n","    dataset: Dict[str, Dict[str, Any]] = {}\n","    for dialogue in tqdm(dialogues, desc=\"Creating dataset\"):\n","        history: List[str] = []\n","        for turn in dialogue:\n","            # custom function to create a dataset entry\n","            dataset_entry = create_dialogue_dataset_entry(turn, history)\n","            # custom function to create a dataset key\n","            key = create_entry_key(turn)\n","            if dataset_entry is not None:\n","                dataset[key] = dataset_entry\n","    return dataset\n","\n","\n","def save_dialogue_dataset(\n","    dataset: Dict[str, Dict[str, Any]], file_name: str, dst_folder: str = \".\"\n",") -> None:\n","    \"\"\"\n","    Saves the dialogue dataset at dst_folder/file_name as a json file.\n","\n","    Arguments\n","    ---------\n","    dataset: Dict[str, Dict[str, Any]]\n","        Dataset, keys are str, values are dictionaries containing the\n","        dialogue history, the system reply, and the mean length.\n","    file_name: str\n","        Name of the file where the dataset will be saved.\n","    dst_folder: str\n","        Path to the directory where the dataset will be saved. If it\n","        does not exists, it creates it.\n","    \"\"\"\n","    os.makedirs(dst_folder, exist_ok=True)\n","    dataset_path = os.path.join(dst_folder, file_name)\n","    with open(dataset_path, \"w\") as f:\n","        json.dump(dataset, f, indent=4)\n","\n","\n","def encode_dialogue_dataset(\n","    file_name: str,\n","    dst_folder: str,\n","    data_path: str,\n","    data_split: List[str],\n","    override: bool,\n","    logger: logging.Logger,\n","    replacements_path: str = \"utils/mapping.pair\",\n","    random_dialogues: Optional[int] = None,\n",") -> None:\n","    \"\"\"\n","    Wrapper function that loads processed data stored at\n","    dst_folder/file_name. If they are not available, it processes the\n","    original data and then saves them at dst_folder/file_name.\n","\n","    Arguments\n","    ---------\n","    file_name: str\n","        Name of the file where the dataset will be saved.\n","    dst_folder: str\n","        Path to the directory where the dataset will be saved. If it\n","        does not exists, it creates it.\n","    data_path: str\n","        Path to the data pre-processed by trade.\n","    data_split: list of str\n","        List of string containing MultiWOZ 2.1 keys of the dialogues\n","        associated to a certain split (train, dev, test).\n","    override: bool\n","        Whether or not override the data stored at dst_folder/file_name.\n","    logger: logging.Logger instance\n","        Logger to report the processing steps carried out in the current\n","        execution.\n","    replacements_path: str\n","        Path to TRADE file containing (from, to) pairs, one per line.\n","    random_dialogues: int\n","        Number of dialogues to randomly sample from the current data.\n","    \"\"\"\n","    dataset_path = os.path.join(dst_folder, file_name)\n","    if os.path.isfile(dataset_path) and (not override):\n","        logger.info(f\"Dataset already created at {dataset_path}\")\n","    else:\n","        replacements = get_replacements(replacements_path)\n","        logger.info(f\"Extract dialogues from {data_path}\")\n","        # custom loading function to return the important elements of a dialogue\n","        dialogues = load_dialogues(data_path, data_split, replacements)\n","        if random_dialogues:\n","            dialogues = random.sample(dialogues, min(random_dialogues, len(dialogues)))\n","        logger.info(\"Create dataset\")\n","        dataset = create_dialogue_dataset(dialogues)\n","        logger.info(f\"Save dataset in {dataset_path}\")\n","        save_dialogue_dataset(dataset, file_name, dst_folder)\n","\n","\n","def build_dialogue_dataset(\n","    data_path: str,\n","    logger: logging.Logger,\n","    data_split: List[str],\n","    file_name: str = \"train.json\",\n","    dst_folder: str = \".\",\n","    override: bool = False,\n","    replacements_path: str = \"utils/mapping.pair\",\n","    random_dialogues: Optional[int] = None,\n",") -> None:\n","    \"\"\"\n","    Returns the dialogue dataset for the corresponding data_path.\n","\n","    Arguments\n","    ---------\n","    data_path: str\n","        Path to the data pre-processed by trade.\n","    logger: logging.Logger instance\n","        Logger to report the processing steps carried out in the current\n","        execution.\n","    data_split: list of str\n","        List of string containing MultiWOZ 2.1 keys of the dialogues\n","        associated to a certain split (train, dev, test).\n","    file_name: str\n","        Name of the file where the dataset will be saved.\n","    dst_folder: str\n","        Path to the directory where the dataset will be saved. If it\n","        does not exists, it creates it.\n","    override: bool\n","        Whether or not override the data stored at dst_folder/file_name.\n","    replacements_path: str\n","        Path to TRADE file containing (from, to) pairs, one per line.\n","    random_dialogues: int\n","        Number of dialogues to randomly sample from the current data.\n","\n","    Returns\n","    -------\n","    dataset:\n","        dataset, keys are str, values are dictionaries containing the\n","        dialogue history, the system reply, and the mean length.\n","    \"\"\"\n","    logger.info(f\"Prepare {file_name}\")\n","    encode_dialogue_dataset(\n","        file_name,\n","        dst_folder,\n","        data_path,\n","        data_split,\n","        override,\n","        logger,\n","        replacements_path,\n","        random_dialogues=random_dialogues,\n","    )\n","\n","\n","def download_mwoz_21(destination):\n","    \"\"\"Download dataset repo, unpack it, and remove unnecessary elements.\n","    Arguments\n","    ---------\n","    destination : str\n","        Place to put dataset.\n","    \"\"\"\n","    mwoz_21_archive = os.path.join(destination, \"MultiWOZ_21.zip\")\n","    download_file(MULTIWOZ_21_DATASET_URL, mwoz_21_archive)\n","    shutil.unpack_archive(mwoz_21_archive, destination)\n","    shutil.rmtree(os.path.join(destination, \"__MACOSX\"))\n","\n","    mwoz_21 = os.path.join(destination, \"MultiWOZ_21\")\n","    os.makedirs(mwoz_21, exist_ok=True)\n","\n","    mwoz_21_repo = os.path.join(destination, \"MultiWOZ_2.1\")\n","    for relevant_file in [\"data.json\", \"valListFile.txt\", \"testListFile.txt\"]:\n","        shutil.move(\n","            os.path.join(mwoz_21_repo, relevant_file),\n","            os.path.join(mwoz_21, relevant_file),\n","        )\n","\n","    shutil.rmtree(mwoz_21_repo)\n","\n","\n","def get_splits(dataset_folder) -> Tuple[List[str], List[str], List[str]]:\n","    mwoz_21_dialouges = get_json_object(os.path.join(dataset_folder, \"data.json\"))\n","    dialougues_keys: Set[str] = set(mwoz_21_dialouges.keys())\n","    tr_split: List[str] = []\n","    with open(os.path.join(dataset_folder, \"valListFile.txt\")) as f:\n","        dev_split: List[str] = [key.strip() for key in f]\n","    with open(os.path.join(dataset_folder, \"testListFile.txt\")) as f:\n","        te_split: List[str] = [key.strip() for key in f]\n","\n","    for key in dialougues_keys:\n","        if key not in dev_split and key not in te_split:\n","            tr_split.append(key)\n","\n","    return tr_split, dev_split, te_split\n","\n","\n"],"metadata":{"id":"6h18vok49c0U","executionInfo":{"status":"ok","timestamp":1711988191917,"user_tz":240,"elapsed":365,"user":{"displayName":"pooneh mousavi","userId":"07980475777627459411"}}},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":["For this tutorial, we only use 1000, 100 and 200 sentences for training, valid and test respectively."],"metadata":{"id":"Kuok1VOkWES_"}},{"cell_type":"code","source":["prepare_mwoz_21(\"data_dir\", \"data\", False, \"mapping.pair\", 1000, 100, 200)\n"],"metadata":{"id":"B3Z3EljGDskB","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1711988246235,"user_tz":240,"elapsed":52429,"user":{"displayName":"pooneh mousavi","userId":"07980475777627459411"}},"outputId":"6576a363-4c0e-4331-9c5d-bcae88b69147"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["Downloading https://github.com/budzianowski/multiwoz/raw/master/data/MultiWOZ_2.1.zip to data/MultiWOZ_21.zip\n"]},{"output_type":"stream","name":"stderr","text":["MultiWOZ_2.1.zip: 20.2MB [00:01, 17.2MB/s]                            \n","Load Dialogues: 100%|██████████| 8438/8438 [00:09<00:00, 850.35it/s]\n","Creating dataset: 100%|██████████| 1000/1000 [00:00<00:00, 2331.41it/s]\n","Load Dialogues: 100%|██████████| 1000/1000 [00:01<00:00, 832.48it/s]\n","Creating dataset: 100%|██████████| 100/100 [00:00<00:00, 5534.33it/s]\n","Load Dialogues: 100%|██████████| 1000/1000 [00:01<00:00, 826.54it/s]\n","Creating dataset: 100%|██████████| 200/200 [00:00<00:00, 9161.97it/s]\n"]}]},{"cell_type":"markdown","source":["### **Step 2: Tokenizer**\n","GPT-2 comes equipped with its own tokenizer known as GPT2Tokenizer, which becomes accessible upon instantiation of the Speechbrain GPT interface.\n","\n","```\n","  tokenizer = hparams['GPT2']GPT2.tokenizer\n","```\n","We need to add special tokens to the tokenizer to identify which speaker is talking (system or user).\n","\n","\n","```\n","def add_special_tokens_(model, tokenizer, attr_to_special_token) -> None:\n","    orig_num_tokens = len(tokenizer.encoder)\n","    num_added_tokens = tokenizer.add_special_tokens(\n","        attr_to_special_token  # type: ignore\n","    )  # doesn't add if they are already there\n","    if num_added_tokens > 0:\n","        model.resize_token_embeddings(\n","            new_num_tokens=orig_num_tokens + num_added_tokens\n","        )\n","```\n","\n","\n","\n","\n","\n","\n"],"metadata":{"id":"cP1IrVEtF9m8"}},{"cell_type":"markdown","source":["### **Step 3: Train a Model**"],"metadata":{"id":"7xsnCbEJIdC9"}},{"cell_type":"markdown","metadata":{"id":"LBqJpUdUi2aL"},"source":["Since we are performing a language modeling task, for our model, we will be finetuning GPT2LMHeadModel. For fine-tuning, it is enough to load the pre-trained version of the model and use the default forward method implementation. It is important to specify the correct hugging face path, which for this model is gpt_hub: gpt2.\n","\n","The hyperparameter file for our model is the following:"]},{"cell_type":"code","source":["%%file hparams_gpt2.yaml\n","# ########################################\n","# Model: GPT2LMHeadModel +  NLL\n","# Authors:\n","    # Pooneh Mousavi 2023\n","    # Simone Alghisi 2023\n","# ########################################\n","\n","# Seed needs to be set at top of yaml, before objects with parameters are made\n","seed: 1995\n","__set_seed: !apply:torch.manual_seed [!ref <seed>]\n","# Dataset will be downloaded to the `data_original`\n","data_folder: !PLACEHOLDER\n","output_folder: !ref /content/results/train_with_gpt2/<seed>\n","save_folder: !ref <output_folder>/save\n","train_log: !ref <output_folder>/train_log.txt\n","bleu_4_test_file: !ref <output_folder>/bleu_4_test.txt\n","bleu_4_valid_file: !ref <output_folder>/bleu_4_valid.txt\n","\n","# URL for the gpt2 model\n","gpt_hub: gpt2\n","gpt_folder: !ref <save_folder>/gpt_checkpoint\n","\n","# Path where data manifest files will be stored\n","train_annotation: !ref <data_folder>/train.json\n","valid_annotation: !ref <data_folder>/valid.json\n","test_annotation: !ref <data_folder>/test.json\n","\n","skip_prep: False\n","\n","# The train logger writes training statistics to a file, as well as stdout.\n","train_logger: !new:speechbrain.utils.train_logger.FileTrainLogger\n","    save_file: !ref <train_log>\n","\n","# Special tokens\n","bos_token: \"BOS\"\n","eos_token: \"EOS\"\n","\n","system_token: \"SPK_1\"\n","user_token: \"SPK_2\"\n","\n","special_tokens: [\n","    !ref <bos_token>,\n","    !ref <eos_token>,\n","    !ref <system_token>,\n","    !ref <user_token>\n","]\n","\n","attr_to_special_tokens:\n","    \"bos_token\": !ref <bos_token>\n","    \"eos_token\": !ref <eos_token>\n","    \"additional_special_tokens\": [!ref <system_token>, !ref <user_token>]\n","\n","# history_window, i.e. how many user-system exchanges consider as context.\n","max_history: 5\n","\n","ignore_index: -100\n","label_smoothing: 0\n","\n","####################### Training Parameters ####################################\n","number_of_epochs: 4\n","batch_size: 8\n","test_batch_size: 4\n","lr: 1.97125e-4\n","\n","#freeze GPT model\n","freeze_gptmodel: False\n","num_beams: 3\n","max_new_tokens: 50\n","top_k: 45\n","top_p: 0.9\n","\n","\n","train_dataloader_options:\n","    batch_size: !ref <batch_size>\n","    shuffle: True\n","    num_workers: 2\n","    drop_last: False\n","\n","test_dataloader_options:\n","    batch_size: !ref <test_batch_size>\n","    shuffle: True\n","    num_workers: 2\n","    drop_last: True\n","\n","# Masks\n","padding_mask: !name:speechbrain.lobes.models.transformer.Transformer.get_key_padding_mask\n","\n","# gpt model\n","gpt_model: !new:speechbrain.lobes.models.huggingface_transformers.gpt.GPT\n","    source: !ref <gpt_hub>\n","    freeze: !ref <freeze_gptmodel>\n","    save_path: !ref <gpt_folder>\n","    max_new_tokens: !ref <max_new_tokens>\n","    num_beams: !ref <num_beams>\n","    top_k: !ref  <top_k>\n","    top_p: !ref <top_p>\n","\n","epoch_counter: !new:speechbrain.utils.epoch_loop.EpochCounter\n","    limit: !ref <number_of_epochs>\n","\n","modules:\n","    gpt_model: !ref <gpt_model>\n","\n","model: !new:torch.nn.ModuleList\n","    - [!ref <gpt_model>]\n","\n","\n","ce_loss: !new:torch.nn.CrossEntropyLoss\n","    ignore_index: !ref <ignore_index>\n","    label_smoothing: !ref <label_smoothing>\n","\n","opt_class: !name:torch.optim.AdamW\n","    lr: !ref <lr>\n","\n","\n","lr_annealing: !new:speechbrain.nnet.schedulers.NewBobScheduler\n","    initial_value: !ref <lr>\n","    improvement_threshold: 0.0025\n","    annealing_factor: 0.9\n","    patient: 0\n","\n","checkpointer: !new:speechbrain.utils.checkpoints.Checkpointer\n","    checkpoints_dir: !ref <save_folder>\n","    recoverables:\n","        gpt_model: !ref <gpt_model>\n","        lr_annealing_output: !ref <lr_annealing>\n","        counter: !ref <epoch_counter>\n","\n","\n","bleu_4_computer: !name:speechbrain.utils.bleu.BLEUStats\n","    max_ngram_order: 4\n","\n","bleu_2_computer: !name:speechbrain.utils.bleu.BLEUStats\n","    max_ngram_order: 2"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"I9mCa7xrd_uj","executionInfo":{"status":"ok","timestamp":1711989008442,"user_tz":240,"elapsed":163,"user":{"displayName":"pooneh mousavi","userId":"07980475777627459411"}},"outputId":"274661a4-620a-4d9d-fb8a-f3b69e133da8"},"execution_count":19,"outputs":[{"output_type":"stream","name":"stdout","text":["Writing hparams_gpt2.yaml\n"]}]},{"cell_type":"markdown","metadata":{"id":"MYcKmhPmlALt"},"source":["The training script follows a standard approach, and you should be able to identify the common operations that are necessary to implement a neural classifier:\n"]},{"cell_type":"code","source":["%%file train.py\n","\"\"\"\n","Recipe for training a gpt_based response generation model with MultiWOZ.\n","The system employs GPT2 (https://life-extension.github.io/2020/05/27/GPT%E6%8A%80%E6%9C%AF%E5%88%9D%E6%8E%A2/language-models.pdf).\n","This recipe takes the GPT2LMHeadModel to fine-tune for the response generation task on the NLL.\n","\n","To run this recipe, do the following:\n","> python train_with_gpt.py hparams/train_gpt.yaml\n","\n","Authors\n"," * Pooneh Mousavi 2023\n"," * Simone Alghisi 2023\n","\"\"\"\n","\n","\n","import sys\n","import speechbrain as sb\n","import torch\n","from itertools import chain\n","from hyperpyyaml import load_hyperpyyaml\n","from speechbrain.utils.distributed import run_on_main\n","import math\n","from speechbrain.dataio.batch import PaddedBatch\n","\n","\n","class ResGenBrain(sb.Brain):\n","    def compute_forward(self, batch, stage):\n","        \"\"\"Computation pipeline based on a gpt decoder.\"\"\"\n","        # Get required data from batch\n","        batch = batch.to(self.device)\n","        input_ids, _ = batch.input_ids\n","        token_type_ids, _ = batch.token_type_ids\n","\n","        # Forward Pass\n","        padding_mask = ~self.hparams.padding_mask(\n","            input_ids, pad_idx=tokenizer.unk_token_id\n","        )\n","        outputs = self.modules.gpt_model(\n","            input_ids, token_type_ids, padding_mask\n","        ).logits\n","\n","        return outputs\n","\n","    def compute_objectives(self, predictions, batch, stage):\n","        \"\"\"Computes the NLL-loss using reply as label.\"\"\"\n","        # Get required data from batch\n","        batch = batch.to(self.device)\n","        ids = batch.id\n","        lm_labels, labels_lens = batch.lm_labels\n","        history_bos, history_lens = batch.history_bos\n","        reply_eos, reply_lens = batch.reply_eos\n","        history_token_type, _ = batch.history_token_type\n","\n","        loss = self.hparams.ce_loss(\n","            predictions.flatten(end_dim=-2), lm_labels.flatten()\n","        )\n","\n","        if stage == sb.Stage.VALID:\n","            # hyps = None\n","            # current_epoch = self.hparams.epoch_counter.current\n","            # if current_epoch % self.hparams.valid_search_interval == 0:\n","            # history_bos = torch.LongTensor([hparams[\"bos_index\"]] + (history_bos))\n","            padding_mask = ~self.hparams.padding_mask(\n","                history_bos, pad_idx=tokenizer.unk_token_id\n","            )\n","            hyps = self.modules.gpt_model.generate(\n","                history_bos.detach(),\n","                history_token_type.detach(),\n","                padding_mask.detach(),\n","            )\n","        elif stage == sb.Stage.TEST:\n","            padding_mask = ~self.hparams.padding_mask(\n","                history_bos, pad_idx=tokenizer.unk_token_id\n","            )\n","            hyps = self.modules.gpt_model.generate(\n","                history_bos.detach(),\n","                history_token_type.detach(),\n","                padding_mask.detach(),\n","                \"beam\",\n","            )\n","\n","        if stage != sb.Stage.TRAIN:\n","            reply_truncated = [\n","                reply_eos[i][\n","                    : int(reply_lens[i].item() * reply_eos.shape[1] - 1)\n","                ].detach()\n","                for i in range(reply_eos.shape[0])\n","            ]\n","            predicted_words = tokenizer.batch_decode(\n","                hyps[:, history_bos.shape[1] :],\n","                skip_special_tokens=True,\n","                clean_up_tokenization_spaces=True,\n","            )\n","            target_words = tokenizer.batch_decode(\n","                reply_truncated,\n","                skip_special_tokens=True,\n","                clean_up_tokenization_spaces=True,\n","            )\n","            self.bleu_4_metric.append(ids, predicted_words, target_words)\n","            self.bleu_2_metric.append(ids, predicted_words, target_words)\n","            if stage != sb.Stage.TRAIN:\n","                self.hyps.extend(predicted_words)\n","                self.references.extend(target_words)\n","\n","        return loss\n","\n","    def on_stage_start(self, stage, epoch):\n","        \"\"\"Gets called at the beginning of each epoch\"\"\"\n","        if stage != sb.Stage.TRAIN:\n","            self.bleu_4_metric = self.hparams.bleu_4_computer()\n","            self.bleu_2_metric = self.hparams.bleu_2_computer()\n","            self.hyps = []\n","            self.references = []\n","\n","    def on_stage_end(self, stage, stage_loss, epoch):\n","        \"\"\"Gets called at the end of an epoch.\n","\n","        Arguments\n","        ---------\n","        stage : sb.Stage\n","            One of sb.Stage.TRAIN, sb.Stage.VALID, sb.Stage.TEST\n","        stage_loss : float\n","            The average loss for all of the data processed in this stage.\n","        epoch : int\n","            The currently-starting epoch. This is passed\n","            `None` during the test stage.\n","        \"\"\"\n","\n","        # Store the train loss until the validation stage.\n","        stage_stats = {\"loss\": stage_loss}\n","        stage_stats[\"PPL\"] = math.exp(stage_loss)\n","        if stage == sb.Stage.TRAIN:\n","            self.train_stats = stage_stats\n","        else:\n","            stage_stats[\"BLEU_4\"] = self.bleu_4_metric.summarize(\"BLEU\")\n","            stage_stats[\"BLEU_2\"] = self.bleu_2_metric.summarize(\"BLEU\")\n","        # Perform end-of-iteration things, like annealing, logging, etc.\n","        if stage == sb.Stage.VALID:\n","            # Update learning rate\n","            old_lr, new_lr = self.hparams.lr_annealing(epoch)\n","            sb.nnet.schedulers.update_learning_rate(self.optimizer, new_lr)\n","\n","            # The train_logger writes a summary to stdout and to the logfile.\n","\n","            self.hparams.train_logger.log_stats(\n","                stats_meta={\"epoch\": epoch, \"lr\": old_lr},\n","                train_stats=self.train_stats,\n","                valid_stats=stage_stats,\n","            )\n","            # Save the current checkpoint and delete previous checkpoints.\n","            self.checkpointer.save_and_keep_only(\n","                meta={\"PPL\": stage_stats[\"PPL\"]},\n","                min_keys=[\"PPL\"],\n","            )\n","            if epoch == hparams[\"number_of_epochs\"] - 1:\n","                with open(self.hparams.bleu_4_valid_file, \"w\") as w:\n","                    self.bleu_4_metric.write_stats(w)\n","                    for i in range(len(self.hyps)):\n","                        w.write(\"target: \" + str(self.references[i]) + \"\\n\")\n","                        w.write(\"predicted:\" + str(self.hyps[i]) + \"\\n\")\n","                        w.write(\n","                            \"++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\\n\"\n","                        )\n","\n","        # We also write statistics about test data to stdout and to the logfile.\n","        elif stage == sb.Stage.TEST:\n","            self.hparams.train_logger.log_stats(\n","                stats_meta={\"Epoch loaded\": self.hparams.epoch_counter.current},\n","                test_stats=stage_stats,\n","            )\n","            with open(self.hparams.bleu_4_test_file, \"w\") as w:\n","                self.bleu_4_metric.write_stats(w)\n","                for i in range(len(self.hyps)):\n","                    w.write(\"target: \" + str(self.references[i]) + \"\\n\")\n","                    w.write(\"predicted:\" + str(self.hyps[i]) + \"\\n\")\n","                    w.write(\n","                        \"++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\\n\"\n","                    )\n","\n","    def init_optimizers(self):\n","        \"Initializes the model optimizer\"\n","        self.optimizer = self.hparams.opt_class(self.hparams.model.parameters())\n","\n","        if self.checkpointer is not None:\n","            self.checkpointer.add_recoverable(\"optimizer\", self.optimizer)\n","\n","        self.optimizers_dict = {\n","            \"optimizer\": self.optimizer,\n","        }\n","\n","\n","def add_special_tokens_(model, tokenizer, attr_to_special_token) -> None:\n","    orig_num_tokens = len(tokenizer.encoder)\n","    num_added_tokens = tokenizer.add_special_tokens(\n","        attr_to_special_token  # type: ignore\n","    )  # doesn't add if they are already there\n","    if num_added_tokens > 0:\n","        model.resize_token_embeddings(\n","            new_num_tokens=orig_num_tokens + num_added_tokens\n","        )\n","\n","\n","def dataio_prep(hparams, tokenizer):\n","    \"\"\"This function prepares the datasets to be used in the brain class.\n","    It also defines the data processing pipeline through user-defined\n","    functions. We expect `prepare_multiwoz` to have been called before\n","    this, so that the `train.json`, `dev.json`,  and `test.json` manifest\n","    files are available.\n","\n","    Arguments\n","    ---------\n","    hparams : dict\n","        This dictionary is loaded from the `train.yaml` file, and it includes\n","        all the hyperparameters needed for dataset construction and loading.\n","    tokenizer : tokenizer\n","        Object for converting text to tokens.\n","\n","    Returns\n","    -------\n","    datasets : dict\n","        Contains two keys, \"train\" and \"valid\" that correspond\n","        to the appropriate DynamicItemDataset object.\n","    \"\"\"\n","\n","    # convert special tokens to their ids\n","    bos, eos, system, user = tokenizer.convert_tokens_to_ids(\n","        hparams[\"special_tokens\"]\n","    )\n","    # history_window, i.e. how many user-system exchanges consider as context (+1 to consider at least the last user turn)\n","    history_window = 2 * hparams[\"max_history\"] + 1\n","\n","    #  Define history pipeline:\n","    @sb.utils.data_pipeline.takes(\"history\")\n","    @sb.utils.data_pipeline.provides(\n","        \"history\",\n","        \"history_tokens_lists\",\n","        \"history_ids\",\n","        \"history_bos\",\n","        \"history_token_type\",\n","    )\n","    def history_pipeline(history):\n","        yield history\n","\n","        # encode each turn of the history\n","        history_tokens_lists = [tokenizer.encode(turn) for turn in history]\n","        yield history_tokens_lists\n","\n","        # add speaker tokens to the history turns (user is even, system is odd)\n","        # BEFORE:  [Hi how are you?], [I'm fine, thanks]\n","        # AFTER:   [SPK_1 Hi how are you?], [SPK_2 I'm fine, thanks]\n","        history_input_lists = [\n","            [user if i % 2 == 0 else system] + encoded_turn\n","            for i, encoded_turn in enumerate(history_tokens_lists)\n","        ]\n","\n","        history_ids = history_input_lists[-history_window:]\n","        # concatenate every token into a single list\n","        # list(chain(*[[1, 2], [3, 4], [5]]))\n","        # >>> [1, 2, 3, 4, 5]\n","        history_ids = torch.LongTensor(list(chain(*history_ids)))\n","        # without bos for lm_labels\n","        yield history_ids\n","\n","        # create bos version for the input\n","        history_bos = torch.cat((torch.tensor([bos]), history_ids))\n","        yield history_bos\n","\n","        # create a mapping that associates each token in the input to a speaker\n","        # INPUT: [SPK_1 Hi    how   are   you? ], [SPK_2 I'm   fine, thanks]\n","        # TYPE:  [SPK_1 SPK_1 SPK_1 SPK_1 SPK_1], [SPK_2 SPK_2 SPK_2 SPK_2 ]\n","        history_token_type_lists = [\n","            [user if i % 2 == 0 else system] * len(encoded_turn)\n","            for i, encoded_turn in enumerate(history_input_lists)\n","        ]\n","        history_token_type = torch.LongTensor(\n","            list(\n","                chain(\n","                    *([[system]] + history_token_type_lists[-history_window:])\n","                )\n","            )\n","        )\n","\n","        yield history_token_type\n","\n","    #  Define reply pipeline:\n","    @sb.utils.data_pipeline.takes(\"reply\")\n","    @sb.utils.data_pipeline.provides(\n","        \"reply\",\n","        \"reply_tokens_list\",\n","        \"reply_ids\",\n","        \"reply_eos\",\n","        \"reply_token_type\",\n","    )\n","    def reply_pipeline(reply):\n","        yield reply\n","\n","        reply_tokens_list = tokenizer.encode(reply)\n","        yield reply_tokens_list\n","\n","        # specify that the system will say the reply\n","        reply_input_list = [system] + reply_tokens_list\n","        reply_ids = torch.LongTensor(reply_input_list)\n","        yield reply_ids\n","\n","        # create eos version of the reply for lm_labels\n","        reply_eos = torch.cat((reply_ids, torch.tensor([eos])))\n","        yield reply_eos\n","\n","        # specify the speaker for each token in the reply\n","        reply_token_type = torch.LongTensor([system] * len(reply_input_list))\n","        yield reply_token_type\n","\n","    # Define input_and_token_type_pipeline\n","    @sb.utils.data_pipeline.takes(\n","        \"history_ids\",\n","        \"history_bos\",\n","        \"history_token_type\",\n","        \"reply_ids\",\n","        \"reply_eos\",\n","        \"reply_token_type\",\n","    )\n","    @sb.utils.data_pipeline.provides(\"input_ids\", \"token_type_ids\", \"lm_labels\")\n","    def input_and_token_type_pipeline(\n","        history_ids,\n","        history_bos,\n","        history_token_type,\n","        reply_ids,\n","        reply_eos,\n","        reply_token_type,\n","    ):\n","        # put history and reply together\n","        # N.B. input_sequence = history_bos + reply_ids, we don't have eos in the input\n","        input_ids = torch.cat((history_bos, reply_ids), -1)\n","        yield input_ids\n","\n","        token_type_ids = torch.cat((history_token_type, reply_token_type), -1)\n","        yield token_type_ids\n","\n","        # create the language model label (ground truth) for the current input\n","        # -100 is a special tokens that is ignored during the loss computation\n","        # the idea is to mask everything except the reply (without the speaker token)\n","        # N.B. we don't have bos in the input\n","        lm_labels = (\n","            [hparams[\"ignore_index\"]] * history_ids.shape[0]\n","            + [hparams[\"ignore_index\"]]\n","            + reply_eos[1:].tolist()\n","        )\n","        lm_labels = torch.LongTensor(lm_labels)\n","\n","        yield lm_labels\n","\n","    # Define datasets. We also connect the dataset with the data processing\n","    # functions defined above.\n","    datasets = {}\n","    data_info = {\n","        \"train\": hparams[\"train_annotation\"],\n","        \"valid\": hparams[\"valid_annotation\"],\n","        \"test\": hparams[\"test_annotation\"],\n","    }\n","    for dataset in data_info:\n","        datasets[dataset] = sb.dataio.dataset.DynamicItemDataset.from_json(\n","            json_path=data_info[dataset],\n","            replacements={\"data_root\": hparams[\"data_folder\"]},\n","            dynamic_items=[\n","                reply_pipeline,\n","                history_pipeline,\n","                input_and_token_type_pipeline,\n","            ],\n","            output_keys=[\n","                \"id\",\n","                \"input_ids\",\n","                \"token_type_ids\",\n","                \"history_bos\",\n","                \"reply_eos\",\n","                \"history_token_type\",\n","                \"reply_token_type\",\n","                \"lm_labels\",\n","            ],\n","        )\n","\n","    return datasets\n","\n","\n","# RECIPE BEGINS!\n","if __name__ == \"__main__\":\n","    # Reading command line arguments.\n","    hparams_file, run_opts, overrides = sb.parse_arguments(sys.argv[1:])\n","\n","    # Initialize ddp (useful only for multi-GPU DDP training).\n","    sb.utils.distributed.ddp_init_group(run_opts)\n","\n","    # Load hyperparameters file with command-line overrides.\n","    with open(hparams_file) as fin:\n","        hparams = load_hyperpyyaml(fin, overrides)\n","\n","    # Create experiment directory\n","    sb.create_experiment_directory(\n","        experiment_directory=hparams[\"output_folder\"],\n","        hyperparams_to_save=hparams_file,\n","        overrides=overrides,\n","    )\n","\n","\n","    # Load tokenizer and add special tokens\n","    tokenizer = hparams[\"gpt_model\"].tokenizer\n","\n","    #  Load pretrained GPT\n","    hparams[\"gpt_model\"] = hparams[\"gpt_model\"].to(device=run_opts[\"device\"])\n","\n","    # Add special tokens to the tokenizer and resize model embedding\n","    add_special_tokens_(\n","        hparams[\"gpt_model\"].model, tokenizer, hparams[\"attr_to_special_tokens\"]\n","    )\n","\n","    class CustomPaddedBatch(PaddedBatch):\n","        \"\"\"PaddedBatch with custom padding values.\n","\n","        See the documentation of `speechbrain.dataio.batch.PaddedBatch`.\n","\n","        \"\"\"\n","\n","        def __init__(self, examples, *args, **kwargs):\n","            _, _, system, _ = tokenizer.convert_tokens_to_ids(\n","                hparams[\"special_tokens\"]\n","            )\n","            for k in [\n","                \"input_ids\",\n","                \"history_bos\",\n","                \"lm_labels\",\n","                \"token_type_ids\",\n","                \"history_token_type\",\n","            ]:\n","                max_len = max([len(x[k]) for x in examples])\n","                pad_value = 0\n","                if k in [\n","                    \"input_ids\",\n","                    \"history_bos\",\n","                    \"token_type_ids\",\n","                    \"history_token_type\",\n","                ]:\n","                    pad_value = tokenizer.unk_token_id\n","                elif k == \"lm_labels\":\n","                    pad_value = hparams[\"ignore_index\"]\n","                for example in examples:\n","                    x = example[k]\n","                    if k in [\"history_bos\", \"history_token_type\"]:\n","                        x = torch.cat(\n","                            (example[k], torch.LongTensor([system])), -1\n","                        )\n","                        example[k] = torch.nn.functional.pad(\n","                            x, [max_len - len(x), 0], value=pad_value\n","                        )\n","                    else:\n","                        example[k] = torch.nn.functional.pad(\n","                            x, [0, max_len - len(x)], value=pad_value\n","                        )\n","            super().__init__(examples, *args, **kwargs)\n","\n","    hparams[\"train_dataloader_options\"][\"collate_fn\"] = CustomPaddedBatch\n","    hparams[\"test_dataloader_options\"][\"collate_fn\"] = CustomPaddedBatch\n","\n","    # Create dataset objects \"train\", \"valid\", and \"test\".\n","    datasets = dataio_prep(hparams, tokenizer)\n","\n","    # Initialize the Brain object to prepare for mask training.\n","    res_gen_brain = ResGenBrain(\n","        modules=hparams[\"modules\"],\n","        opt_class=hparams[\"opt_class\"],\n","        hparams=hparams,\n","        run_opts=run_opts,\n","        checkpointer=hparams[\"checkpointer\"],\n","    )\n","\n","    # We load the pretrained whisper model\n","    if \"pretrainer\" in hparams.keys():\n","        run_on_main(hparams[\"pretrainer\"].collect_files)\n","        hparams[\"pretrainer\"].load_collected(res_gen_brain.device)\n","\n","    # The `fit()` method iterates the training loop, calling the methods\n","    # necessary to update the parameters of the model. Since all objects\n","    # with changing state are managed by the Checkpointer, training can be\n","    # stopped at any point, and will be resumed on next call.\n","    res_gen_brain.fit(\n","        epoch_counter=res_gen_brain.hparams.epoch_counter,\n","        train_set=datasets[\"train\"],\n","        valid_set=datasets[\"valid\"],\n","        train_loader_kwargs=hparams[\"train_dataloader_options\"],\n","        valid_loader_kwargs=hparams[\"test_dataloader_options\"],\n","    )\n","\n","    # Load the best checkpoint for evaluation\n","    test_stats = res_gen_brain.evaluate(\n","        test_set=datasets[\"test\"],\n","        min_key=\"PPL\",\n","        test_loader_kwargs=hparams[\"test_dataloader_options\"],\n","    )"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8t7tgkkbem9f","executionInfo":{"status":"ok","timestamp":1711988737712,"user_tz":240,"elapsed":186,"user":{"displayName":"pooneh mousavi","userId":"07980475777627459411"}},"outputId":"c550ae09-5a15-4caf-cc07-56675363108a"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["Writing train.py\n"]}]},{"cell_type":"markdown","source":["You should pay attention to how we generate training data suitable for teacher forcing by concatenating histories and replies together. We also need to have a custom padding (CustomPaddedBatch) to handle padded value based on the type of input (history_bos, input_ids, lm_labels, token_type_ids or history_token_type).\n","\n","Now, we train a model for  3 epochs. It takes around 20-30 minutes to train."],"metadata":{"id":"kSIeEBSrh9E2"}},{"cell_type":"code","execution_count":24,"metadata":{"id":"zx3p1jWfHo_F","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1711991982325,"user_tz":240,"elapsed":1915440,"user":{"displayName":"pooneh mousavi","userId":"07980475777627459411"}},"outputId":"d71c8b3d-2a76-43b7-f9e6-a22661a57a52"},"outputs":[{"output_type":"stream","name":"stdout","text":["config.json: 100% 665/665 [00:00<00:00, 2.81MB/s]\n","/usr/local/lib/python3.10/dist-packages/transformers/models/auto/modeling_auto.py:1595: FutureWarning: The class `AutoModelWithLMHead` is deprecated and will be removed in a future version. Please use `AutoModelForCausalLM` for causal language models, `AutoModelForMaskedLM` for masked language models and `AutoModelForSeq2SeqLM` for encoder-decoder models.\n","  warnings.warn(\n","model.safetensors: 100% 548M/548M [00:02<00:00, 249MB/s]\n","generation_config.json: 100% 124/124 [00:00<00:00, 642kB/s]\n","speechbrain.core - Beginning experiment!\n","speechbrain.core - Experiment folder: /content/results/train_with_gpt2/1995\n","speechbrain.core - Gradscaler enabled: False. Using precision: fp32.\n","speechbrain.core - ResGenBrain Model Statistics:\n","* Total Number of Trainable Parameters: 124.4M\n","* Total Number of Parameters: 124.4M\n","* Trainable Parameters represent 100.0000% of the total size.\n","speechbrain.utils.checkpoints - Would load a checkpoint here, but none found yet.\n","speechbrain.utils.epoch_loop - Going into epoch 1\n","100% 841/841 [07:41<00:00,  1.82it/s, train_loss=2.4]\n","  0% 0/185 [00:00<?, ?it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n","  1% 1/185 [00:00<01:44,  1.76it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n","  1% 2/185 [00:01<01:46,  1.71it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n","  2% 3/185 [00:01<01:31,  1.98it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n","  2% 4/185 [00:02<01:36,  1.88it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n","  3% 5/185 [00:02<01:40,  1.79it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n","  3% 6/185 [00:03<01:42,  1.75it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n","  4% 7/185 [00:03<01:42,  1.74it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n","  4% 8/185 [00:04<01:50,  1.60it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n","  5% 9/185 [00:05<01:44,  1.69it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n","  5% 10/185 [00:05<01:30,  1.93it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n","  6% 11/185 [00:05<01:23,  2.07it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n","  6% 12/185 [00:06<01:23,  2.07it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n","  7% 13/185 [00:06<01:19,  2.15it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n","  8% 14/185 [00:07<01:35,  1.79it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n","  8% 15/185 [00:08<01:27,  1.94it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n","  9% 16/185 [00:08<01:21,  2.07it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n","  9% 17/185 [00:08<01:21,  2.06it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 10% 18/185 [00:09<01:16,  2.17it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 10% 19/185 [00:09<01:12,  2.29it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 11% 20/185 [00:10<01:09,  2.37it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 11% 21/185 [00:10<01:07,  2.43it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 12% 22/185 [00:11<01:13,  2.22it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 12% 23/185 [00:11<01:10,  2.28it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 13% 24/185 [00:11<01:12,  2.22it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 14% 25/185 [00:12<01:12,  2.19it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 14% 26/185 [00:12<01:13,  2.16it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 15% 27/185 [00:13<01:18,  2.01it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 15% 28/185 [00:13<01:14,  2.10it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 16% 29/185 [00:14<01:11,  2.19it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 16% 30/185 [00:14<01:06,  2.32it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 17% 31/185 [00:15<01:07,  2.29it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 17% 32/185 [00:15<01:14,  2.05it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 18% 33/185 [00:16<01:16,  2.00it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 18% 34/185 [00:16<01:10,  2.13it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 19% 35/185 [00:17<01:05,  2.29it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 19% 36/185 [00:17<01:03,  2.33it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 20% 37/185 [00:17<01:06,  2.24it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 21% 38/185 [00:18<00:59,  2.47it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 21% 39/185 [00:18<00:59,  2.44it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 22% 40/185 [00:19<01:02,  2.30it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 22% 41/185 [00:19<01:00,  2.36it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 23% 42/185 [00:19<01:02,  2.30it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 23% 43/185 [00:20<01:03,  2.25it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 24% 44/185 [00:20<01:06,  2.14it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 24% 45/185 [00:21<01:09,  2.00it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 25% 46/185 [00:21<01:04,  2.16it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 25% 47/185 [00:22<01:06,  2.08it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 26% 48/185 [00:23<01:12,  1.90it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 26% 49/185 [00:23<01:14,  1.84it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 27% 50/185 [00:24<01:15,  1.79it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 28% 51/185 [00:24<01:11,  1.86it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 28% 52/185 [00:25<01:03,  2.08it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 29% 53/185 [00:25<01:06,  1.98it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 29% 54/185 [00:26<01:07,  1.93it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 30% 55/185 [00:26<01:07,  1.94it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 30% 56/185 [00:27<01:10,  1.82it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 31% 57/185 [00:27<01:14,  1.72it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 31% 58/185 [00:28<01:11,  1.78it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 32% 59/185 [00:28<01:06,  1.90it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 32% 60/185 [00:29<01:01,  2.03it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 33% 61/185 [00:29<01:01,  2.01it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 34% 62/185 [00:30<00:58,  2.09it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 34% 63/185 [00:30<00:59,  2.05it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 35% 64/185 [00:31<01:01,  1.96it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 35% 65/185 [00:31<00:58,  2.06it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 36% 66/185 [00:32<00:57,  2.07it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 36% 67/185 [00:32<00:57,  2.05it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 37% 68/185 [00:33<00:59,  1.97it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 37% 69/185 [00:33<00:59,  1.96it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 38% 70/185 [00:34<00:57,  2.00it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 38% 71/185 [00:34<00:53,  2.13it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 39% 72/185 [00:35<00:50,  2.24it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 39% 73/185 [00:35<00:53,  2.10it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 40% 74/185 [00:35<00:47,  2.36it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 41% 75/185 [00:36<00:47,  2.30it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 41% 76/185 [00:36<00:49,  2.20it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 42% 77/185 [00:37<00:46,  2.31it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 42% 78/185 [00:37<00:43,  2.44it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 43% 79/185 [00:38<00:45,  2.34it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 43% 80/185 [00:38<00:42,  2.45it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 44% 81/185 [00:38<00:43,  2.40it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 44% 82/185 [00:39<00:47,  2.17it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 45% 83/185 [00:40<00:53,  1.90it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 45% 84/185 [00:40<00:49,  2.02it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 46% 85/185 [00:41<00:49,  2.04it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 46% 86/185 [00:41<00:50,  1.97it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 47% 87/185 [00:42<00:53,  1.85it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 48% 88/185 [00:42<00:49,  1.96it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 48% 89/185 [00:43<00:50,  1.91it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 49% 90/185 [00:43<00:51,  1.86it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 49% 91/185 [00:44<00:51,  1.82it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 50% 92/185 [00:44<00:51,  1.81it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 50% 93/185 [00:45<00:48,  1.88it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 51% 94/185 [00:45<00:46,  1.94it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 51% 95/185 [00:46<00:41,  2.15it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 52% 96/185 [00:46<00:38,  2.33it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 52% 97/185 [00:47<00:36,  2.38it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 53% 98/185 [00:47<00:36,  2.39it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 54% 99/185 [00:48<00:40,  2.15it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 54% 100/185 [00:48<00:40,  2.08it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 55% 101/185 [00:48<00:37,  2.24it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 55% 102/185 [00:49<00:37,  2.18it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 56% 103/185 [00:49<00:38,  2.10it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 56% 104/185 [00:50<00:48,  1.66it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 57% 105/185 [00:51<00:50,  1.60it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 57% 106/185 [00:51<00:46,  1.70it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 58% 107/185 [00:52<00:42,  1.82it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 58% 108/185 [00:52<00:42,  1.81it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 59% 109/185 [00:53<00:41,  1.85it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 59% 110/185 [00:53<00:39,  1.89it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 60% 111/185 [00:54<00:42,  1.76it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 61% 112/185 [00:55<00:40,  1.80it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 61% 113/185 [00:55<00:38,  1.89it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 62% 114/185 [00:55<00:33,  2.11it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 62% 115/185 [00:56<00:33,  2.07it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 63% 116/185 [00:56<00:32,  2.09it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 63% 117/185 [00:57<00:32,  2.10it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 64% 118/185 [00:57<00:31,  2.12it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 64% 119/185 [00:58<00:31,  2.12it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 65% 120/185 [00:58<00:31,  2.06it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 65% 121/185 [00:59<00:29,  2.18it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 66% 122/185 [00:59<00:30,  2.07it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 66% 123/185 [01:00<00:29,  2.13it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 67% 124/185 [01:00<00:28,  2.14it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 68% 125/185 [01:01<00:27,  2.15it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 68% 126/185 [01:01<00:25,  2.32it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 69% 127/185 [01:01<00:24,  2.39it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 69% 128/185 [01:02<00:27,  2.05it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 70% 129/185 [01:03<00:27,  2.03it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 70% 130/185 [01:03<00:28,  1.91it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 71% 131/185 [01:04<00:30,  1.77it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 71% 132/185 [01:04<00:29,  1.78it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 72% 133/185 [01:05<00:28,  1.81it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 72% 134/185 [01:05<00:26,  1.90it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 73% 135/185 [01:06<00:26,  1.91it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 74% 136/185 [01:06<00:24,  2.03it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 74% 137/185 [01:07<00:22,  2.11it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 75% 138/185 [01:07<00:23,  2.04it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 75% 139/185 [01:08<00:23,  1.98it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 76% 140/185 [01:08<00:20,  2.20it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 76% 141/185 [01:09<00:19,  2.27it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 77% 142/185 [01:09<00:17,  2.48it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 77% 143/185 [01:09<00:17,  2.37it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 78% 144/185 [01:10<00:18,  2.22it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 78% 145/185 [01:10<00:19,  2.10it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 79% 146/185 [01:11<00:18,  2.08it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 79% 147/185 [01:11<00:16,  2.25it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 80% 148/185 [01:12<00:16,  2.27it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 81% 149/185 [01:12<00:16,  2.17it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 81% 150/185 [01:13<00:17,  2.03it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 82% 151/185 [01:13<00:15,  2.14it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 82% 152/185 [01:14<00:15,  2.09it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 83% 153/185 [01:14<00:16,  1.97it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 83% 154/185 [01:15<00:15,  2.07it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 84% 155/185 [01:15<00:14,  2.13it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 84% 156/185 [01:16<00:14,  2.07it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 85% 157/185 [01:16<00:13,  2.00it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 85% 158/185 [01:17<00:14,  1.92it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 86% 159/185 [01:17<00:14,  1.82it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 86% 160/185 [01:18<00:12,  2.02it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 87% 161/185 [01:18<00:11,  2.03it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 88% 162/185 [01:19<00:11,  1.99it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 88% 163/185 [01:19<00:10,  2.00it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 89% 164/185 [01:20<00:09,  2.20it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 89% 165/185 [01:20<00:08,  2.22it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 90% 166/185 [01:21<00:08,  2.14it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 90% 167/185 [01:21<00:08,  2.21it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 91% 168/185 [01:21<00:06,  2.43it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 91% 169/185 [01:22<00:07,  2.28it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 92% 170/185 [01:22<00:06,  2.34it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 92% 171/185 [01:22<00:05,  2.63it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 93% 172/185 [01:23<00:05,  2.41it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 94% 173/185 [01:23<00:05,  2.29it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 94% 174/185 [01:24<00:04,  2.26it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 95% 175/185 [01:24<00:04,  2.25it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 95% 176/185 [01:25<00:03,  2.28it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 96% 177/185 [01:25<00:03,  2.14it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 96% 178/185 [01:26<00:03,  2.21it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 97% 179/185 [01:26<00:02,  2.05it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 97% 180/185 [01:27<00:03,  1.67it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 98% 181/185 [01:28<00:02,  1.65it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 98% 182/185 [01:28<00:01,  1.81it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 99% 183/185 [01:29<00:01,  1.84it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 99% 184/185 [01:29<00:00,  2.05it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n","100% 185/185 [01:29<00:00,  2.06it/s]\n","sacrebleu - That's 100 lines that end in a tokenized period ('.')\n","sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n","sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n","sacrebleu - That's 100 lines that end in a tokenized period ('.')\n","sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n","sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n","speechbrain.utils.train_logger - epoch: 1, lr: 1.97e-04 - train loss: 2.40, train PPL: 11.05 - valid loss: 1.89, valid PPL: 6.60, valid BLEU_4: 5.91e-03, valid BLEU_2: 7.67e-02\n","speechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in /content/results/train_with_gpt2/1995/save/CKPT+2024-04-01+16-55-12+00\n","speechbrain.utils.epoch_loop - Going into epoch 2\n","100% 841/841 [07:37<00:00,  1.84it/s, train_loss=1.68]\n","  0% 0/185 [00:00<?, ?it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n","  1% 1/185 [00:00<01:46,  1.74it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n","  1% 2/185 [00:01<01:59,  1.53it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n","  2% 3/185 [00:01<01:44,  1.74it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n","  2% 4/185 [00:02<01:37,  1.86it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n","  3% 5/185 [00:02<01:35,  1.89it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n","  3% 6/185 [00:03<01:31,  1.95it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n","  4% 7/185 [00:03<01:32,  1.92it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n","  4% 8/185 [00:04<01:51,  1.59it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n","  5% 9/185 [00:05<01:47,  1.63it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n","  5% 10/185 [00:05<01:40,  1.74it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n","  6% 11/185 [00:06<01:35,  1.82it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n","  6% 12/185 [00:06<01:35,  1.81it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n","  7% 13/185 [00:07<01:39,  1.74it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n","  8% 14/185 [00:07<01:34,  1.81it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n","  8% 15/185 [00:08<01:31,  1.86it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n","  9% 16/185 [00:08<01:30,  1.87it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n","  9% 17/185 [00:09<01:29,  1.87it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 10% 18/185 [00:09<01:24,  1.97it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 10% 19/185 [00:10<01:23,  2.00it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 11% 20/185 [00:10<01:24,  1.95it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 11% 21/185 [00:11<01:23,  1.95it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 12% 22/185 [00:11<01:21,  1.99it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 12% 23/185 [00:12<01:20,  2.02it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 13% 24/185 [00:12<01:20,  2.01it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 14% 25/185 [00:13<01:22,  1.94it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 14% 26/185 [00:13<01:19,  2.01it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 15% 27/185 [00:14<01:24,  1.86it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 15% 28/185 [00:15<01:22,  1.91it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 16% 29/185 [00:15<01:19,  1.96it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 16% 30/185 [00:15<01:12,  2.13it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 17% 31/185 [00:16<01:21,  1.89it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 17% 32/185 [00:17<01:29,  1.70it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 18% 33/185 [00:17<01:32,  1.64it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 18% 34/185 [00:18<01:28,  1.72it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 19% 35/185 [00:19<01:31,  1.65it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 19% 36/185 [00:19<01:24,  1.77it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 20% 37/185 [00:20<01:31,  1.61it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 21% 38/185 [00:20<01:27,  1.68it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 21% 39/185 [00:21<01:27,  1.67it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 22% 40/185 [00:21<01:21,  1.78it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 22% 41/185 [00:22<01:12,  1.97it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 23% 42/185 [00:22<01:13,  1.93it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 23% 43/185 [00:23<01:12,  1.95it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 24% 44/185 [00:23<01:09,  2.02it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 24% 45/185 [00:24<01:12,  1.93it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 25% 46/185 [00:24<01:10,  1.97it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 25% 47/185 [00:25<01:06,  2.06it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 26% 48/185 [00:25<01:08,  2.01it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 26% 49/185 [00:26<01:09,  1.95it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 27% 50/185 [00:27<01:15,  1.79it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 28% 51/185 [00:27<01:09,  1.93it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 28% 52/185 [00:27<01:03,  2.10it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 29% 53/185 [00:28<01:02,  2.11it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 29% 54/185 [00:28<01:08,  1.90it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 30% 55/185 [00:29<01:11,  1.82it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 30% 56/185 [00:30<01:08,  1.89it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 31% 57/185 [00:30<01:08,  1.86it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 31% 58/185 [00:31<01:11,  1.76it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 32% 59/185 [00:31<01:06,  1.90it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 32% 60/185 [00:32<01:08,  1.83it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 33% 61/185 [00:32<01:10,  1.76it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 34% 62/185 [00:33<01:16,  1.62it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 34% 63/185 [00:34<01:09,  1.76it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 35% 64/185 [00:34<01:09,  1.73it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 35% 65/185 [00:35<01:05,  1.83it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 36% 66/185 [00:35<01:01,  1.92it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 36% 67/185 [00:36<01:04,  1.84it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 37% 68/185 [00:36<01:00,  1.92it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 37% 69/185 [00:37<01:04,  1.81it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 38% 70/185 [00:37<01:06,  1.72it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 38% 71/185 [00:38<01:04,  1.76it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 39% 72/185 [00:39<01:02,  1.81it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 39% 73/185 [00:39<00:59,  1.88it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 40% 74/185 [00:39<00:56,  1.95it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 41% 75/185 [00:40<01:00,  1.81it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 41% 76/185 [00:41<01:08,  1.60it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 42% 77/185 [00:42<01:12,  1.49it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 42% 78/185 [00:42<01:09,  1.53it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 43% 79/185 [00:43<01:08,  1.54it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 43% 80/185 [00:43<01:01,  1.70it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 44% 81/185 [00:44<00:53,  1.94it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 44% 82/185 [00:44<00:54,  1.89it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 45% 83/185 [00:45<00:51,  1.96it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 45% 84/185 [00:45<00:48,  2.07it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 46% 85/185 [00:46<00:49,  2.03it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 46% 86/185 [00:46<00:51,  1.92it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 47% 87/185 [00:47<00:56,  1.73it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 48% 88/185 [00:47<00:50,  1.93it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 48% 89/185 [00:48<00:49,  1.93it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 49% 90/185 [00:48<00:48,  1.97it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 49% 91/185 [00:49<00:47,  1.98it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 50% 92/185 [00:49<00:46,  2.00it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 50% 93/185 [00:50<00:43,  2.09it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 51% 94/185 [00:50<00:46,  1.97it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 51% 95/185 [00:51<00:46,  1.96it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 52% 96/185 [00:51<00:47,  1.89it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 52% 97/185 [00:52<00:47,  1.86it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 53% 98/185 [00:53<00:51,  1.69it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 54% 99/185 [00:53<00:51,  1.66it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 54% 100/185 [00:54<00:47,  1.78it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 55% 101/185 [00:54<00:44,  1.88it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 55% 102/185 [00:55<00:43,  1.91it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 56% 103/185 [00:55<00:46,  1.77it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 56% 104/185 [00:56<00:42,  1.89it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 57% 105/185 [00:56<00:42,  1.87it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 57% 106/185 [00:57<00:44,  1.78it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 58% 107/185 [00:57<00:40,  1.94it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 58% 108/185 [00:58<00:39,  1.95it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 59% 109/185 [00:59<00:40,  1.87it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 59% 110/185 [00:59<00:38,  1.92it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 60% 111/185 [01:00<00:38,  1.91it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 61% 112/185 [01:00<00:38,  1.90it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 61% 113/185 [01:00<00:34,  2.12it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 62% 114/185 [01:01<00:36,  1.96it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 62% 115/185 [01:02<00:34,  2.02it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 63% 116/185 [01:02<00:38,  1.79it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 63% 117/185 [01:03<00:36,  1.84it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 64% 118/185 [01:03<00:37,  1.79it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 64% 119/185 [01:04<00:40,  1.62it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 65% 120/185 [01:05<00:41,  1.55it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 65% 121/185 [01:05<00:41,  1.55it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 66% 122/185 [01:06<00:37,  1.70it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 66% 123/185 [01:06<00:33,  1.83it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 67% 124/185 [01:07<00:35,  1.73it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 68% 125/185 [01:07<00:32,  1.82it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 68% 126/185 [01:08<00:30,  1.91it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 69% 127/185 [01:09<00:31,  1.81it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 69% 128/185 [01:09<00:30,  1.87it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 70% 129/185 [01:09<00:27,  2.01it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 70% 130/185 [01:10<00:27,  1.97it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 71% 131/185 [01:10<00:26,  2.03it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 71% 132/185 [01:11<00:26,  2.02it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 72% 133/185 [01:11<00:25,  2.05it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 72% 134/185 [01:12<00:25,  2.03it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 73% 135/185 [01:12<00:24,  2.05it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 74% 136/185 [01:13<00:26,  1.82it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 74% 137/185 [01:14<00:24,  1.95it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 75% 138/185 [01:14<00:25,  1.81it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 75% 139/185 [01:15<00:24,  1.86it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 76% 140/185 [01:15<00:22,  2.04it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 76% 141/185 [01:16<00:22,  1.97it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 77% 142/185 [01:16<00:22,  1.92it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 77% 143/185 [01:17<00:23,  1.79it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 78% 144/185 [01:17<00:22,  1.83it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 78% 145/185 [01:18<00:23,  1.71it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 79% 146/185 [01:18<00:20,  1.89it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 79% 147/185 [01:19<00:20,  1.90it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 80% 148/185 [01:19<00:18,  2.01it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 81% 149/185 [01:20<00:18,  1.95it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 81% 150/185 [01:21<00:20,  1.69it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 82% 151/185 [01:21<00:17,  1.89it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 82% 152/185 [01:22<00:17,  1.87it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 83% 153/185 [01:22<00:17,  1.85it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 83% 154/185 [01:23<00:15,  2.05it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 84% 155/185 [01:23<00:17,  1.76it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 84% 156/185 [01:24<00:16,  1.79it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 85% 157/185 [01:24<00:14,  1.87it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 85% 158/185 [01:25<00:13,  2.05it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 86% 159/185 [01:25<00:13,  1.99it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 86% 160/185 [01:26<00:12,  2.03it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 87% 161/185 [01:26<00:11,  2.10it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 88% 162/185 [01:27<00:10,  2.16it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 88% 163/185 [01:27<00:10,  2.12it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 89% 164/185 [01:28<00:09,  2.11it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 89% 165/185 [01:28<00:10,  1.96it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 90% 166/185 [01:29<00:09,  1.93it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 90% 167/185 [01:29<00:08,  2.01it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 91% 168/185 [01:29<00:07,  2.19it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 91% 169/185 [01:30<00:07,  2.21it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 92% 170/185 [01:30<00:07,  2.13it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 92% 171/185 [01:31<00:06,  2.17it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 93% 172/185 [01:31<00:06,  2.10it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 94% 173/185 [01:32<00:05,  2.10it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 94% 174/185 [01:32<00:05,  2.11it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 95% 175/185 [01:33<00:04,  2.01it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 95% 176/185 [01:33<00:04,  1.89it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 96% 177/185 [01:34<00:04,  1.61it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 96% 178/185 [01:35<00:04,  1.66it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 97% 179/185 [01:35<00:03,  1.78it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 97% 180/185 [01:36<00:02,  1.79it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 98% 181/185 [01:36<00:02,  1.88it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 98% 182/185 [01:37<00:01,  1.69it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 99% 183/185 [01:38<00:01,  1.69it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 99% 184/185 [01:38<00:00,  1.86it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n","100% 185/185 [01:39<00:00,  1.86it/s]\n","sacrebleu - That's 100 lines that end in a tokenized period ('.')\n","sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n","sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n","sacrebleu - That's 100 lines that end in a tokenized period ('.')\n","sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n","sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n","speechbrain.nnet.schedulers - Changing lr from 0.0002 to 0.00018\n","speechbrain.utils.train_logger - epoch: 2, lr: 1.97e-04 - train loss: 1.68, train PPL: 5.36 - valid loss: 1.79, valid PPL: 5.98, valid BLEU_4: 4.87e-03, valid BLEU_2: 6.39e-02\n","speechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in /content/results/train_with_gpt2/1995/save/CKPT+2024-04-01+17-04-44+00\n","speechbrain.utils.checkpoints - Deleted checkpoint in /content/results/train_with_gpt2/1995/save/CKPT+2024-04-01+16-55-12+00\n","speechbrain.utils.epoch_loop - Going into epoch 3\n","100% 841/841 [07:39<00:00,  1.83it/s, train_loss=1.41]\n","  0% 0/185 [00:00<?, ?it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n","  1% 1/185 [00:00<02:00,  1.53it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n","  1% 2/185 [00:01<01:49,  1.67it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n","  2% 3/185 [00:01<01:41,  1.80it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n","  2% 4/185 [00:02<01:38,  1.83it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n","  3% 5/185 [00:02<01:37,  1.84it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n","  3% 6/185 [00:03<01:37,  1.83it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n","  4% 7/185 [00:03<01:37,  1.82it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n","  4% 8/185 [00:04<01:35,  1.86it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n","  5% 9/185 [00:04<01:31,  1.92it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n","  5% 10/185 [00:05<01:20,  2.18it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n","  6% 11/185 [00:05<01:14,  2.35it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n","  6% 12/185 [00:06<01:18,  2.21it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n","  7% 13/185 [00:06<01:26,  1.98it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n","  8% 14/185 [00:07<01:29,  1.92it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n","  8% 15/185 [00:07<01:30,  1.87it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n","  9% 16/185 [00:08<01:35,  1.77it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n","  9% 17/185 [00:08<01:33,  1.81it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 10% 18/185 [00:09<01:30,  1.84it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 10% 19/185 [00:10<01:31,  1.82it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 11% 20/185 [00:10<01:29,  1.85it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 11% 21/185 [00:11<01:24,  1.95it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 12% 22/185 [00:11<01:23,  1.95it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 12% 23/185 [00:11<01:17,  2.09it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 13% 24/185 [00:12<01:17,  2.07it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 14% 25/185 [00:12<01:18,  2.04it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 14% 26/185 [00:13<01:29,  1.78it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 15% 27/185 [00:14<01:29,  1.76it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 15% 28/185 [00:14<01:28,  1.78it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 16% 29/185 [00:15<01:28,  1.76it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 16% 30/185 [00:15<01:20,  1.92it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 17% 31/185 [00:16<01:14,  2.06it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 17% 32/185 [00:16<01:12,  2.10it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 18% 33/185 [00:17<01:09,  2.18it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 18% 34/185 [00:17<01:12,  2.08it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 19% 35/185 [00:18<01:16,  1.96it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 19% 36/185 [00:18<01:14,  2.01it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 20% 37/185 [00:19<01:14,  1.99it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 21% 38/185 [00:19<01:11,  2.05it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 21% 39/185 [00:20<01:13,  1.99it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 22% 40/185 [00:20<01:22,  1.75it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 22% 41/185 [00:21<01:20,  1.78it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 23% 42/185 [00:21<01:18,  1.82it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 23% 43/185 [00:22<01:10,  2.02it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 24% 44/185 [00:22<01:06,  2.12it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 24% 45/185 [00:23<01:07,  2.08it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 25% 46/185 [00:23<01:03,  2.19it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 25% 47/185 [00:23<00:57,  2.41it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 26% 48/185 [00:24<01:06,  2.07it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 26% 49/185 [00:25<01:13,  1.84it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 27% 50/185 [00:25<01:15,  1.79it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 28% 51/185 [00:26<01:08,  1.94it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 28% 52/185 [00:26<01:06,  2.00it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 29% 53/185 [00:27<01:15,  1.74it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 29% 54/185 [00:27<01:11,  1.83it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 30% 55/185 [00:28<01:09,  1.87it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 30% 56/185 [00:28<01:05,  1.96it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 31% 57/185 [00:29<01:08,  1.87it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 31% 58/185 [00:30<01:06,  1.92it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 32% 59/185 [00:30<01:05,  1.94it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 32% 60/185 [00:31<01:11,  1.74it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 33% 61/185 [00:31<01:16,  1.62it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 34% 62/185 [00:32<01:17,  1.59it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 34% 63/185 [00:33<01:09,  1.75it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 35% 64/185 [00:33<01:05,  1.84it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 35% 65/185 [00:34<01:06,  1.81it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 36% 66/185 [00:34<01:03,  1.89it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 36% 67/185 [00:35<01:03,  1.86it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 37% 68/185 [00:35<01:10,  1.65it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 37% 69/185 [00:36<01:04,  1.81it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 38% 70/185 [00:36<01:03,  1.81it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 38% 71/185 [00:37<01:00,  1.87it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 39% 72/185 [00:37<01:00,  1.88it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 39% 73/185 [00:38<01:01,  1.81it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 40% 74/185 [00:38<00:57,  1.93it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 41% 75/185 [00:39<00:59,  1.84it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 41% 76/185 [00:40<01:02,  1.74it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 42% 77/185 [00:40<01:06,  1.64it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 42% 78/185 [00:41<01:00,  1.76it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 43% 79/185 [00:41<01:00,  1.76it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 43% 80/185 [00:42<00:58,  1.81it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 44% 81/185 [00:42<00:52,  1.98it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 44% 82/185 [00:43<00:55,  1.86it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 45% 83/185 [00:43<00:53,  1.91it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 45% 84/185 [00:44<00:56,  1.80it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 46% 85/185 [00:45<00:54,  1.83it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 46% 86/185 [00:45<00:51,  1.93it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 47% 87/185 [00:46<00:49,  1.97it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 48% 88/185 [00:46<00:45,  2.15it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 48% 89/185 [00:46<00:46,  2.06it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 49% 90/185 [00:47<00:46,  2.06it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 49% 91/185 [00:47<00:48,  1.94it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 50% 92/185 [00:48<00:51,  1.79it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 50% 93/185 [00:49<00:49,  1.85it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 51% 94/185 [00:49<00:48,  1.87it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 51% 95/185 [00:50<00:47,  1.91it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 52% 96/185 [00:50<00:43,  2.04it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 52% 97/185 [00:51<00:42,  2.07it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 53% 98/185 [00:51<00:46,  1.89it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 54% 99/185 [00:52<00:47,  1.82it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 54% 100/185 [00:52<00:47,  1.78it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 55% 101/185 [00:53<00:44,  1.88it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 55% 102/185 [00:53<00:44,  1.85it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 56% 103/185 [00:54<00:46,  1.77it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 56% 104/185 [00:55<00:51,  1.56it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 57% 105/185 [00:55<00:50,  1.58it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 57% 106/185 [00:56<00:51,  1.53it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 58% 107/185 [00:57<00:44,  1.77it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 58% 108/185 [00:57<00:42,  1.80it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 59% 109/185 [00:58<00:43,  1.76it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 59% 110/185 [00:58<00:40,  1.83it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 60% 111/185 [00:59<00:41,  1.79it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 61% 112/185 [00:59<00:37,  1.95it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 61% 113/185 [01:00<00:35,  2.02it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 62% 114/185 [01:00<00:35,  1.99it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 62% 115/185 [01:01<00:33,  2.07it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 63% 116/185 [01:01<00:33,  2.08it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 63% 117/185 [01:01<00:32,  2.09it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 64% 118/185 [01:02<00:31,  2.15it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 64% 119/185 [01:02<00:30,  2.15it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 65% 120/185 [01:03<00:34,  1.89it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 65% 121/185 [01:04<00:34,  1.86it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 66% 122/185 [01:04<00:34,  1.81it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 66% 123/185 [01:05<00:34,  1.81it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 67% 124/185 [01:05<00:32,  1.89it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 68% 125/185 [01:06<00:32,  1.85it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 68% 126/185 [01:06<00:31,  1.86it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 69% 127/185 [01:07<00:34,  1.70it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 69% 128/185 [01:08<00:37,  1.54it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 70% 129/185 [01:09<00:37,  1.51it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 70% 130/185 [01:09<00:32,  1.68it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 71% 131/185 [01:09<00:30,  1.74it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 71% 132/185 [01:10<00:28,  1.85it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 72% 133/185 [01:10<00:27,  1.91it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 72% 134/185 [01:11<00:26,  1.92it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 73% 135/185 [01:12<00:26,  1.88it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 74% 136/185 [01:12<00:25,  1.95it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 74% 137/185 [01:12<00:23,  2.02it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 75% 138/185 [01:13<00:23,  1.96it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 75% 139/185 [01:13<00:23,  2.00it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 76% 140/185 [01:14<00:21,  2.06it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 76% 141/185 [01:14<00:21,  2.04it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 77% 142/185 [01:15<00:21,  2.01it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 77% 143/185 [01:15<00:20,  2.00it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 78% 144/185 [01:16<00:22,  1.86it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 78% 145/185 [01:17<00:21,  1.83it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 79% 146/185 [01:17<00:23,  1.63it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 79% 147/185 [01:18<00:22,  1.67it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 80% 148/185 [01:19<00:22,  1.62it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 81% 149/185 [01:19<00:21,  1.69it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 81% 150/185 [01:20<00:21,  1.62it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 82% 151/185 [01:20<00:21,  1.60it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 82% 152/185 [01:21<00:18,  1.74it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 83% 153/185 [01:21<00:17,  1.83it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 83% 154/185 [01:22<00:15,  2.00it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 84% 155/185 [01:22<00:15,  1.94it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 84% 156/185 [01:23<00:14,  2.01it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 85% 157/185 [01:23<00:13,  2.09it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 85% 158/185 [01:24<00:13,  2.07it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 86% 159/185 [01:24<00:13,  1.99it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 86% 160/185 [01:25<00:11,  2.09it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 87% 161/185 [01:25<00:12,  1.98it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 88% 162/185 [01:26<00:11,  1.98it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 88% 163/185 [01:26<00:11,  1.94it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 89% 164/185 [01:27<00:09,  2.10it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 89% 165/185 [01:27<00:09,  2.12it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 90% 166/185 [01:28<00:09,  2.00it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 90% 167/185 [01:28<00:08,  2.12it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 91% 168/185 [01:29<00:08,  2.07it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 91% 169/185 [01:29<00:09,  1.77it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 92% 170/185 [01:30<00:09,  1.62it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 92% 171/185 [01:31<00:08,  1.60it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 93% 172/185 [01:31<00:07,  1.63it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 94% 173/185 [01:32<00:06,  1.74it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 94% 174/185 [01:32<00:06,  1.73it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 95% 175/185 [01:33<00:05,  1.73it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 95% 176/185 [01:34<00:05,  1.79it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 96% 177/185 [01:34<00:04,  1.88it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 96% 178/185 [01:35<00:03,  1.86it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 97% 179/185 [01:35<00:03,  1.97it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 97% 180/185 [01:35<00:02,  1.96it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 98% 181/185 [01:36<00:01,  2.02it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 98% 182/185 [01:36<00:01,  2.06it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 99% 183/185 [01:37<00:00,  2.06it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 99% 184/185 [01:37<00:00,  2.20it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n","100% 185/185 [01:38<00:00,  1.88it/s]\n","sacrebleu - That's 100 lines that end in a tokenized period ('.')\n","sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n","sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n","sacrebleu - That's 100 lines that end in a tokenized period ('.')\n","sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n","sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n","speechbrain.nnet.schedulers - Changing lr from 0.00018 to 0.00016\n","speechbrain.utils.train_logger - epoch: 3, lr: 1.77e-04 - train loss: 1.41, train PPL: 4.10 - valid loss: 1.76, valid PPL: 5.83, valid BLEU_4: 5.05e-03, valid BLEU_2: 6.60e-02\n","speechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in /content/results/train_with_gpt2/1995/save/CKPT+2024-04-01+17-14-16+00\n","speechbrain.utils.checkpoints - Deleted checkpoint in /content/results/train_with_gpt2/1995/save/CKPT+2024-04-01+17-04-44+00\n","speechbrain.utils.checkpoints - Loading a checkpoint from /content/results/train_with_gpt2/1995/save/CKPT+2024-04-01+17-14-16+00\n","  0% 0/366 [00:00<?, ?it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n","  0% 1/366 [00:01<10:34,  1.74s/it]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n","  1% 2/366 [00:02<07:25,  1.22s/it]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n","  1% 3/366 [00:03<06:44,  1.12s/it]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n","  1% 4/366 [00:04<06:24,  1.06s/it]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n","  1% 5/366 [00:05<06:05,  1.01s/it]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n","  2% 6/366 [00:06<05:28,  1.10it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n","  2% 7/366 [00:06<05:03,  1.18it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n","  2% 8/366 [00:07<05:00,  1.19it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n","  2% 9/366 [00:08<05:04,  1.17it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n","  3% 10/366 [00:09<05:08,  1.15it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n","  3% 11/366 [00:10<04:57,  1.19it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n","  3% 12/366 [00:11<04:44,  1.25it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n","  4% 13/366 [00:12<05:09,  1.14it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n","  4% 14/366 [00:12<04:52,  1.20it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n","  4% 15/366 [00:13<05:08,  1.14it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n","  4% 16/366 [00:14<05:13,  1.11it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n","  5% 17/366 [00:15<04:47,  1.21it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n","  5% 18/366 [00:16<04:44,  1.22it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n","  5% 19/366 [00:17<04:58,  1.16it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n","  5% 20/366 [00:18<05:06,  1.13it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n","  6% 21/366 [00:18<04:45,  1.21it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n","  6% 22/366 [00:19<04:41,  1.22it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n","  6% 23/366 [00:20<04:38,  1.23it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n","  7% 24/366 [00:21<04:44,  1.20it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n","  7% 25/366 [00:22<04:44,  1.20it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n","  7% 26/366 [00:22<04:45,  1.19it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n","  7% 27/366 [00:23<04:28,  1.26it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n","  8% 28/366 [00:24<04:27,  1.26it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n","  8% 29/366 [00:25<05:05,  1.10it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n","  8% 30/366 [00:26<04:42,  1.19it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n","  8% 31/366 [00:27<04:45,  1.17it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n","  9% 32/366 [00:28<05:08,  1.08it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n","  9% 33/366 [00:29<04:59,  1.11it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n","  9% 34/366 [00:29<04:50,  1.14it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 10% 35/366 [00:30<04:37,  1.19it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 10% 36/366 [00:31<04:33,  1.21it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 10% 37/366 [00:32<04:08,  1.33it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 10% 38/366 [00:33<04:37,  1.18it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 11% 39/366 [00:33<04:38,  1.17it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 11% 40/366 [00:34<04:40,  1.16it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 11% 41/366 [00:35<04:40,  1.16it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 11% 42/366 [00:36<04:47,  1.13it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 12% 43/366 [00:37<04:51,  1.11it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 12% 44/366 [00:38<04:22,  1.23it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 12% 45/366 [00:39<04:21,  1.23it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 13% 46/366 [00:39<04:21,  1.22it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 13% 47/366 [00:40<04:09,  1.28it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 13% 48/366 [00:41<04:07,  1.29it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 13% 49/366 [00:41<03:51,  1.37it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 14% 50/366 [00:42<04:13,  1.25it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 14% 51/366 [00:43<04:17,  1.22it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 14% 52/366 [00:44<04:29,  1.16it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 14% 53/366 [00:45<04:33,  1.14it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 15% 54/366 [00:46<04:21,  1.20it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 15% 55/366 [00:47<04:01,  1.29it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 15% 56/366 [00:48<04:20,  1.19it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 16% 57/366 [00:48<04:29,  1.15it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 16% 58/366 [00:49<04:27,  1.15it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 16% 59/366 [00:50<04:10,  1.23it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 16% 60/366 [00:51<04:32,  1.12it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 17% 61/366 [00:52<04:13,  1.20it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 17% 62/366 [00:53<04:21,  1.16it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 17% 63/366 [00:54<04:32,  1.11it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 17% 64/366 [00:55<04:53,  1.03it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 18% 65/366 [00:56<04:54,  1.02it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 18% 66/366 [00:57<04:44,  1.06it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 18% 67/366 [00:58<04:31,  1.10it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 19% 68/366 [00:58<04:24,  1.13it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 19% 69/366 [00:59<04:06,  1.21it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 19% 70/366 [01:00<04:01,  1.23it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 19% 71/366 [01:01<04:06,  1.20it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 20% 72/366 [01:01<03:48,  1.29it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 20% 73/366 [01:02<03:42,  1.31it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 20% 74/366 [01:03<03:29,  1.39it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 20% 75/366 [01:04<03:37,  1.34it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 21% 76/366 [01:04<03:35,  1.35it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 21% 77/366 [01:05<03:47,  1.27it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 21% 78/366 [01:06<03:32,  1.36it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 22% 79/366 [01:07<03:49,  1.25it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 22% 80/366 [01:07<03:35,  1.33it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 22% 81/366 [01:08<03:37,  1.31it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 22% 82/366 [01:09<03:45,  1.26it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 23% 83/366 [01:10<04:04,  1.16it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 23% 84/366 [01:11<03:39,  1.28it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 23% 85/366 [01:12<04:07,  1.14it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 23% 86/366 [01:12<03:49,  1.22it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 24% 87/366 [01:13<03:58,  1.17it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 24% 88/366 [01:14<04:05,  1.13it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 24% 89/366 [01:15<03:43,  1.24it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 25% 90/366 [01:16<03:45,  1.22it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 25% 91/366 [01:16<03:34,  1.28it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 25% 92/366 [01:17<03:38,  1.25it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 25% 93/366 [01:18<03:43,  1.22it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 26% 94/366 [01:19<03:33,  1.28it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 26% 95/366 [01:20<03:27,  1.30it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 26% 96/366 [01:20<03:17,  1.37it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 27% 97/366 [01:21<03:11,  1.40it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 27% 98/366 [01:22<03:03,  1.46it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 27% 99/366 [01:22<03:09,  1.41it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 27% 100/366 [01:23<03:25,  1.29it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 28% 101/366 [01:24<03:35,  1.23it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 28% 102/366 [01:25<03:32,  1.24it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 28% 103/366 [01:26<03:30,  1.25it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 28% 104/366 [01:26<03:26,  1.27it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 29% 105/366 [01:27<03:26,  1.26it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 29% 106/366 [01:28<03:25,  1.26it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 29% 107/366 [01:29<03:30,  1.23it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 30% 108/366 [01:30<03:15,  1.32it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 30% 109/366 [01:30<03:18,  1.29it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 30% 110/366 [01:31<03:18,  1.29it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 30% 111/366 [01:32<03:31,  1.21it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 31% 112/366 [01:33<03:35,  1.18it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 31% 113/366 [01:34<03:30,  1.20it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 31% 114/366 [01:35<03:25,  1.23it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 31% 115/366 [01:36<03:41,  1.13it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 32% 116/366 [01:37<03:46,  1.10it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 32% 117/366 [01:37<03:46,  1.10it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 32% 118/366 [01:38<03:22,  1.23it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 33% 119/366 [01:39<03:16,  1.26it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 33% 120/366 [01:40<03:18,  1.24it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 33% 121/366 [01:40<03:09,  1.29it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 33% 122/366 [01:41<03:10,  1.28it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 34% 123/366 [01:42<03:25,  1.18it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 34% 124/366 [01:43<03:18,  1.22it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 34% 125/366 [01:44<03:25,  1.17it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 34% 126/366 [01:45<03:27,  1.16it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 35% 127/366 [01:45<03:11,  1.25it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 35% 128/366 [01:46<03:03,  1.30it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 35% 129/366 [01:47<03:26,  1.15it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 36% 130/366 [01:48<03:49,  1.03it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 36% 131/366 [01:49<03:27,  1.13it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 36% 132/366 [01:50<03:26,  1.13it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 36% 133/366 [01:51<03:22,  1.15it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 37% 134/366 [01:51<03:07,  1.23it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 37% 135/366 [01:52<03:13,  1.19it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 37% 136/366 [01:53<03:02,  1.26it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 37% 137/366 [01:54<02:57,  1.29it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 38% 138/366 [01:55<03:05,  1.23it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 38% 139/366 [01:56<03:17,  1.15it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 38% 140/366 [01:57<03:15,  1.15it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 39% 141/366 [01:57<03:20,  1.12it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 39% 142/366 [01:58<03:14,  1.15it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 39% 143/366 [01:59<03:36,  1.03it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 39% 144/366 [02:01<03:42,  1.00s/it]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 40% 145/366 [02:01<03:08,  1.17it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 40% 146/366 [02:02<03:00,  1.22it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 40% 147/366 [02:03<02:59,  1.22it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 40% 148/366 [02:04<03:12,  1.13it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 41% 149/366 [02:04<02:47,  1.29it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 41% 150/366 [02:05<02:36,  1.38it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 41% 151/366 [02:06<02:50,  1.26it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 42% 152/366 [02:07<03:09,  1.13it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 42% 153/366 [02:08<03:11,  1.11it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 42% 154/366 [02:09<03:09,  1.12it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 42% 155/366 [02:09<03:00,  1.17it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 43% 156/366 [02:10<03:00,  1.17it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 43% 157/366 [02:11<02:49,  1.23it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 43% 158/366 [02:12<02:47,  1.24it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 43% 159/366 [02:13<02:52,  1.20it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 44% 160/366 [02:14<02:55,  1.17it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 44% 161/366 [02:14<02:46,  1.23it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 44% 162/366 [02:15<02:47,  1.22it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 45% 163/366 [02:16<02:49,  1.20it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 45% 164/366 [02:17<03:02,  1.11it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 45% 165/366 [02:18<02:57,  1.13it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 45% 166/366 [02:19<03:03,  1.09it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 46% 167/366 [02:20<03:08,  1.05it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 46% 168/366 [02:21<03:20,  1.01s/it]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 46% 169/366 [02:22<03:03,  1.07it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 46% 170/366 [02:23<03:07,  1.04it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 47% 171/366 [02:24<03:19,  1.02s/it]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 47% 172/366 [02:25<02:56,  1.10it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 47% 173/366 [02:25<02:44,  1.17it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 48% 174/366 [02:26<02:42,  1.18it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 48% 175/366 [02:27<02:28,  1.29it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 48% 176/366 [02:28<02:27,  1.29it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 48% 177/366 [02:28<02:22,  1.33it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 49% 178/366 [02:29<02:24,  1.30it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 49% 179/366 [02:30<02:20,  1.34it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 49% 180/366 [02:30<02:13,  1.39it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 49% 181/366 [02:31<02:05,  1.47it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 50% 182/366 [02:32<02:04,  1.48it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 50% 183/366 [02:32<01:58,  1.55it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 50% 184/366 [02:33<02:15,  1.34it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 51% 185/366 [02:34<02:20,  1.29it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 51% 186/366 [02:35<02:12,  1.36it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 51% 187/366 [02:36<02:34,  1.16it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 51% 188/366 [02:37<02:26,  1.21it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 52% 189/366 [02:37<02:21,  1.25it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 52% 190/366 [02:38<02:15,  1.30it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 52% 191/366 [02:39<02:15,  1.29it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 52% 192/366 [02:40<02:18,  1.25it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 53% 193/366 [02:41<02:20,  1.23it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 53% 194/366 [02:41<02:16,  1.26it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 53% 195/366 [02:42<02:24,  1.18it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 54% 196/366 [02:43<02:15,  1.25it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 54% 197/366 [02:44<02:09,  1.30it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 54% 198/366 [02:44<02:04,  1.35it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 54% 199/366 [02:45<02:12,  1.26it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 55% 200/366 [02:46<02:14,  1.23it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 55% 201/366 [02:47<02:20,  1.17it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 55% 202/366 [02:48<02:31,  1.08it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 55% 203/366 [02:49<02:24,  1.13it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 56% 204/366 [02:50<02:18,  1.17it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 56% 205/366 [02:51<02:15,  1.19it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 56% 206/366 [02:51<02:08,  1.24it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 57% 207/366 [02:52<02:09,  1.23it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 57% 208/366 [02:53<02:02,  1.29it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 57% 209/366 [02:54<02:09,  1.22it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 57% 210/366 [02:55<02:06,  1.23it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 58% 211/366 [02:55<02:06,  1.22it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 58% 212/366 [02:56<02:07,  1.21it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 58% 213/366 [02:57<02:02,  1.25it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 58% 214/366 [02:58<02:14,  1.13it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 59% 215/366 [02:59<02:18,  1.09it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 59% 216/366 [03:00<02:28,  1.01it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 59% 217/366 [03:01<02:33,  1.03s/it]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 60% 218/366 [03:02<02:22,  1.04it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 60% 219/366 [03:03<02:24,  1.02it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 60% 220/366 [03:04<02:23,  1.02it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 60% 221/366 [03:05<02:09,  1.12it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 61% 222/366 [03:06<02:07,  1.13it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 61% 223/366 [03:07<02:07,  1.12it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 61% 224/366 [03:07<01:53,  1.25it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 61% 225/366 [03:08<01:57,  1.20it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 62% 226/366 [03:09<01:49,  1.28it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 62% 227/366 [03:10<01:55,  1.20it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 62% 228/366 [03:10<01:54,  1.20it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 63% 229/366 [03:11<01:58,  1.16it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 63% 230/366 [03:12<01:56,  1.17it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 63% 231/366 [03:13<01:49,  1.23it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 63% 232/366 [03:14<01:49,  1.23it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 64% 233/366 [03:15<01:53,  1.18it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 64% 234/366 [03:16<01:50,  1.20it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 64% 235/366 [03:16<01:45,  1.24it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 64% 236/366 [03:17<01:40,  1.29it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 65% 237/366 [03:18<01:43,  1.25it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 65% 238/366 [03:19<01:57,  1.09it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 65% 239/366 [03:20<01:46,  1.20it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 66% 240/366 [03:20<01:44,  1.21it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 66% 241/366 [03:21<01:42,  1.22it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 66% 242/366 [03:23<01:56,  1.07it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 66% 243/366 [03:23<01:54,  1.07it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 67% 244/366 [03:24<01:51,  1.10it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 67% 245/366 [03:25<01:52,  1.08it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 67% 246/366 [03:26<01:46,  1.13it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 67% 247/366 [03:27<01:41,  1.17it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 68% 248/366 [03:28<01:40,  1.17it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 68% 249/366 [03:28<01:35,  1.22it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 68% 250/366 [03:29<01:32,  1.26it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 69% 251/366 [03:30<01:32,  1.24it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 69% 252/366 [03:31<01:29,  1.27it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 69% 253/366 [03:32<01:30,  1.25it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 69% 254/366 [03:33<01:35,  1.18it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 70% 255/366 [03:33<01:30,  1.23it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 70% 256/366 [03:34<01:31,  1.21it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 70% 257/366 [03:35<01:40,  1.08it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 70% 258/366 [03:36<01:39,  1.08it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 71% 259/366 [03:37<01:41,  1.06it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 71% 260/366 [03:38<01:31,  1.16it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 71% 261/366 [03:39<01:28,  1.18it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 72% 262/366 [03:40<01:32,  1.12it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 72% 263/366 [03:40<01:27,  1.17it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 72% 264/366 [03:41<01:30,  1.13it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 72% 265/366 [03:42<01:22,  1.23it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 73% 266/366 [03:43<01:13,  1.36it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 73% 267/366 [03:43<01:10,  1.40it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 73% 268/366 [03:44<01:11,  1.37it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 73% 269/366 [03:45<01:13,  1.33it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 74% 270/366 [03:45<01:06,  1.44it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 74% 271/366 [03:46<01:11,  1.33it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 74% 272/366 [03:47<01:17,  1.21it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 75% 273/366 [03:48<01:19,  1.17it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 75% 274/366 [03:49<01:10,  1.30it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 75% 275/366 [03:50<01:11,  1.27it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 75% 276/366 [03:50<01:12,  1.24it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 76% 277/366 [03:51<01:16,  1.17it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 76% 278/366 [03:52<01:11,  1.23it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 76% 279/366 [03:53<01:07,  1.29it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 77% 280/366 [03:54<01:05,  1.31it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 77% 281/366 [03:54<00:58,  1.44it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 77% 282/366 [03:55<01:01,  1.37it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 77% 283/366 [03:56<01:04,  1.29it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 78% 284/366 [03:57<01:03,  1.29it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 78% 285/366 [03:58<01:08,  1.19it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 78% 286/366 [03:59<01:11,  1.12it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 78% 287/366 [03:59<01:10,  1.12it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 79% 288/366 [04:01<01:14,  1.05it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 79% 289/366 [04:01<01:08,  1.12it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 79% 290/366 [04:02<01:07,  1.12it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 80% 291/366 [04:03<01:11,  1.05it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 80% 292/366 [04:04<01:16,  1.03s/it]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 80% 293/366 [04:05<01:13,  1.01s/it]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 80% 294/366 [04:07<01:14,  1.04s/it]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 81% 295/366 [04:07<01:08,  1.03it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 81% 296/366 [04:08<01:04,  1.08it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 81% 297/366 [04:09<01:02,  1.10it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 81% 298/366 [04:10<00:54,  1.24it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 82% 299/366 [04:10<00:55,  1.21it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 82% 300/366 [04:11<00:54,  1.20it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 82% 301/366 [04:12<00:54,  1.20it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 83% 302/366 [04:13<00:51,  1.24it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 83% 303/366 [04:14<00:53,  1.18it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 83% 304/366 [04:15<00:49,  1.26it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 83% 305/366 [04:15<00:47,  1.29it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 84% 306/366 [04:16<00:47,  1.26it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 84% 307/366 [04:17<00:48,  1.22it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 84% 308/366 [04:18<00:44,  1.31it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 84% 309/366 [04:18<00:41,  1.38it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 85% 310/366 [04:19<00:46,  1.21it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 85% 311/366 [04:20<00:44,  1.24it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 85% 312/366 [04:21<00:41,  1.29it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 86% 313/366 [04:22<00:41,  1.27it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 86% 314/366 [04:22<00:41,  1.26it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 86% 315/366 [04:23<00:45,  1.13it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 86% 316/366 [04:24<00:43,  1.14it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 87% 317/366 [04:25<00:43,  1.12it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 87% 318/366 [04:26<00:46,  1.04it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 87% 319/366 [04:27<00:43,  1.07it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 87% 320/366 [04:28<00:37,  1.22it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 88% 321/366 [04:29<00:36,  1.23it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 88% 322/366 [04:29<00:33,  1.30it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 88% 323/366 [04:30<00:38,  1.11it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 89% 324/366 [04:31<00:38,  1.09it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 89% 325/366 [04:32<00:36,  1.13it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 89% 326/366 [04:33<00:33,  1.21it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 89% 327/366 [04:34<00:36,  1.07it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 90% 328/366 [04:35<00:35,  1.06it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 90% 329/366 [04:36<00:31,  1.16it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 90% 330/366 [04:37<00:31,  1.15it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 90% 331/366 [04:38<00:30,  1.16it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 91% 332/366 [04:39<00:31,  1.08it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 91% 333/366 [04:39<00:28,  1.17it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 91% 334/366 [04:40<00:28,  1.13it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 92% 335/366 [04:41<00:27,  1.12it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 92% 336/366 [04:42<00:27,  1.09it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 92% 337/366 [04:43<00:26,  1.10it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 92% 338/366 [04:44<00:26,  1.06it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 93% 339/366 [04:45<00:24,  1.10it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 93% 340/366 [04:46<00:24,  1.08it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 93% 341/366 [04:47<00:22,  1.09it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 93% 342/366 [04:47<00:20,  1.16it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 94% 343/366 [04:48<00:19,  1.17it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 94% 344/366 [04:49<00:18,  1.20it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 94% 345/366 [04:50<00:18,  1.14it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 95% 346/366 [04:51<00:17,  1.12it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 95% 347/366 [04:52<00:16,  1.14it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 95% 348/366 [04:53<00:15,  1.17it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 95% 349/366 [04:53<00:14,  1.21it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 96% 350/366 [04:54<00:13,  1.18it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 96% 351/366 [04:55<00:12,  1.21it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 96% 352/366 [04:56<00:12,  1.12it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 96% 353/366 [04:57<00:10,  1.19it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 97% 354/366 [04:58<00:10,  1.14it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 97% 355/366 [04:59<00:10,  1.08it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 97% 356/366 [05:00<00:09,  1.07it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 98% 357/366 [05:01<00:07,  1.14it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 98% 358/366 [05:01<00:07,  1.11it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 98% 359/366 [05:02<00:06,  1.15it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 98% 360/366 [05:03<00:04,  1.20it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 99% 361/366 [05:04<00:04,  1.23it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 99% 362/366 [05:05<00:03,  1.19it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 99% 363/366 [05:05<00:02,  1.24it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"," 99% 364/366 [05:06<00:01,  1.25it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n","100% 365/366 [05:07<00:00,  1.35it/s]Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n","100% 366/366 [05:08<00:00,  1.19it/s]\n","sacrebleu - That's 100 lines that end in a tokenized period ('.')\n","sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n","sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n","sacrebleu - That's 100 lines that end in a tokenized period ('.')\n","sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n","sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n","speechbrain.utils.train_logger - Epoch loaded: 3 - test loss: 1.73, test PPL: 5.62, test BLEU_4: 2.85e-03, test BLEU_2: 4.43e-02\n"]}],"source":["!rm -rf results\n","!python train.py hparams_gpt2.yaml --data_folder='/content/data_dir' --device='cuda:0' --number_of_epochs=3 --batch_size=8\n"]},{"cell_type":"markdown","metadata":{"id":"NjFo7OrLSUjy"},"source":["\n","After training the model for 3 epochs on just 1000 data points, we achieved a test loss of 1.73, PL: 5.62,  BLEU_4: 2.85e-03, and  BLEU_2: 4.43e-02. The full recipe in Speechbrain, including other LLMs for fine-tuning  such as LLama2, can be found . [here](https://github.com/speechbrain/speechbrain/tree/develop/recipes/MultiWOZ/response_generation).\n"]},{"cell_type":"markdown","source":["### **Step 4: Inference**\n","\n","At this point, we can use the trained response generator. For this type of model, Speechbrain made available some classes (take a look [here](https://github.com/speechbrain/speechbrain/blob/develop/speechbrain/inference/text.py)) such as the https://github.com/speechbrain/speechbrain/blob/develop/speechbrain/inference/text.py one that can make inference easier. For instance, we can have a chatbot with a pre-trained GPT model hosted in our HuggingFace repository in only a few lines of code."],"metadata":{"id":"N45qFQCqlJfR"}},{"cell_type":"code","source":["from speechbrain.inference.text import GPTResponseGenerator\n","res_gen_model = GPTResponseGenerator.from_hparams(source=\"speechbrain/MultiWOZ-GPT-Response_Generation\", savedir=\"pretrained_models/MultiWOZ-GPT-Response_Generation\", pymodule_file=\"custom.py\")\n","print(\"Hi,How could I help you today?\", end=\"\\n\")\n","i=0\n","while i<10:\n","  turn = input()\n","  response = res_gen_model.generate_response(turn)\n","  print(response, end=\"\\n\")\n","  i+=1\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-qDbLlZVmNTo","executionInfo":{"status":"ok","timestamp":1711992512223,"user_tz":240,"elapsed":117438,"user":{"displayName":"pooneh mousavi","userId":"07980475777627459411"}},"outputId":"9613f4c8-1e04-43bd-ba86-ae4c30e8e21d"},"execution_count":27,"outputs":[{"output_type":"stream","name":"stderr","text":["WARNING:speechbrain.lobes.models.huggingface_transformers.huggingface:speechbrain.lobes.models.huggingface_transformers.huggingface - GPT2LMHeadModel is frozen.\n","WARNING:speechbrain.lobes.models.huggingface_transformers.gpt:huggingface_GPT - GPT  is frozen.\n"]},{"name":"stdout","output_type":"stream","text":["Hi,How could I help you today?\n","I want to book a table for dinner.\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"]},{"name":"stdout","output_type":"stream","text":["what type of food would you like?\n","I prefer italian food\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"]},{"name":"stdout","output_type":"stream","text":["there are 22 results. is there a certain area you would like to dine in?\n","near downtown\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"]},{"name":"stdout","output_type":"stream","text":["there are 2 expensive italian restaurants in the city centre, would you prefer a cheap or moderate price range?\n","cheap would be better\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"]},{"name":"stdout","output_type":"stream","text":["there are 2 cheap italian restaurants in the centre. would you like me to book 1 for you?\n","yes. thanks\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"]},{"name":"stdout","output_type":"stream","text":["what day and time would you like the reservation for?\n","april 3rd at 6 PM\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"]},{"name":"stdout","output_type":"stream","text":["what time would you like to dine?\n","6 PM\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"]},{"name":"stdout","output_type":"stream","text":["what time would you like to dine?\n","At 6 PM\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"]},{"name":"stdout","output_type":"stream","text":["booking was successful for la margherita. the table will be reserved for 15 minutes. reference number is : 1y3lqe5. anything else i can help with?\n","No. That is all thanks.\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"]},{"name":"stdout","output_type":"stream","text":["have a good day.\n","You too.\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":["thank you for using our services.\n"]}]}],"metadata":{"colab":{"provenance":[],"toc_visible":true},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"20da3d8065b74ede8716220e59cb07f7":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_e4704b95bb8349acb04b27c4af999c5b","IPY_MODEL_6121c7d4f1a8480a8d2ddef053fca228","IPY_MODEL_6daf60831ce848bca1c4f8b7a6669484"],"layout":"IPY_MODEL_61dbaed269a046828b32eb7059f3cbb9"}},"e4704b95bb8349acb04b27c4af999c5b":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_e0e63e59ba0e4a2ab5bf54d6207b462b","placeholder":"​","style":"IPY_MODEL_da2fc61e45de4fa5831c625d7594cc07","value":"config.json: 100%"}},"6121c7d4f1a8480a8d2ddef053fca228":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_d72f15aaa40e4d8ba518a9f2f1c11e41","max":665,"min":0,"orientation":"horizontal","style":"IPY_MODEL_68e9ac63395a4f4b8666f2474b56bec6","value":665}},"6daf60831ce848bca1c4f8b7a6669484":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_adad2ae6407d48f2a8a31050d439d92b","placeholder":"​","style":"IPY_MODEL_1d9b65821da1420587c38eb0994ae7e4","value":" 665/665 [00:00&lt;00:00, 25.3kB/s]"}},"61dbaed269a046828b32eb7059f3cbb9":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e0e63e59ba0e4a2ab5bf54d6207b462b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"da2fc61e45de4fa5831c625d7594cc07":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"d72f15aaa40e4d8ba518a9f2f1c11e41":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"68e9ac63395a4f4b8666f2474b56bec6":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"adad2ae6407d48f2a8a31050d439d92b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1d9b65821da1420587c38eb0994ae7e4":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"f841c313b7f24ff0833a7975fbb34fe7":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_f951c371c47e4ce28eb00f15f312d6cc","IPY_MODEL_0a10985c24ba403f913500896641c3c3","IPY_MODEL_b37d6582c06c404eb06c0d47aea4ee13"],"layout":"IPY_MODEL_a0e4a2f282704cb0ae3fdbd01a199166"}},"f951c371c47e4ce28eb00f15f312d6cc":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_76ac4042a5a54b00a9417995c17012a8","placeholder":"​","style":"IPY_MODEL_8db732949b614047918ab5efa2ab8c8a","value":"model.safetensors: 100%"}},"0a10985c24ba403f913500896641c3c3":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_450394b1b0254de4bb492bfce2ef07ec","max":548105171,"min":0,"orientation":"horizontal","style":"IPY_MODEL_8997d71e6bb243cb8a5bc0a4c78e6de2","value":548105171}},"b37d6582c06c404eb06c0d47aea4ee13":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_d7378c4cc7454ba998aeb7c106d97bcc","placeholder":"​","style":"IPY_MODEL_7393b6cc95b744aba89c47b1830e9e82","value":" 548M/548M [00:04&lt;00:00, 96.9MB/s]"}},"a0e4a2f282704cb0ae3fdbd01a199166":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"76ac4042a5a54b00a9417995c17012a8":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8db732949b614047918ab5efa2ab8c8a":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"450394b1b0254de4bb492bfce2ef07ec":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8997d71e6bb243cb8a5bc0a4c78e6de2":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"d7378c4cc7454ba998aeb7c106d97bcc":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7393b6cc95b744aba89c47b1830e9e82":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"089777b581d44dc5906c1f4ddd3a9d0d":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_8ec838df7fc149edbca79f37cc6bc3a5","IPY_MODEL_cb4e377006004b2db63c89dbeda500ca","IPY_MODEL_e34e231b08c34ddabb774ea5f32267dc"],"layout":"IPY_MODEL_358ad6ab63d8484f9bfd6cfe34a2b6db"}},"8ec838df7fc149edbca79f37cc6bc3a5":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_1d59a1c156ca473780bb921491529707","placeholder":"​","style":"IPY_MODEL_f35017f3c86d4a7f84cb49315ca8e0a6","value":"generation_config.json: 100%"}},"cb4e377006004b2db63c89dbeda500ca":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_ded0f928dd094454b4b82614bcb65586","max":124,"min":0,"orientation":"horizontal","style":"IPY_MODEL_64b756ea2cc5401291124f0d4af04554","value":124}},"e34e231b08c34ddabb774ea5f32267dc":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_9e472fab69644572af50d5057d2a0af5","placeholder":"​","style":"IPY_MODEL_ecf2a7cf461746fc8089a403a036715c","value":" 124/124 [00:00&lt;00:00, 3.01kB/s]"}},"358ad6ab63d8484f9bfd6cfe34a2b6db":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1d59a1c156ca473780bb921491529707":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f35017f3c86d4a7f84cb49315ca8e0a6":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"ded0f928dd094454b4b82614bcb65586":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"64b756ea2cc5401291124f0d4af04554":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"9e472fab69644572af50d5057d2a0af5":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ecf2a7cf461746fc8089a403a036715c":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"2f78c0e702d54b3a96a9bdcbdd20422e":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_621fbaa2beb845b3809f696afbbac794","IPY_MODEL_cb848f587e7b4eba9ae597319a94c25a","IPY_MODEL_541fc23b9e3f496fa10d39487dfa5476"],"layout":"IPY_MODEL_15fa4e99cc644c3db2643ea76431cca2"}},"621fbaa2beb845b3809f696afbbac794":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_91d6e20a21eb4257ab000004fe8fb011","placeholder":"​","style":"IPY_MODEL_154ec8478f214f6d9d68eaf27d5fb9e3","value":"tokenizer_config.json: 100%"}},"cb848f587e7b4eba9ae597319a94c25a":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_aef6c7f15ac141759cba088b77b0cc93","max":26,"min":0,"orientation":"horizontal","style":"IPY_MODEL_415554f78eb949d7ab375a351c574ca7","value":26}},"541fc23b9e3f496fa10d39487dfa5476":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_2b5167edbc3b4309817b2500f6ac307d","placeholder":"​","style":"IPY_MODEL_6e01afb1e0ee4384945867fbdcffb203","value":" 26.0/26.0 [00:00&lt;00:00, 624B/s]"}},"15fa4e99cc644c3db2643ea76431cca2":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"91d6e20a21eb4257ab000004fe8fb011":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"154ec8478f214f6d9d68eaf27d5fb9e3":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"aef6c7f15ac141759cba088b77b0cc93":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"415554f78eb949d7ab375a351c574ca7":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"2b5167edbc3b4309817b2500f6ac307d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6e01afb1e0ee4384945867fbdcffb203":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"7e6aeae67ece43438b026b00584ab333":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_4c89484d51dc471fbc84ca8818cc3482","IPY_MODEL_80cdfa75fe1f430ba932e4a9752395a8","IPY_MODEL_5f8f791413694498aa7d280ba13cffe7"],"layout":"IPY_MODEL_2263769cbe6f40a5acfa9b630c1fe3ce"}},"4c89484d51dc471fbc84ca8818cc3482":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_d0e4730a60fa4b72aa44e00cc64f4e50","placeholder":"​","style":"IPY_MODEL_4211317ecaea4a6f9fa4d7e8324c4367","value":"vocab.json: 100%"}},"80cdfa75fe1f430ba932e4a9752395a8":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_89bf1fefe38d4545a71142e705fbbeef","max":1042301,"min":0,"orientation":"horizontal","style":"IPY_MODEL_fc6d954b0cea424683caaf5d681a811c","value":1042301}},"5f8f791413694498aa7d280ba13cffe7":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_d76186287aa4425e9b05dc6450d25d02","placeholder":"​","style":"IPY_MODEL_643550e06e234cbbba976e37e6f86d2f","value":" 1.04M/1.04M [00:00&lt;00:00, 4.06MB/s]"}},"2263769cbe6f40a5acfa9b630c1fe3ce":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d0e4730a60fa4b72aa44e00cc64f4e50":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4211317ecaea4a6f9fa4d7e8324c4367":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"89bf1fefe38d4545a71142e705fbbeef":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"fc6d954b0cea424683caaf5d681a811c":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"d76186287aa4425e9b05dc6450d25d02":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"643550e06e234cbbba976e37e6f86d2f":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"5f3faae1ba1c4a4b8027cc2d75730f11":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_e6d6ad2f4abe4b379fee77ddbf2775d4","IPY_MODEL_b8a4d67ed7924b0286645066b9438313","IPY_MODEL_f01a7f79746f4aa4a081518980e9e28b"],"layout":"IPY_MODEL_821a9731ef16419783af46afa3a9efa2"}},"e6d6ad2f4abe4b379fee77ddbf2775d4":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_8956d05ab6244100b1921e4538445af7","placeholder":"​","style":"IPY_MODEL_84bf0c771cbd48abb3a9b84a305f28be","value":"merges.txt: 100%"}},"b8a4d67ed7924b0286645066b9438313":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_7a0b5f8b635b4778b82f105a71c87575","max":456318,"min":0,"orientation":"horizontal","style":"IPY_MODEL_4d980d77e3a1495f9ec9ea1e7c295325","value":456318}},"f01a7f79746f4aa4a081518980e9e28b":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_a4c2bb321a69439ca6668cadf94062ff","placeholder":"​","style":"IPY_MODEL_462f217c245646868b4f07e4c8789faa","value":" 456k/456k [00:00&lt;00:00, 6.89MB/s]"}},"821a9731ef16419783af46afa3a9efa2":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8956d05ab6244100b1921e4538445af7":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"84bf0c771cbd48abb3a9b84a305f28be":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"7a0b5f8b635b4778b82f105a71c87575":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4d980d77e3a1495f9ec9ea1e7c295325":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"a4c2bb321a69439ca6668cadf94062ff":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"462f217c245646868b4f07e4c8789faa":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"84515f2df6ba4c06ba112e777cf88324":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_1a3c2347b69f41b39580b280ef76497a","IPY_MODEL_7c7f6799f6e648e8b5aa376914176e5a","IPY_MODEL_91fd048c13164740be0530b1a07cbfdb"],"layout":"IPY_MODEL_02d34838586446279ba5344a7b16a2b7"}},"1a3c2347b69f41b39580b280ef76497a":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_20a399879f1045adbc7d49e6bf472cb9","placeholder":"​","style":"IPY_MODEL_c8e6d0a0f8274873ae42a83f0084be44","value":"tokenizer.json: 100%"}},"7c7f6799f6e648e8b5aa376914176e5a":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_e976167922ae4343ad1b4b70cd7b2720","max":1355256,"min":0,"orientation":"horizontal","style":"IPY_MODEL_c5d00235382b489c9477fdf87d9f5367","value":1355256}},"91fd048c13164740be0530b1a07cbfdb":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_33abc0f470ae4fdf9ad04b4d05878ede","placeholder":"​","style":"IPY_MODEL_de7fe412b3e44f1ba2be319b44582c7f","value":" 1.36M/1.36M [00:00&lt;00:00, 4.50MB/s]"}},"02d34838586446279ba5344a7b16a2b7":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"20a399879f1045adbc7d49e6bf472cb9":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c8e6d0a0f8274873ae42a83f0084be44":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"e976167922ae4343ad1b4b70cd7b2720":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c5d00235382b489c9477fdf87d9f5367":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"33abc0f470ae4fdf9ad04b4d05878ede":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"de7fe412b3e44f1ba2be319b44582c7f":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"nbformat":4,"nbformat_minor":0}