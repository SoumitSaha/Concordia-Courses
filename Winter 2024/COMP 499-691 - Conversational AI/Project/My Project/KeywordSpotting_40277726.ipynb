{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sUOFTaQQPP9Y"
      },
      "source": [
        "Keyword Spotting Project\\\n",
        "Author: Soumit Kanti Saha\\\n",
        "Date: 04/07/2024"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EDF6JS6SaVMH"
      },
      "source": [
        "Installing Speechbrain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "nN6U0yvMv45B"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!git clone https://github.com/speechbrain/speechbrain.git\n",
        "%cd speechbrain\n",
        "!pip install -r requirements.txt\n",
        "!pip install .\n",
        "%cd .."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Writing the utility functions to prepare Google Speech Commands Dataset to be used.\\\n",
        "This file has been copied from\\\n",
        "https://github.com/speechbrain/speechbrain/blob/develop/recipes/Google-speech-commands/prepare_GSC.py\n",
        "\\\n",
        "But modified (in `def prepare_GSC`) as I have found the problem in logic of extraction from zip file of GSC dataset."
      ],
      "metadata": {
        "id": "lErjuxsCQohT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%file prepare_GSC.py\n",
        "\n",
        "\"\"\"\n",
        "Data preparation for Google Speech Commands v0.02.\n",
        "\n",
        "Download: http://download.tensorflow.org/data/speech_commands_v0.02.tar.gz\n",
        "\n",
        "Author\n",
        "------\n",
        "David Raby-Pepin 2021\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "import os\n",
        "from os import walk\n",
        "import glob\n",
        "import shutil\n",
        "import logging\n",
        "import torch\n",
        "import re\n",
        "import hashlib\n",
        "import copy\n",
        "import numpy as np\n",
        "from speechbrain.utils.data_utils import download_file\n",
        "from speechbrain.dataio.dataio import read_audio\n",
        "\n",
        "try:\n",
        "    import pandas as pd\n",
        "except ImportError:\n",
        "    err_msg = (\n",
        "        \"The optional dependency pandas must be installed to run this recipe.\\n\"\n",
        "    )\n",
        "    err_msg += \"Install using `pip install pandas`.\\n\"\n",
        "    raise ImportError(err_msg)\n",
        "\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "GSC_URL = \"http://download.tensorflow.org/data/speech_commands_v0.02.tar.gz\"\n",
        "\n",
        "# List of all the words (i.e. classes) within the GSC v2 dataset\n",
        "all_words = [\n",
        "    \"yes\",\n",
        "    \"no\",\n",
        "    \"up\",\n",
        "    \"down\",\n",
        "    \"left\",\n",
        "    \"right\",\n",
        "    \"on\",\n",
        "    \"off\",\n",
        "    \"stop\",\n",
        "    \"go\",\n",
        "    \"zero\",\n",
        "    \"one\",\n",
        "    \"two\",\n",
        "    \"three\",\n",
        "    \"four\",\n",
        "    \"five\",\n",
        "    \"six\",\n",
        "    \"seven\",\n",
        "    \"eight\",\n",
        "    \"nine\",\n",
        "    \"bed\",\n",
        "    \"bird\",\n",
        "    \"cat\",\n",
        "    \"dog\",\n",
        "    \"happy\",\n",
        "    \"house\",\n",
        "    \"marvin\",\n",
        "    \"sheila\",\n",
        "    \"tree\",\n",
        "    \"wow\",\n",
        "    \"backward\",\n",
        "    \"forward\",\n",
        "    \"follow\",\n",
        "    \"learn\",\n",
        "    \"visual\",\n",
        "]\n",
        "\n",
        "\n",
        "def prepare_GSC(\n",
        "    data_folder,\n",
        "    save_folder,\n",
        "    validation_percentage=10,\n",
        "    testing_percentage=10,\n",
        "    percentage_unknown=10,\n",
        "    percentage_silence=10,\n",
        "    words_wanted=[\n",
        "        \"yes\",\n",
        "        \"no\",\n",
        "        \"up\",\n",
        "        \"down\",\n",
        "        \"left\",\n",
        "        \"right\",\n",
        "        \"on\",\n",
        "        \"off\",\n",
        "        \"stop\",\n",
        "        \"go\",\n",
        "    ],\n",
        "    skip_prep=False,\n",
        "):\n",
        "    \"\"\"\n",
        "    Prepares the Google Speech Commands V2 dataset.\n",
        "\n",
        "    Arguments\n",
        "    ---------\n",
        "    data_folder : str\n",
        "        path to dataset. If not present, it will be downloaded here.\n",
        "    save_folder: str\n",
        "        folder where to store the data manifest files.\n",
        "    validation_percentage: int\n",
        "        How much of the data set to use for validation.\n",
        "    testing_percentage: int\n",
        "        How much of the data set to use for testing.\n",
        "    percentage_unknown: int.\n",
        "        How much data outside of the known (i.e wanted) words to preserve; relative to the total number of known words.\n",
        "    percentage_silence: int\n",
        "        How many silence samples to generate; relative to the total number of known words.\n",
        "    words_wanted: list\n",
        "        The list of commands to use from the dataset.\n",
        "    skip_prep: bool\n",
        "        If True, skip data preparation.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    None\n",
        "\n",
        "    Example\n",
        "    -------\n",
        "    >>> data_folder = '/path/to/GSC'\n",
        "    >>> prepare_GSC(data_folder)\n",
        "    \"\"\"\n",
        "\n",
        "    if skip_prep:\n",
        "        return\n",
        "\n",
        "    # If the data folders do not exist, we need to extract the data\n",
        "    if not os.path.isfile(os.path.join(data_folder, \"README.md\")):\n",
        "        # Check for zip file and download if it doesn't exist\n",
        "        tar_location = os.path.join(data_folder, \"speech_commands_v0.02.tar.gz\")\n",
        "        if not os.path.exists(tar_location):\n",
        "            download_file(GSC_URL, tar_location, unpack=False)\n",
        "\n",
        "        logger.info(\"Extracting speech_commands_v0.02.tar.gz...\")\n",
        "        shutil.unpack_archive(tar_location, data_folder)\n",
        "\n",
        "    # Define the words that we do not want to identify\n",
        "    unknown_words = list(np.setdiff1d(all_words, words_wanted))\n",
        "\n",
        "    # All metadata fields to appear within our dataset annotation files (i.e. train.csv, valid.csv, test.cvs)\n",
        "    fields = {\n",
        "        \"ID\": [],\n",
        "        \"duration\": [],\n",
        "        \"start\": [],\n",
        "        \"stop\": [],\n",
        "        \"wav\": [],\n",
        "        \"spk_id\": [],\n",
        "        \"command\": [],\n",
        "        \"transcript\": [],\n",
        "    }\n",
        "\n",
        "    splits = {\n",
        "        \"train\": copy.deepcopy(fields),\n",
        "        \"valid\": copy.deepcopy(fields),\n",
        "        \"test\": copy.deepcopy(fields),\n",
        "    }\n",
        "\n",
        "    num_known_samples_per_split = {\"train\": 0, \"valid\": 0, \"test\": 0}\n",
        "    words_wanted_parsed = False\n",
        "    commands = words_wanted + unknown_words\n",
        "    for i, command in enumerate(commands):\n",
        "        # logger.info(\"Preparing {}/{} commands...\".format(i, len(commands)))\n",
        "\n",
        "        # Indicate once all wanted words are parsed\n",
        "        if i >= len(words_wanted) and not words_wanted_parsed:\n",
        "            num_known_samples_total = np.sum(\n",
        "                list(num_known_samples_per_split.values())\n",
        "            )\n",
        "            num_unknown_samples_total = 105829 - num_known_samples_total\n",
        "            percentage_applied_to_unknown_samples = (\n",
        "                percentage_unknown * num_known_samples_total\n",
        "            ) / num_unknown_samples_total\n",
        "            words_wanted_parsed = True\n",
        "\n",
        "        # Read all files under a specific class (i.e. command)\n",
        "        files = []\n",
        "        for dirpath, dirnames, filenames in walk(\n",
        "            os.path.join(data_folder, command)\n",
        "        ):\n",
        "            files.extend(filenames)\n",
        "            break\n",
        "\n",
        "        # Fill in all fields with metadata for each audio sample file under a specific class\n",
        "        for filename in files:\n",
        "            # Once all wanted words are parsed, only retain the required percentage of unknown words\n",
        "            if (\n",
        "                words_wanted_parsed\n",
        "                and torch.rand(1)[0].tolist()\n",
        "                > percentage_applied_to_unknown_samples / 100\n",
        "            ):\n",
        "                continue\n",
        "\n",
        "            # select the required split (i.e. set) for the sample\n",
        "            split = which_set(\n",
        "                filename, validation_percentage, testing_percentage\n",
        "            )\n",
        "\n",
        "            splits[split][\"ID\"].append(\n",
        "                command + \"/\" + re.sub(r\".wav\", \"\", filename)\n",
        "            )\n",
        "\n",
        "            # We know that all recordings are 1 second long (i.e.16000 frames). No need to compute the duration.\n",
        "            splits[split][\"duration\"].append(1.0)\n",
        "            splits[split][\"start\"].append(0)\n",
        "            splits[split][\"stop\"].append(16000)\n",
        "\n",
        "            splits[split][\"wav\"].append(\n",
        "                os.path.join(data_folder, command, filename)\n",
        "            )\n",
        "\n",
        "            splits[split][\"spk_id\"].append(re.sub(r\"_.*\", \"\", filename))\n",
        "\n",
        "            if command in words_wanted:\n",
        "                splits[split][\"command\"].append(command)\n",
        "\n",
        "                num_known_samples_per_split[split] += 1\n",
        "            else:\n",
        "                splits[split][\"command\"].append(\"unknown\")\n",
        "\n",
        "            splits[split][\"transcript\"].append(command)\n",
        "\n",
        "    if percentage_silence > 0:\n",
        "        generate_silence_data(\n",
        "            num_known_samples_per_split,\n",
        "            splits,\n",
        "            data_folder,\n",
        "            percentage_silence=percentage_silence,\n",
        "        )\n",
        "\n",
        "    for split in splits:\n",
        "        new_filename = os.path.join(save_folder, split) + \".csv\"\n",
        "        new_df = pd.DataFrame(splits[split])\n",
        "        new_df.to_csv(new_filename, index=False)\n",
        "\n",
        "\n",
        "MAX_NUM_WAVS_PER_CLASS = 2**27 - 1  # ~134M\n",
        "\n",
        "\n",
        "def which_set(filename, validation_percentage, testing_percentage):\n",
        "    \"\"\"Determines which data partition the file should belong to.\n",
        "\n",
        "    We want to keep files in the same training, validation, or testing sets even\n",
        "    if new ones are added over time. This makes it less likely that testing\n",
        "    samples will accidentally be reused in training when long runs are restarted\n",
        "    for example. To keep this stability, a hash of the filename is taken and used\n",
        "    to determine which set it should belong to. This determination only depends on\n",
        "    the name and the set proportions, so it won't change as other files are added.\n",
        "\n",
        "    It's also useful to associate particular files as related (for example words\n",
        "    spoken by the same person), so anything after '_nohash_' in a filename is\n",
        "    ignored for set determination. This ensures that 'bobby_nohash_0.wav' and\n",
        "    'bobby_nohash_1.wav' are always in the same set, for example.\n",
        "\n",
        "    Arguments\n",
        "    ---------\n",
        "    filename: path\n",
        "        File path of the data sample.\n",
        "    validation_percentage: int\n",
        "        How much of the data set to use for validation.\n",
        "    testing_percentage: int\n",
        "        How much of the data set to use for testing.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    result: str\n",
        "        one of 'training', 'validation', or 'testing'.\n",
        "    \"\"\"\n",
        "    base_name = os.path.basename(filename)\n",
        "    # We want to ignore anything after '_nohash_' in the file name when\n",
        "    # deciding which set to put a wav in, so the data set creator has a way of\n",
        "    # grouping wavs that are close variations of each other.\n",
        "    hash_name = re.sub(r\"_nohash_.*$\", \"\", base_name).encode(\"utf-8\")\n",
        "    # This looks a bit magical, but we need to decide whether this file should\n",
        "    # go into the training, testing, or validation sets, and we want to keep\n",
        "    # existing files in the same set even if more files are subsequently\n",
        "    # added.\n",
        "    # To do that, we need a stable way of deciding based on just the file name\n",
        "    # itself, so we do a hash of that and then use that to generate a\n",
        "    # probability value that we use to assign it.\n",
        "    hash_name_hashed = hashlib.sha1(hash_name).hexdigest()\n",
        "    percentage_hash = (\n",
        "        int(hash_name_hashed, 16) % (MAX_NUM_WAVS_PER_CLASS + 1)\n",
        "    ) * (100.0 / MAX_NUM_WAVS_PER_CLASS)\n",
        "    if percentage_hash < validation_percentage:\n",
        "        result = \"valid\"\n",
        "    elif percentage_hash < (testing_percentage + validation_percentage):\n",
        "        result = \"test\"\n",
        "    else:\n",
        "        result = \"train\"\n",
        "    return result\n",
        "\n",
        "\n",
        "def generate_silence_data(\n",
        "    num_known_samples_per_split, splits, data_folder, percentage_silence=26\n",
        "):\n",
        "    \"\"\"Generates silence samples.\n",
        "\n",
        "    Arguments\n",
        "    ---------\n",
        "    num_known_samples_per_split: int\n",
        "        Total number of samples of known words for each split (i.e. set).\n",
        "    splits: str\n",
        "        Training, validation and test sets.\n",
        "    data_folder: str\n",
        "        path to dataset.\n",
        "    percentage_silence: int\n",
        "        How many silence samples to generate; relative to the total number of known words.\n",
        "    \"\"\"\n",
        "    for split in splits:\n",
        "        num_silence_samples = int(\n",
        "            (percentage_silence / 100.0) * num_known_samples_per_split[split]\n",
        "        )\n",
        "\n",
        "        # Fetch all background noise wav files used to generate silence samples\n",
        "        search_path = os.path.join(data_folder, \"_background_noise_\", \"*.wav\")\n",
        "        silence_paths = []\n",
        "        for wav_path in glob.glob(search_path):\n",
        "            silence_paths.append(wav_path)\n",
        "\n",
        "        # Generate random silence samples\n",
        "        # Assumes that the pytorch seed has been defined in the HyperPyYaml file\n",
        "        num_silence_samples_per_path = int(\n",
        "            num_silence_samples / len(silence_paths)\n",
        "        )\n",
        "        for silence_path in silence_paths:\n",
        "            signal = read_audio(silence_path)\n",
        "            random_starts = (\n",
        "                (\n",
        "                    torch.rand(num_silence_samples_per_path)\n",
        "                    * (signal.shape[0] - 16001)\n",
        "                )\n",
        "                .type(torch.int)\n",
        "                .tolist()\n",
        "            )\n",
        "\n",
        "            for i, random_start in enumerate(random_starts):\n",
        "                splits[split][\"ID\"].append(\n",
        "                    re.sub(\n",
        "                        r\".wav\",\n",
        "                        \"/\" + str(random_start) + \"_\" + str(i),\n",
        "                        re.sub(r\".+?(?=_background_noise_)\", \"\", silence_path),\n",
        "                    )\n",
        "                )\n",
        "\n",
        "                splits[split][\"duration\"].append(1.0)\n",
        "                splits[split][\"start\"].append(random_start)\n",
        "                splits[split][\"stop\"].append(random_start + 16000)\n",
        "                splits[split][\"wav\"].append(silence_path)\n",
        "                splits[split][\"spk_id\"].append(None)\n",
        "                splits[split][\"command\"].append(\"silence\")\n",
        "                splits[split][\"transcript\"].append(None)"
      ],
      "metadata": {
        "id": "HlSiacbUQjNB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "50259e01-8fe1-45db-a38c-22831abd1ba6"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing prepare_GSC.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a7nA3qPQacKI"
      },
      "source": [
        "Writing the YAML file to train with FBANK (feature extractor)\\\n",
        "and XVector as embedding model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7_s8VCgAyYwS",
        "outputId": "a2196151-0258-48d9-b3e8-e3ec06768380"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing xvect.yaml\n"
          ]
        }
      ],
      "source": [
        "%%file xvect.yaml\n",
        "\n",
        "# ################################\n",
        "# Model: Classification with xvector\n",
        "# Authors: Hwidong Na & Mirco Ravanelli\n",
        "#          Script adapted by David Raby-Pepin 2021\n",
        "# ################################\n",
        "\n",
        "# Basic parameters\n",
        "seed: 1986\n",
        "__set_seed: !apply:torch.manual_seed [!ref <seed>]\n",
        "\n",
        "# Use 12 for V2 12 task and 35 for V2 35 task\n",
        "number_of_commands: 12\n",
        "output_folder: !ref /content/Keyword_Spotting/results/xvect_v<number_of_commands>/<seed>\n",
        "save_folder: !ref <output_folder>/save\n",
        "train_log: !ref <output_folder>/train_log.txt\n",
        "\n",
        "# Data for augmentation\n",
        "NOISE_DATASET_URL: https://www.dropbox.com/scl/fi/a09pj97s5ifan81dqhi4n/noises.zip?rlkey=j8b0n9kdjdr32o1f06t0cw5b7&dl=1\n",
        "RIR_DATASET_URL: https://www.dropbox.com/scl/fi/linhy77c36mu10965a836/RIRs.zip?rlkey=pg9cu8vrpn2u173vhiqyu743u&dl=1\n",
        "\n",
        "# Data files\n",
        "data_folder: !PLACEHOLDER  # e.g. /path/to/GSC\n",
        "data_folder_noise: !ref <data_folder>/noise # The noisy sequences for data augmentation will automatically be downloaded here.\n",
        "data_folder_rir: !ref <data_folder>/rir # The impulse responses used for data augmentation will automatically be downloaded here.\n",
        "train_annotation: !ref <save_folder>/train.csv\n",
        "valid_annotation: !ref <save_folder>/valid.csv\n",
        "test_annotation: !ref <save_folder>/test.csv\n",
        "noise_annotation: !ref <save_folder>/noise.csv\n",
        "rir_annotation: !ref <save_folder>/rir.csv\n",
        "\n",
        "# Percentage of files used for validation and test\n",
        "validation_percentage: 5\n",
        "testing_percentage: 5\n",
        "\n",
        "# Percentage of unknown and silence examples\n",
        "# (relative to total of known word samples) to include\n",
        "percentage_unknown: 10 # Set this to 0 for the V2 35 task\n",
        "percentage_silence: 10 # Set this to 0 for the V2 35 task\n",
        "\n",
        "skip_prep: False\n",
        "ckpt_interval_minutes: 15 # save checkpoint every N min\n",
        "\n",
        "####################### Training Parameters ####################################\n",
        "number_of_epochs: 5\n",
        "batch_size: 16\n",
        "lr: 0.001\n",
        "lr_final: 0.0001\n",
        "\n",
        "sample_rate: 16000\n",
        "shuffle: True\n",
        "\n",
        "\n",
        "# Feature parameters\n",
        "n_mels: 24\n",
        "left_frames: 0\n",
        "right_frames: 0\n",
        "deltas: False\n",
        "\n",
        "# Number of classes (i.e. different commands)\n",
        "out_n_neurons: !ref <number_of_commands>  #includes core commands & auxiliary words\n",
        "\n",
        "num_workers: 4\n",
        "dataloader_options:\n",
        "    batch_size: !ref <batch_size>\n",
        "    shuffle: !ref <shuffle>\n",
        "    num_workers: !ref <num_workers>\n",
        "\n",
        "# Functions\n",
        "compute_features: !new:speechbrain.lobes.features.Fbank\n",
        "    n_mels: !ref <n_mels>\n",
        "    left_frames: !ref <left_frames>\n",
        "    right_frames: !ref <right_frames>\n",
        "    deltas: !ref <deltas>\n",
        "\n",
        "embedding_model: !new:speechbrain.lobes.models.Xvector.Xvector\n",
        "    in_channels: !ref <n_mels>\n",
        "    activation: !name:torch.nn.LeakyReLU\n",
        "    tdnn_blocks: 5\n",
        "    tdnn_channels: [512, 512, 512, 512, 1500]\n",
        "    tdnn_kernel_sizes: [5, 3, 3, 1, 1]\n",
        "    tdnn_dilations: [1, 2, 3, 1, 1]\n",
        "    lin_neurons: 512\n",
        "\n",
        "classifier: !new:speechbrain.lobes.models.Xvector.Classifier\n",
        "    input_shape: [null, null, 512]\n",
        "    activation: !name:torch.nn.LeakyReLU\n",
        "    lin_blocks: 1\n",
        "    lin_neurons: 512\n",
        "    out_neurons: !ref <out_n_neurons>\n",
        "\n",
        "softmax: !new:speechbrain.nnet.activations.Softmax\n",
        "    apply_log: True\n",
        "\n",
        "epoch_counter: !new:speechbrain.utils.epoch_loop.EpochCounter\n",
        "    limit: !ref <number_of_epochs>\n",
        "\n",
        "\n",
        "# Download and prepare the dataset of noisy sequences for augmentation\n",
        "prepare_noise_data: !name:speechbrain.augment.preparation.prepare_dataset_from_URL\n",
        "    URL: !ref <NOISE_DATASET_URL>\n",
        "    dest_folder: !ref <data_folder_noise>\n",
        "    ext: wav\n",
        "    csv_file: !ref <noise_annotation>\n",
        "\n",
        "# Add noise to input signal\n",
        "snr_low: 0  # Min SNR for noise augmentation\n",
        "snr_high: 15  # Max SNR for noise augmentation\n",
        "\n",
        "add_noise: !new:speechbrain.augment.time_domain.AddNoise\n",
        "    csv_file: !ref <noise_annotation>\n",
        "    snr_low: !ref <snr_low>\n",
        "    snr_high: !ref <snr_high>\n",
        "    noise_sample_rate: !ref <sample_rate>\n",
        "    clean_sample_rate: !ref <sample_rate>\n",
        "    num_workers: !ref <num_workers>\n",
        "\n",
        "\n",
        "# Download and prepare the dataset of room impulse responses for augmentation\n",
        "prepare_rir_data: !name:speechbrain.augment.preparation.prepare_dataset_from_URL\n",
        "    URL: !ref <RIR_DATASET_URL>\n",
        "    dest_folder: !ref <data_folder_rir>\n",
        "    ext: wav\n",
        "    csv_file: !ref <rir_annotation>\n",
        "\n",
        "# Add reverberation to input signal\n",
        "add_reverb: !new:speechbrain.augment.time_domain.AddReverb\n",
        "    csv_file: !ref <rir_annotation>\n",
        "    reverb_sample_rate: !ref <sample_rate>\n",
        "    clean_sample_rate: !ref <sample_rate>\n",
        "    num_workers: !ref <num_workers>\n",
        "\n",
        "# Frequency drop: randomly drops a number of frequency bands to zero.\n",
        "drop_freq_low: 0  # Min frequency band dropout probability\n",
        "drop_freq_high: 1  # Max frequency band dropout probability\n",
        "drop_freq_count_low: 1  # Min number of frequency bands to drop\n",
        "drop_freq_count_high: 3  # Max number of frequency bands to drop\n",
        "drop_freq_width: 0.05  # Width of frequency bands to drop\n",
        "\n",
        "drop_freq: !new:speechbrain.augment.time_domain.DropFreq\n",
        "    drop_freq_low: !ref <drop_freq_low>\n",
        "    drop_freq_high: !ref <drop_freq_high>\n",
        "    drop_freq_count_low: !ref <drop_freq_count_low>\n",
        "    drop_freq_count_high: !ref <drop_freq_count_high>\n",
        "    drop_freq_width: !ref <drop_freq_width>\n",
        "\n",
        "# Time drop: randomly drops a number of temporal chunks.\n",
        "drop_chunk_count_low: 1  # Min number of audio chunks to drop\n",
        "drop_chunk_count_high: 5  # Max number of audio chunks to drop\n",
        "drop_chunk_length_low: 1000  # Min length of audio chunks to drop\n",
        "drop_chunk_length_high: 2000  # Max length of audio chunks to drop\n",
        "\n",
        "drop_chunk: !new:speechbrain.augment.time_domain.DropChunk\n",
        "    drop_length_low: !ref <drop_chunk_length_low>\n",
        "    drop_length_high: !ref <drop_chunk_length_high>\n",
        "    drop_count_low: !ref <drop_chunk_count_low>\n",
        "    drop_count_high: !ref <drop_chunk_count_high>\n",
        "\n",
        "# Augmenter: Combines previously defined augmentations to perform data augmentation\n",
        "wav_augment: !new:speechbrain.augment.augmenter.Augmenter\n",
        "    parallel_augment: True\n",
        "    concat_original: True\n",
        "    repeat_augment: 1\n",
        "    shuffle_augmentations: False\n",
        "    min_augmentations: 4\n",
        "    max_augmentations: 4\n",
        "    augment_prob: 1.0\n",
        "    augmentations: [\n",
        "        !ref <add_noise>,\n",
        "        !ref <add_reverb>,\n",
        "        !ref <drop_freq>,\n",
        "        !ref <drop_chunk>]\n",
        "\n",
        "mean_var_norm: !new:speechbrain.processing.features.InputNormalization\n",
        "    norm_type: sentence\n",
        "    std_norm: False\n",
        "\n",
        "modules:\n",
        "    compute_features: !ref <compute_features>\n",
        "    embedding_model: !ref <embedding_model>\n",
        "    classifier: !ref <classifier>\n",
        "    softmax: !ref <softmax>\n",
        "    mean_var_norm: !ref <mean_var_norm>\n",
        "\n",
        "\n",
        "# Cost + optimization\n",
        "compute_cost: !name:speechbrain.nnet.losses.nll_loss\n",
        "# compute_error: !name:speechbrain.nnet.losses.classification_error\n",
        "\n",
        "opt_class: !name:torch.optim.Adam\n",
        "    lr: !ref <lr>\n",
        "    weight_decay: 0.000002\n",
        "\n",
        "lr_annealing: !new:speechbrain.nnet.schedulers.LinearScheduler\n",
        "    initial_value: !ref <lr>\n",
        "    final_value: !ref <lr_final>\n",
        "    epoch_count: !ref <number_of_epochs>\n",
        "\n",
        "# Logging + checkpoints\n",
        "train_logger: !new:speechbrain.utils.train_logger.FileTrainLogger\n",
        "    save_file: !ref <train_log>\n",
        "\n",
        "error_stats: !name:speechbrain.utils.metric_stats.MetricStats\n",
        "    metric: !name:speechbrain.nnet.losses.classification_error\n",
        "        reduction: batch\n",
        "\n",
        "checkpointer: !new:speechbrain.utils.checkpoints.Checkpointer\n",
        "    checkpoints_dir: !ref <save_folder>\n",
        "    recoverables:\n",
        "        embedding_model: !ref <embedding_model>\n",
        "        classifier: !ref <classifier>\n",
        "        normalizer: !ref <mean_var_norm>\n",
        "        counter: !ref <epoch_counter>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u6bf15Bvctnu"
      },
      "source": [
        "Writing the `train.py`.\\\n",
        "This file has been copied from\\\n",
        "https://github.com/speechbrain/speechbrain/blob/develop/recipes/Google-speech-commands/train.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XDjPELMuzkt4",
        "outputId": "70e7143f-3e1d-48f9-b513-862bdf8af353"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing train.py\n"
          ]
        }
      ],
      "source": [
        "%%file train.py\n",
        "\n",
        "#!/usr/bin/python3\n",
        "\"\"\"Recipe for training a classifier using the\n",
        "Google Speech Commands v0.02 Dataset.\n",
        "\n",
        "To run this recipe, use the following command:\n",
        "> python train.py {hyperparameter_file}\n",
        "\n",
        "Using your own hyperparameter file or one of the following:\n",
        "    hyperparams/xvect.yaml (xvector system)\n",
        "\n",
        "Author\n",
        "    * Mirco Ravanelli 2020\n",
        "    * Hwidong Na 2020\n",
        "    * Nauman Dawalatabad 2020\n",
        "    * Sarthak Yadav 2022\n",
        "    Script adapted by David Raby-Pepin 2021\n",
        "\"\"\"\n",
        "import os\n",
        "import sys\n",
        "import torch\n",
        "import torchaudio\n",
        "import speechbrain as sb\n",
        "from hyperpyyaml import load_hyperpyyaml\n",
        "\n",
        "import speechbrain.nnet.CNN\n",
        "from speechbrain.utils.distributed import run_on_main\n",
        "\n",
        "\n",
        "class SpeakerBrain(sb.core.Brain):\n",
        "    \"\"\"Class for GSC training\" \"\"\"\n",
        "\n",
        "    def compute_forward(self, batch, stage):\n",
        "        \"\"\"Computation pipeline based on a encoder + command classifier.\n",
        "        Data augmentation and environmental corruption are applied to the\n",
        "        input speech.\n",
        "        \"\"\"\n",
        "        batch = batch.to(self.device)\n",
        "        wavs, lens = batch.sig\n",
        "\n",
        "        # Add waveform augmentation if specified.\n",
        "        if stage == sb.Stage.TRAIN and hasattr(self.hparams, \"wav_augment\"):\n",
        "            wavs, lens = self.hparams.wav_augment(wavs, lens)\n",
        "\n",
        "        if isinstance(\n",
        "            self.modules.compute_features, speechbrain.lobes.features.Leaf\n",
        "        ):\n",
        "            # if leaf, first normalize the wavs before feeding them to leaf\n",
        "            # no normalization is needed after LEAF\n",
        "            feats = self.modules.mean_var_norm(wavs, lens)\n",
        "            feats = self.modules.compute_features(feats)\n",
        "        else:\n",
        "            # Feature extraction and normalization\n",
        "            feats = self.modules.compute_features(wavs)\n",
        "            feats = self.modules.mean_var_norm(feats, lens)\n",
        "\n",
        "        # Embeddings + classifier\n",
        "        embeddings = self.modules.embedding_model(feats)\n",
        "        outputs = self.modules.classifier(embeddings)\n",
        "\n",
        "        # Ecapa model uses softmax outside of its classifier\n",
        "        if \"softmax\" in self.modules.keys():\n",
        "            outputs = self.modules.softmax(outputs)\n",
        "\n",
        "        return outputs, lens\n",
        "\n",
        "    def compute_objectives(self, predictions, batch, stage):\n",
        "        \"\"\"Computes the loss using command-id as label.\"\"\"\n",
        "        predictions, lens = predictions\n",
        "        uttid = batch.id\n",
        "        command, _ = batch.command_encoded\n",
        "\n",
        "        # Concatenate labels (due to data augmentation)\n",
        "        if stage == sb.Stage.TRAIN and hasattr(self.hparams, \"wav_augment\"):\n",
        "            command = self.hparams.wav_augment.replicate_labels(command)\n",
        "\n",
        "        # compute the cost function\n",
        "        loss = self.hparams.compute_cost(predictions, command, lens)\n",
        "        # loss = sb.nnet.losses.nll_loss(predictions, command, lens)\n",
        "\n",
        "        if hasattr(self.hparams.lr_annealing, \"on_batch_end\"):\n",
        "            self.hparams.lr_annealing.on_batch_end(self.optimizer)\n",
        "\n",
        "        if stage != sb.Stage.TRAIN:\n",
        "            self.error_metrics.append(uttid, predictions, command, lens)\n",
        "\n",
        "        return loss\n",
        "\n",
        "    def on_stage_start(self, stage, epoch=None):\n",
        "        \"\"\"Gets called at the beginning of an epoch.\"\"\"\n",
        "        if stage != sb.Stage.TRAIN:\n",
        "            self.error_metrics = self.hparams.error_stats()\n",
        "\n",
        "    def on_stage_end(self, stage, stage_loss, epoch=None):\n",
        "        \"\"\"Gets called at the end of an epoch.\"\"\"\n",
        "        # Compute/store important stats\n",
        "        stage_stats = {\"loss\": stage_loss}\n",
        "        if stage == sb.Stage.TRAIN:\n",
        "            self.train_stats = stage_stats\n",
        "        else:\n",
        "            stage_stats[\"ErrorRate\"] = self.error_metrics.summarize(\"average\")\n",
        "\n",
        "        # Perform end-of-iteration things, like annealing, logging, etc.\n",
        "        if stage == sb.Stage.VALID:\n",
        "            old_lr, new_lr = self.hparams.lr_annealing(epoch)\n",
        "            sb.nnet.schedulers.update_learning_rate(self.optimizer, new_lr)\n",
        "\n",
        "            self.hparams.train_logger.log_stats(\n",
        "                stats_meta={\"epoch\": epoch, \"lr\": old_lr},\n",
        "                train_stats=self.train_stats,\n",
        "                valid_stats=stage_stats,\n",
        "            )\n",
        "            self.checkpointer.save_and_keep_only(\n",
        "                meta={\"ErrorRate\": stage_stats[\"ErrorRate\"]},\n",
        "                min_keys=[\"ErrorRate\"],\n",
        "            )\n",
        "\n",
        "        # We also write statistics about test data to stdout and to the logfile.\n",
        "        if stage == sb.Stage.TEST:\n",
        "            self.hparams.train_logger.log_stats(\n",
        "                {\"Epoch loaded\": self.hparams.epoch_counter.current},\n",
        "                test_stats=stage_stats,\n",
        "            )\n",
        "\n",
        "\n",
        "def dataio_prep(hparams):\n",
        "    \"Creates the datasets and their data processing pipelines.\"\n",
        "\n",
        "    data_folder = hparams[\"data_folder\"]\n",
        "\n",
        "    # 1. Declarations:\n",
        "    train_data = sb.dataio.dataset.DynamicItemDataset.from_csv(\n",
        "        csv_path=hparams[\"train_annotation\"],\n",
        "        replacements={\"data_root\": data_folder},\n",
        "    )\n",
        "\n",
        "    valid_data = sb.dataio.dataset.DynamicItemDataset.from_csv(\n",
        "        csv_path=hparams[\"valid_annotation\"],\n",
        "        replacements={\"data_root\": data_folder},\n",
        "    )\n",
        "\n",
        "    test_data = sb.dataio.dataset.DynamicItemDataset.from_csv(\n",
        "        csv_path=hparams[\"test_annotation\"],\n",
        "        replacements={\"data_root\": data_folder},\n",
        "    )\n",
        "\n",
        "    datasets = [train_data, valid_data, test_data]\n",
        "    label_encoder = sb.dataio.encoder.CategoricalEncoder()\n",
        "\n",
        "    # 2. Define audio pipeline:\n",
        "    @sb.utils.data_pipeline.takes(\"wav\", \"start\", \"stop\", \"duration\")\n",
        "    @sb.utils.data_pipeline.provides(\"sig\")\n",
        "    def audio_pipeline(wav, start, stop, duration):\n",
        "        start = int(start)\n",
        "        stop = int(stop)\n",
        "        num_frames = stop - start\n",
        "        sig, fs = torchaudio.load(\n",
        "            wav, num_frames=num_frames, frame_offset=start\n",
        "        )\n",
        "        sig = sig.transpose(0, 1).squeeze(1)\n",
        "        return sig\n",
        "\n",
        "    sb.dataio.dataset.add_dynamic_item(datasets, audio_pipeline)\n",
        "\n",
        "    # 3. Define text pipeline:\n",
        "    @sb.utils.data_pipeline.takes(\"command\")\n",
        "    @sb.utils.data_pipeline.provides(\"command\", \"command_encoded\")\n",
        "    def label_pipeline(command):\n",
        "        yield command\n",
        "        command_encoded = label_encoder.encode_sequence_torch([command])\n",
        "        yield command_encoded\n",
        "\n",
        "    sb.dataio.dataset.add_dynamic_item(datasets, label_pipeline)\n",
        "\n",
        "    # 3. Fit encoder:\n",
        "    # Load or compute the label encoder (with multi-GPU DDP support)\n",
        "    lab_enc_file = os.path.join(hparams[\"save_folder\"], \"label_encoder.txt\")\n",
        "    label_encoder.load_or_create(\n",
        "        path=lab_enc_file,\n",
        "        from_didatasets=[train_data],\n",
        "        output_key=\"command\",\n",
        "    )\n",
        "\n",
        "    # 4. Set output:\n",
        "    sb.dataio.dataset.set_output_keys(\n",
        "        datasets, [\"id\", \"sig\", \"command_encoded\"]\n",
        "    )\n",
        "\n",
        "    return train_data, valid_data, test_data, label_encoder\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # This flag enables the inbuilt cudnn auto-tuner\n",
        "    torch.backends.cudnn.benchmark = True\n",
        "\n",
        "    # CLI:\n",
        "    hparams_file, run_opts, overrides = sb.parse_arguments(sys.argv[1:])\n",
        "\n",
        "    # Initialize ddp (useful only for multi-GPU DDP training)\n",
        "    sb.utils.distributed.ddp_init_group(run_opts)\n",
        "\n",
        "    # Load hyperparameters file with command-line overrides\n",
        "    with open(hparams_file) as fin:\n",
        "        hparams = load_hyperpyyaml(fin, overrides)\n",
        "\n",
        "    # Create experiment directory\n",
        "    sb.core.create_experiment_directory(\n",
        "        experiment_directory=hparams[\"output_folder\"],\n",
        "        hyperparams_to_save=hparams_file,\n",
        "        overrides=overrides,\n",
        "    )\n",
        "\n",
        "    # Dataset prep (parsing GSC and annotation into csv files)\n",
        "    from prepare_GSC import prepare_GSC\n",
        "\n",
        "    # Known words for V2 12 and V2 35 sets\n",
        "    if hparams[\"number_of_commands\"] == 12:\n",
        "        words_wanted = [\n",
        "            \"yes\",\n",
        "            \"no\",\n",
        "            \"up\",\n",
        "            \"down\",\n",
        "            \"left\",\n",
        "            \"right\",\n",
        "            \"on\",\n",
        "            \"off\",\n",
        "            \"stop\",\n",
        "            \"go\",\n",
        "        ]\n",
        "    elif hparams[\"number_of_commands\"] == 35:\n",
        "        words_wanted = [\n",
        "            \"yes\",\n",
        "            \"no\",\n",
        "            \"up\",\n",
        "            \"down\",\n",
        "            \"left\",\n",
        "            \"right\",\n",
        "            \"on\",\n",
        "            \"off\",\n",
        "            \"stop\",\n",
        "            \"go\",\n",
        "            \"zero\",\n",
        "            \"one\",\n",
        "            \"two\",\n",
        "            \"three\",\n",
        "            \"four\",\n",
        "            \"five\",\n",
        "            \"six\",\n",
        "            \"seven\",\n",
        "            \"eight\",\n",
        "            \"nine\",\n",
        "            \"bed\",\n",
        "            \"bird\",\n",
        "            \"cat\",\n",
        "            \"dog\",\n",
        "            \"happy\",\n",
        "            \"house\",\n",
        "            \"marvin\",\n",
        "            \"sheila\",\n",
        "            \"tree\",\n",
        "            \"wow\",\n",
        "            \"backward\",\n",
        "            \"forward\",\n",
        "            \"follow\",\n",
        "            \"learn\",\n",
        "            \"visual\",\n",
        "        ]\n",
        "    else:\n",
        "        raise ValueError(\"number_of_commands must be 12 or 35\")\n",
        "\n",
        "    # Data preparation\n",
        "    run_on_main(\n",
        "        prepare_GSC,\n",
        "        kwargs={\n",
        "            \"data_folder\": hparams[\"data_folder\"],\n",
        "            \"save_folder\": hparams[\"save_folder\"],\n",
        "            \"validation_percentage\": hparams[\"validation_percentage\"],\n",
        "            \"testing_percentage\": hparams[\"testing_percentage\"],\n",
        "            \"percentage_unknown\": hparams[\"percentage_unknown\"],\n",
        "            \"percentage_silence\": hparams[\"percentage_silence\"],\n",
        "            \"words_wanted\": words_wanted,\n",
        "            \"skip_prep\": hparams[\"skip_prep\"],\n",
        "        },\n",
        "    )\n",
        "    sb.utils.distributed.run_on_main(hparams[\"prepare_noise_data\"])\n",
        "    sb.utils.distributed.run_on_main(hparams[\"prepare_rir_data\"])\n",
        "\n",
        "    # Dataset IO prep: creating Dataset objects and proper encodings for phones\n",
        "    train_data, valid_data, test_data, label_encoder = dataio_prep(hparams)\n",
        "\n",
        "    # Brain class initialization\n",
        "    speaker_brain = SpeakerBrain(\n",
        "        modules=hparams[\"modules\"],\n",
        "        opt_class=hparams[\"opt_class\"],\n",
        "        hparams=hparams,\n",
        "        run_opts=run_opts,\n",
        "        checkpointer=hparams[\"checkpointer\"],\n",
        "    )\n",
        "\n",
        "    # with torch.autograd.detect_anomaly():\n",
        "    # Training\n",
        "    speaker_brain.fit(\n",
        "        speaker_brain.hparams.epoch_counter,\n",
        "        train_data,\n",
        "        valid_data,\n",
        "        train_loader_kwargs=hparams[\"dataloader_options\"],\n",
        "        valid_loader_kwargs=hparams[\"dataloader_options\"],\n",
        "    )\n",
        "\n",
        "    # Load the best checkpoint for evaluation\n",
        "    test_stats = speaker_brain.evaluate(\n",
        "        test_set=test_data,\n",
        "        min_key=\"ErrorRate\",\n",
        "        test_loader_kwargs=hparams[\"dataloader_options\"],\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dGMzCGa9dJXt"
      },
      "source": [
        "Delete the downloaded GSC dataset and all generated results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "riSh9xdoTvxY"
      },
      "outputs": [],
      "source": [
        "# Uncomment only if dataset download failed or you want to download again\n",
        "\n",
        "# !rm -rf ./Google_Speech_Command_Dataset/\n",
        "# !rm -rf ./Keyword_Spotting/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QjZiOqkuiGVs"
      },
      "source": [
        "Remove previously generated results for Xvector."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3NI54ALpiPIH"
      },
      "outputs": [],
      "source": [
        "# !rm -rf ./Keyword_Spotting/results/xvect_v12/1986"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7wGmQ06tdTvL"
      },
      "source": [
        "run with xvect.yaml"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XTe8hambzvSl"
      },
      "outputs": [],
      "source": [
        "!python train.py xvect.yaml --data_folder=/content/Google_Speech_Command_Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "After running it, we get result like\n",
        "```\n",
        "epoch: 1, lr: 1.00e-03 - train loss: 7.27e-01 - valid loss: 2.49e-01, valid ErrorRate: 8.04e-02\n",
        "epoch: 2, lr: 7.75e-04 - train loss: 4.18e-01 - valid loss: 1.83e-01, valid ErrorRate: 5.84e-02\n",
        "epoch: 3, lr: 5.50e-04 - train loss: 3.19e-01 - valid loss: 1.12e-01, valid ErrorRate: 3.83e-02\n",
        "epoch: 4, lr: 3.25e-04 - train loss: 2.47e-01 - valid loss: 1.01e-01, valid ErrorRate: 3.04e-02\n",
        "epoch: 5, lr: 1.00e-04 - train loss: 1.92e-01 - valid loss: 8.82e-02, valid ErrorRate: 2.62e-02\n",
        "Epoch loaded: 5 - test loss: 1.35e-01, test ErrorRate: 3.75e-02\n",
        "\n",
        "```"
      ],
      "metadata": {
        "id": "-ZimpMhZztRq"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SBrnxMLjdbZM"
      },
      "source": [
        "YAML file with wav2vec2, avg pooling and a linear layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZNC8axvilwaf",
        "outputId": "3df94b70-c708-4082-c3d0-7b0c0fb447ed"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing wav2vec2.yaml\n"
          ]
        }
      ],
      "source": [
        "%%file wav2vec2.yaml\n",
        "\n",
        "# Basic parameters\n",
        "seed: 1986\n",
        "__set_seed: !apply:torch.manual_seed [!ref <seed>]\n",
        "\n",
        "# Use 12 for V2 12 task and 35 for V2 35 task\n",
        "number_of_commands: 12\n",
        "output_folder: !ref /content/Keyword_Spotting/results/wav2vec2/<seed>\n",
        "save_folder: !ref <output_folder>/save\n",
        "train_log: !ref <output_folder>/train_log.txt\n",
        "\n",
        "# Data for augmentation\n",
        "NOISE_DATASET_URL: https://www.dropbox.com/scl/fi/a09pj97s5ifan81dqhi4n/noises.zip?rlkey=j8b0n9kdjdr32o1f06t0cw5b7&dl=1\n",
        "RIR_DATASET_URL: https://www.dropbox.com/scl/fi/linhy77c36mu10965a836/RIRs.zip?rlkey=pg9cu8vrpn2u173vhiqyu743u&dl=1\n",
        "\n",
        "# Data files\n",
        "data_folder: !PLACEHOLDER  # e.g. /path/to/GSC\n",
        "data_folder_noise: !ref <data_folder>/noise # The noisy sequences for data augmentation will automatically be downloaded here.\n",
        "data_folder_rir: !ref <data_folder>/rir # The impulse responses used for data augmentation will automatically be downloaded here.\n",
        "train_annotation: !ref <save_folder>/train.csv\n",
        "valid_annotation: !ref <save_folder>/valid.csv\n",
        "test_annotation: !ref <save_folder>/test.csv\n",
        "noise_annotation: !ref <save_folder>/noise.csv\n",
        "rir_annotation: !ref <save_folder>/rir.csv\n",
        "\n",
        "# Percentage of files used for validation and test\n",
        "validation_percentage: 5\n",
        "testing_percentage: 5\n",
        "\n",
        "# Percentage of unknown and silence examples\n",
        "# (relative to total of known word samples) to include\n",
        "percentage_unknown: 10 # Set this to 0 for the V2 35 task\n",
        "percentage_silence: 10 # Set this to 0 for the V2 35 task\n",
        "\n",
        "skip_prep: False\n",
        "ckpt_interval_minutes: 15 # save checkpoint every N min\n",
        "\n",
        "####################### Training Parameters ####################################\n",
        "number_of_epochs: 2\n",
        "batch_size: 16\n",
        "lr: 0.0001\n",
        "lr_final: 0.00001\n",
        "\n",
        "sample_rate: 16000\n",
        "shuffle: True\n",
        "\n",
        "\n",
        "# Feature parameters\n",
        "n_mels: 24\n",
        "left_frames: 0\n",
        "right_frames: 0\n",
        "deltas: False\n",
        "\n",
        "# Number of classes (i.e. different commands)\n",
        "out_n_neurons: !ref <number_of_commands>  #includes core commands & auxiliary words\n",
        "\n",
        "num_workers: 2\n",
        "dataloader_options:\n",
        "    batch_size: !ref <batch_size>\n",
        "    shuffle: !ref <shuffle>\n",
        "    num_workers: !ref <num_workers>\n",
        "\n",
        "# Functions\n",
        "compute_features: !new:speechbrain.lobes.features.Fbank\n",
        "    n_mels: !ref <n_mels>\n",
        "    left_frames: !ref <left_frames>\n",
        "    right_frames: !ref <right_frames>\n",
        "    deltas: !ref <deltas>\n",
        "\n",
        "embedding_model: !new:speechbrain.lobes.models.Xvector.Xvector\n",
        "    in_channels: !ref <n_mels>\n",
        "    activation: !name:torch.nn.LeakyReLU\n",
        "    tdnn_blocks: 5\n",
        "    tdnn_channels: [512, 512, 512, 512, 1500]\n",
        "    tdnn_kernel_sizes: [5, 3, 3, 1, 1]\n",
        "    tdnn_dilations: [1, 2, 3, 1, 1]\n",
        "    lin_neurons: 512\n",
        "\n",
        "freeze_ssl: False\n",
        "freeze_ssl_conv: True\n",
        "sslmodel_hub: facebook/wav2vec2-base\n",
        "sslmodel_folder: !ref <save_folder>/ssl_checkpoint\n",
        "encoder_dim: 768\n",
        "\n",
        "# wav2vec2\n",
        "ssl_model: !new:speechbrain.lobes.models.huggingface_transformers.wav2vec2.Wav2Vec2\n",
        "    source: !ref <sslmodel_hub>\n",
        "    output_norm: True\n",
        "    freeze: !ref <freeze_ssl>\n",
        "    freeze_feature_extractor: !ref <freeze_ssl_conv>\n",
        "    save_path: !ref <sslmodel_folder>\n",
        "\n",
        "avg_pool: !new:speechbrain.nnet.pooling.StatisticsPooling\n",
        "    return_std: False\n",
        "\n",
        "output_mlp: !new:speechbrain.nnet.linear.Linear\n",
        "    input_size: !ref <encoder_dim>\n",
        "    n_neurons: !ref <out_n_neurons>\n",
        "    bias: False\n",
        "\n",
        "classifier: !new:speechbrain.lobes.models.Xvector.Classifier\n",
        "    input_shape: [null, null, 512]\n",
        "    activation: !name:torch.nn.LeakyReLU\n",
        "    lin_blocks: 1\n",
        "    lin_neurons: 512\n",
        "    out_neurons: !ref <out_n_neurons>\n",
        "\n",
        "softmax: !new:speechbrain.nnet.activations.Softmax\n",
        "    apply_log: True\n",
        "\n",
        "epoch_counter: !new:speechbrain.utils.epoch_loop.EpochCounter\n",
        "    limit: !ref <number_of_epochs>\n",
        "\n",
        "\n",
        "# Download and prepare the dataset of noisy sequences for augmentation\n",
        "prepare_noise_data: !name:speechbrain.augment.preparation.prepare_dataset_from_URL\n",
        "    URL: !ref <NOISE_DATASET_URL>\n",
        "    dest_folder: !ref <data_folder_noise>\n",
        "    ext: wav\n",
        "    csv_file: !ref <noise_annotation>\n",
        "\n",
        "# Add noise to input signal\n",
        "snr_low: 0  # Min SNR for noise augmentation\n",
        "snr_high: 15  # Max SNR for noise augmentation\n",
        "\n",
        "add_noise: !new:speechbrain.augment.time_domain.AddNoise\n",
        "    csv_file: !ref <noise_annotation>\n",
        "    snr_low: !ref <snr_low>\n",
        "    snr_high: !ref <snr_high>\n",
        "    noise_sample_rate: !ref <sample_rate>\n",
        "    clean_sample_rate: !ref <sample_rate>\n",
        "    num_workers: !ref <num_workers>\n",
        "\n",
        "\n",
        "# Download and prepare the dataset of room impulse responses for augmentation\n",
        "prepare_rir_data: !name:speechbrain.augment.preparation.prepare_dataset_from_URL\n",
        "    URL: !ref <RIR_DATASET_URL>\n",
        "    dest_folder: !ref <data_folder_rir>\n",
        "    ext: wav\n",
        "    csv_file: !ref <rir_annotation>\n",
        "\n",
        "# Add reverberation to input signal\n",
        "add_reverb: !new:speechbrain.augment.time_domain.AddReverb\n",
        "    csv_file: !ref <rir_annotation>\n",
        "    reverb_sample_rate: !ref <sample_rate>\n",
        "    clean_sample_rate: !ref <sample_rate>\n",
        "    num_workers: !ref <num_workers>\n",
        "\n",
        "# Frequency drop: randomly drops a number of frequency bands to zero.\n",
        "drop_freq_low: 0  # Min frequency band dropout probability\n",
        "drop_freq_high: 1  # Max frequency band dropout probability\n",
        "drop_freq_count_low: 1  # Min number of frequency bands to drop\n",
        "drop_freq_count_high: 3  # Max number of frequency bands to drop\n",
        "drop_freq_width: 0.05  # Width of frequency bands to drop\n",
        "\n",
        "drop_freq: !new:speechbrain.augment.time_domain.DropFreq\n",
        "    drop_freq_low: !ref <drop_freq_low>\n",
        "    drop_freq_high: !ref <drop_freq_high>\n",
        "    drop_freq_count_low: !ref <drop_freq_count_low>\n",
        "    drop_freq_count_high: !ref <drop_freq_count_high>\n",
        "    drop_freq_width: !ref <drop_freq_width>\n",
        "\n",
        "# Time drop: randomly drops a number of temporal chunks.\n",
        "drop_chunk_count_low: 1  # Min number of audio chunks to drop\n",
        "drop_chunk_count_high: 5  # Max number of audio chunks to drop\n",
        "drop_chunk_length_low: 1000  # Min length of audio chunks to drop\n",
        "drop_chunk_length_high: 2000  # Max length of audio chunks to drop\n",
        "\n",
        "drop_chunk: !new:speechbrain.augment.time_domain.DropChunk\n",
        "    drop_length_low: !ref <drop_chunk_length_low>\n",
        "    drop_length_high: !ref <drop_chunk_length_high>\n",
        "    drop_count_low: !ref <drop_chunk_count_low>\n",
        "    drop_count_high: !ref <drop_chunk_count_high>\n",
        "\n",
        "# Augmenter: Combines previously defined augmentations to perform data augmentation\n",
        "wav_augment: !new:speechbrain.augment.augmenter.Augmenter\n",
        "    parallel_augment: True\n",
        "    concat_original: True\n",
        "    repeat_augment: 1\n",
        "    shuffle_augmentations: False\n",
        "    min_augmentations: 4\n",
        "    max_augmentations: 4\n",
        "    augment_prob: 1.0\n",
        "    augmentations: [\n",
        "        !ref <add_noise>,\n",
        "        !ref <add_reverb>,\n",
        "        !ref <drop_freq>,\n",
        "        !ref <drop_chunk>]\n",
        "\n",
        "mean_var_norm: !new:speechbrain.processing.features.InputNormalization\n",
        "    norm_type: sentence\n",
        "    std_norm: False\n",
        "\n",
        "modules:\n",
        "    compute_features: !ref <compute_features>\n",
        "    embedding_model: !ref <embedding_model>\n",
        "    classifier: !ref <classifier>\n",
        "    softmax: !ref <softmax>\n",
        "    mean_var_norm: !ref <mean_var_norm>\n",
        "    ssl_model: !ref <ssl_model>\n",
        "    output_mlp: !ref <output_mlp>\n",
        "\n",
        "\n",
        "# Cost + optimization\n",
        "compute_cost: !name:speechbrain.nnet.losses.nll_loss\n",
        "# compute_error: !name:speechbrain.nnet.losses.classification_error\n",
        "\n",
        "opt_class: !name:torch.optim.Adam\n",
        "    lr: !ref <lr>\n",
        "    weight_decay: 0.000002\n",
        "\n",
        "lr_annealing: !new:speechbrain.nnet.schedulers.LinearScheduler\n",
        "    initial_value: !ref <lr>\n",
        "    final_value: !ref <lr_final>\n",
        "    epoch_count: !ref <number_of_epochs>\n",
        "\n",
        "# Logging + checkpoints\n",
        "train_logger: !new:speechbrain.utils.train_logger.FileTrainLogger\n",
        "    save_file: !ref <train_log>\n",
        "\n",
        "error_stats: !name:speechbrain.utils.metric_stats.MetricStats\n",
        "    metric: !name:speechbrain.nnet.losses.classification_error\n",
        "        reduction: batch\n",
        "\n",
        "checkpointer: !new:speechbrain.utils.checkpoints.Checkpointer\n",
        "    checkpoints_dir: !ref <save_folder>\n",
        "    recoverables:\n",
        "        embedding_model: !ref <embedding_model>\n",
        "        classifier: !ref <classifier>\n",
        "        normalizer: !ref <mean_var_norm>\n",
        "        counter: !ref <epoch_counter>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9abmaBsId7jx"
      },
      "source": [
        "Writing the train file for self supervised language (ssl) models (wav2vec2, HuBert, WavLM)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0yWRcDLhtVaB",
        "outputId": "63e2f639-e194-4620-8df1-489f17455284"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting train_self_supervised.py\n"
          ]
        }
      ],
      "source": [
        "%%file train_self_supervised.py\n",
        "\n",
        "#!/usr/bin/python3\n",
        "\n",
        "import os\n",
        "import sys\n",
        "import torch\n",
        "import torchaudio\n",
        "import speechbrain as sb\n",
        "from hyperpyyaml import load_hyperpyyaml\n",
        "\n",
        "import speechbrain.nnet.CNN\n",
        "from speechbrain.utils.distributed import run_on_main\n",
        "\n",
        "\n",
        "class SpeakerBrain(sb.core.Brain):\n",
        "    \"\"\"Class for GSC training\" \"\"\"\n",
        "\n",
        "    def compute_forward(self, batch, stage):\n",
        "        \"\"\"Computation pipeline based on a ssl_encoder + command classifier.\n",
        "        Data augmentation and environmental corruption are applied to the\n",
        "        input speech.\n",
        "        \"\"\"\n",
        "        batch = batch.to(self.device)\n",
        "        wavs, lens = batch.sig\n",
        "\n",
        "        # Add waveform augmentation if specified.\n",
        "        if stage == sb.Stage.TRAIN and hasattr(self.hparams, \"wav_augment\"):\n",
        "            wavs, lens = self.hparams.wav_augment(wavs, lens)\n",
        "\n",
        "        # Embeddings + classifier\n",
        "        outputs = self.modules.ssl_model(wavs, lens)\n",
        "        # last dim will be used for AdaptativeAVG pool\n",
        "        outputs = self.hparams.avg_pool(outputs, lens)\n",
        "        outputs = outputs.view(outputs.shape[0], -1)\n",
        "        outputs = self.modules.output_mlp(outputs)\n",
        "\n",
        "        # Ecapa model uses softmax outside of its classifier\n",
        "        if \"softmax\" in self.modules.keys():\n",
        "            outputs = self.modules.softmax(outputs)\n",
        "\n",
        "        return outputs, lens\n",
        "\n",
        "    def compute_objectives(self, predictions, batch, stage):\n",
        "        \"\"\"Computes the loss using command-id as label.\"\"\"\n",
        "        predictions, lens = predictions\n",
        "        uttid = batch.id\n",
        "        command, _ = batch.command_encoded\n",
        "\n",
        "        # Concatenate labels (due to data augmentation)\n",
        "        if stage == sb.Stage.TRAIN and hasattr(self.hparams, \"wav_augment\"):\n",
        "            command = self.hparams.wav_augment.replicate_labels(command)\n",
        "\n",
        "        \"\"\"to meet the input form of nll loss\"\"\"\n",
        "        command = command.squeeze(1)\n",
        "\n",
        "        # compute the cost function\n",
        "        loss = self.hparams.compute_cost(predictions, command)\n",
        "        # loss = sb.nnet.losses.nll_loss(predictions, command, lens)\n",
        "\n",
        "        # if hasattr(self.hparams.lr_annealing, \"on_batch_end\"):\n",
        "        #     self.hparams.lr_annealing.on_batch_end(self.optimizer)\n",
        "\n",
        "        if stage != sb.Stage.TRAIN:\n",
        "            self.error_metrics.append(uttid, predictions, command)\n",
        "\n",
        "        return loss\n",
        "\n",
        "    def on_stage_start(self, stage, epoch=None):\n",
        "        \"\"\"Gets called at the beginning of an epoch.\"\"\"\n",
        "        if stage != sb.Stage.TRAIN:\n",
        "            self.error_metrics = self.hparams.error_stats()\n",
        "\n",
        "    def on_stage_end(self, stage, stage_loss, epoch=None):\n",
        "        \"\"\"Gets called at the end of an epoch.\"\"\"\n",
        "        # Compute/store important stats\n",
        "        stage_stats = {\"loss\": stage_loss}\n",
        "        if stage == sb.Stage.TRAIN:\n",
        "            self.train_stats = stage_stats\n",
        "        else:\n",
        "            stage_stats[\"ErrorRate\"] = self.error_metrics.summarize(\"average\")\n",
        "\n",
        "        # Perform end-of-iteration things, like annealing, logging, etc.\n",
        "        if stage == sb.Stage.VALID:\n",
        "            old_lr, new_lr = self.hparams.lr_annealing(epoch)\n",
        "            sb.nnet.schedulers.update_learning_rate(self.optimizer, new_lr)\n",
        "\n",
        "            self.hparams.train_logger.log_stats(\n",
        "                stats_meta={\"epoch\": epoch, \"lr\": old_lr},\n",
        "                train_stats=self.train_stats,\n",
        "                valid_stats=stage_stats,\n",
        "            )\n",
        "            self.checkpointer.save_and_keep_only(\n",
        "                meta={\"ErrorRate\": stage_stats[\"ErrorRate\"]},\n",
        "                min_keys=[\"ErrorRate\"],\n",
        "            )\n",
        "\n",
        "        # We also write statistics about test data to stdout and to the logfile.\n",
        "        if stage == sb.Stage.TEST:\n",
        "            self.hparams.train_logger.log_stats(\n",
        "                {\"Epoch loaded\": self.hparams.epoch_counter.current},\n",
        "                test_stats=stage_stats,\n",
        "            )\n",
        "\n",
        "\n",
        "def dataio_prep(hparams):\n",
        "    \"Creates the datasets and their data processing pipelines.\"\n",
        "\n",
        "    data_folder = hparams[\"data_folder\"]\n",
        "\n",
        "    # 1. Declarations:\n",
        "    train_data = sb.dataio.dataset.DynamicItemDataset.from_csv(\n",
        "        csv_path=hparams[\"train_annotation\"],\n",
        "        replacements={\"data_root\": data_folder},\n",
        "    )\n",
        "\n",
        "    valid_data = sb.dataio.dataset.DynamicItemDataset.from_csv(\n",
        "        csv_path=hparams[\"valid_annotation\"],\n",
        "        replacements={\"data_root\": data_folder},\n",
        "    )\n",
        "\n",
        "    test_data = sb.dataio.dataset.DynamicItemDataset.from_csv(\n",
        "        csv_path=hparams[\"test_annotation\"],\n",
        "        replacements={\"data_root\": data_folder},\n",
        "    )\n",
        "\n",
        "    datasets = [train_data, valid_data, test_data]\n",
        "    label_encoder = sb.dataio.encoder.CategoricalEncoder()\n",
        "\n",
        "    # 2. Define audio pipeline:\n",
        "    @sb.utils.data_pipeline.takes(\"wav\", \"start\", \"stop\", \"duration\")\n",
        "    @sb.utils.data_pipeline.provides(\"sig\")\n",
        "    def audio_pipeline(wav, start, stop, duration):\n",
        "        start = int(start)\n",
        "        stop = int(stop)\n",
        "        num_frames = stop - start\n",
        "        sig, fs = torchaudio.load(\n",
        "            wav, num_frames=num_frames, frame_offset=start\n",
        "        )\n",
        "        sig = sig.transpose(0, 1).squeeze(1)\n",
        "        return sig\n",
        "\n",
        "    sb.dataio.dataset.add_dynamic_item(datasets, audio_pipeline)\n",
        "\n",
        "    # 3. Define text pipeline:\n",
        "    @sb.utils.data_pipeline.takes(\"command\")\n",
        "    @sb.utils.data_pipeline.provides(\"command\", \"command_encoded\")\n",
        "    def label_pipeline(command):\n",
        "        yield command\n",
        "        command_encoded = label_encoder.encode_sequence_torch([command])\n",
        "        yield command_encoded\n",
        "\n",
        "    sb.dataio.dataset.add_dynamic_item(datasets, label_pipeline)\n",
        "\n",
        "    # 3. Fit encoder:\n",
        "    # Load or compute the label encoder (with multi-GPU DDP support)\n",
        "    lab_enc_file = os.path.join(hparams[\"save_folder\"], \"label_encoder.txt\")\n",
        "    label_encoder.load_or_create(\n",
        "        path=lab_enc_file,\n",
        "        from_didatasets=[train_data],\n",
        "        output_key=\"command\",\n",
        "    )\n",
        "\n",
        "    # 4. Set output:\n",
        "    sb.dataio.dataset.set_output_keys(\n",
        "        datasets, [\"id\", \"sig\", \"command_encoded\"]\n",
        "    )\n",
        "\n",
        "    return train_data, valid_data, test_data, label_encoder\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # This flag enables the inbuilt cudnn auto-tuner\n",
        "    torch.backends.cudnn.benchmark = True\n",
        "\n",
        "    # CLI:\n",
        "    hparams_file, run_opts, overrides = sb.parse_arguments(sys.argv[1:])\n",
        "\n",
        "    # Initialize ddp (useful only for multi-GPU DDP training)\n",
        "    sb.utils.distributed.ddp_init_group(run_opts)\n",
        "\n",
        "    # Load hyperparameters file with command-line overrides\n",
        "    with open(hparams_file) as fin:\n",
        "        hparams = load_hyperpyyaml(fin, overrides)\n",
        "\n",
        "    # Create experiment directory\n",
        "    sb.core.create_experiment_directory(\n",
        "        experiment_directory=hparams[\"output_folder\"],\n",
        "        hyperparams_to_save=hparams_file,\n",
        "        overrides=overrides,\n",
        "    )\n",
        "\n",
        "    # Dataset prep (parsing GSC and annotation into csv files)\n",
        "    from prepare_GSC import prepare_GSC\n",
        "\n",
        "    # Known words for V2 12 and V2 35 sets\n",
        "    if hparams[\"number_of_commands\"] == 12:\n",
        "        words_wanted = [\n",
        "            \"yes\",\n",
        "            \"no\",\n",
        "            \"up\",\n",
        "            \"down\",\n",
        "            \"left\",\n",
        "            \"right\",\n",
        "            \"on\",\n",
        "            \"off\",\n",
        "            \"stop\",\n",
        "            \"go\",\n",
        "        ]\n",
        "    elif hparams[\"number_of_commands\"] == 35:\n",
        "        words_wanted = [\n",
        "            \"yes\",\n",
        "            \"no\",\n",
        "            \"up\",\n",
        "            \"down\",\n",
        "            \"left\",\n",
        "            \"right\",\n",
        "            \"on\",\n",
        "            \"off\",\n",
        "            \"stop\",\n",
        "            \"go\",\n",
        "            \"zero\",\n",
        "            \"one\",\n",
        "            \"two\",\n",
        "            \"three\",\n",
        "            \"four\",\n",
        "            \"five\",\n",
        "            \"six\",\n",
        "            \"seven\",\n",
        "            \"eight\",\n",
        "            \"nine\",\n",
        "            \"bed\",\n",
        "            \"bird\",\n",
        "            \"cat\",\n",
        "            \"dog\",\n",
        "            \"happy\",\n",
        "            \"house\",\n",
        "            \"marvin\",\n",
        "            \"sheila\",\n",
        "            \"tree\",\n",
        "            \"wow\",\n",
        "            \"backward\",\n",
        "            \"forward\",\n",
        "            \"follow\",\n",
        "            \"learn\",\n",
        "            \"visual\",\n",
        "        ]\n",
        "    else:\n",
        "        raise ValueError(\"number_of_commands must be 12 or 35\")\n",
        "\n",
        "    # Data preparation\n",
        "    run_on_main(\n",
        "        prepare_GSC,\n",
        "        kwargs={\n",
        "            \"data_folder\": hparams[\"data_folder\"],\n",
        "            \"save_folder\": hparams[\"save_folder\"],\n",
        "            \"validation_percentage\": hparams[\"validation_percentage\"],\n",
        "            \"testing_percentage\": hparams[\"testing_percentage\"],\n",
        "            \"percentage_unknown\": hparams[\"percentage_unknown\"],\n",
        "            \"percentage_silence\": hparams[\"percentage_silence\"],\n",
        "            \"words_wanted\": words_wanted,\n",
        "            \"skip_prep\": hparams[\"skip_prep\"],\n",
        "        },\n",
        "    )\n",
        "    sb.utils.distributed.run_on_main(hparams[\"prepare_noise_data\"])\n",
        "    sb.utils.distributed.run_on_main(hparams[\"prepare_rir_data\"])\n",
        "\n",
        "    # Dataset IO prep: creating Dataset objects and proper encodings for phones\n",
        "    train_data, valid_data, test_data, label_encoder = dataio_prep(hparams)\n",
        "\n",
        "    # Brain class initialization\n",
        "    speaker_brain = SpeakerBrain(\n",
        "        modules=hparams[\"modules\"],\n",
        "        opt_class=hparams[\"opt_class\"],\n",
        "        hparams=hparams,\n",
        "        run_opts=run_opts,\n",
        "        checkpointer=hparams[\"checkpointer\"],\n",
        "    )\n",
        "\n",
        "    # with torch.autograd.detect_anomaly():\n",
        "    # Training\n",
        "    speaker_brain.fit(\n",
        "        speaker_brain.hparams.epoch_counter,\n",
        "        train_data,\n",
        "        valid_data,\n",
        "        train_loader_kwargs=hparams[\"dataloader_options\"],\n",
        "        valid_loader_kwargs=hparams[\"dataloader_options\"],\n",
        "    )\n",
        "\n",
        "    # Load the best checkpoint for evaluation\n",
        "    test_stats = speaker_brain.evaluate(\n",
        "        test_set=test_data,\n",
        "        min_key=\"ErrorRate\",\n",
        "        test_loader_kwargs=hparams[\"dataloader_options\"],\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZmJdIEaVeMVi"
      },
      "source": [
        "Remove previously generated results for wev2vec2."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dv5aOwiFurIA"
      },
      "outputs": [],
      "source": [
        "# !rm -rf ./Keyword_Spotting/results/wav2vec2/1986"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DhpZmrTleR3s"
      },
      "source": [
        "Generate the results for wav2vec2."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yCjO_m9lurWy"
      },
      "outputs": [],
      "source": [
        "!python train_self_supervised.py wav2vec2.yaml --data_folder=/content/Google_Speech_Command_Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "After running this we get something like:\n",
        "\n",
        "```\n",
        "epoch: 1, lr: 1.00e-04 - train loss: 2.52e-01 - valid loss: 5.11e-02, valid ErrorRate: 1.35e-02\n",
        "epoch: 2, lr: 1.00e-05 - train loss: 9.43e-02 - valid loss: 3.74e-02, valid ErrorRate: 1.16e-02\n",
        "Epoch loaded: 2 - test loss: 8.81e-02, test ErrorRate: 2.42e-02\n",
        "\n",
        "```"
      ],
      "metadata": {
        "id": "zOFtHeh4Q9P8"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w-gaxiIneZEJ"
      },
      "source": [
        "YAML file with HuBert, avg pooling and a linear layer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X3bTHte7Q9OZ",
        "outputId": "e3a2d3c3-df79-4e13-9dad-08c7f6d385d6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Writing hubert.yaml\n"
          ]
        }
      ],
      "source": [
        "%%file hubert.yaml\n",
        "\n",
        "# Basic parameters\n",
        "seed: 1986\n",
        "__set_seed: !apply:torch.manual_seed [!ref <seed>]\n",
        "\n",
        "# Use 12 for V2 12 task and 35 for V2 35 task\n",
        "number_of_commands: 12\n",
        "output_folder: !ref /content/Keyword_Spotting/results/hubert/<seed>\n",
        "save_folder: !ref <output_folder>/save\n",
        "train_log: !ref <output_folder>/train_log.txt\n",
        "\n",
        "# Data for augmentation\n",
        "NOISE_DATASET_URL: https://www.dropbox.com/scl/fi/a09pj97s5ifan81dqhi4n/noises.zip?rlkey=j8b0n9kdjdr32o1f06t0cw5b7&dl=1\n",
        "RIR_DATASET_URL: https://www.dropbox.com/scl/fi/linhy77c36mu10965a836/RIRs.zip?rlkey=pg9cu8vrpn2u173vhiqyu743u&dl=1\n",
        "\n",
        "# Data files\n",
        "data_folder: !PLACEHOLDER  # e.g. /path/to/GSC\n",
        "data_folder_noise: !ref <data_folder>/noise # The noisy sequences for data augmentation will automatically be downloaded here.\n",
        "data_folder_rir: !ref <data_folder>/rir # The impulse responses used for data augmentation will automatically be downloaded here.\n",
        "train_annotation: !ref <save_folder>/train.csv\n",
        "valid_annotation: !ref <save_folder>/valid.csv\n",
        "test_annotation: !ref <save_folder>/test.csv\n",
        "noise_annotation: !ref <save_folder>/noise.csv\n",
        "rir_annotation: !ref <save_folder>/rir.csv\n",
        "\n",
        "# Percentage of files used for validation and test\n",
        "validation_percentage: 5\n",
        "testing_percentage: 5\n",
        "\n",
        "# Percentage of unknown and silence examples\n",
        "# (relative to total of known word samples) to include\n",
        "percentage_unknown: 10 # Set this to 0 for the V2 35 task\n",
        "percentage_silence: 10 # Set this to 0 for the V2 35 task\n",
        "\n",
        "skip_prep: False\n",
        "ckpt_interval_minutes: 15 # save checkpoint every N min\n",
        "\n",
        "####################### Training Parameters ####################################\n",
        "number_of_epochs: 2\n",
        "batch_size: 16\n",
        "lr: 0.0001\n",
        "lr_final: 0.00001\n",
        "\n",
        "sample_rate: 16000\n",
        "shuffle: True\n",
        "\n",
        "\n",
        "# Feature parameters\n",
        "n_mels: 24\n",
        "left_frames: 0\n",
        "right_frames: 0\n",
        "deltas: False\n",
        "\n",
        "# Number of classes (i.e. different commands)\n",
        "out_n_neurons: !ref <number_of_commands>  #includes core commands & auxiliary words\n",
        "\n",
        "num_workers: 2\n",
        "dataloader_options:\n",
        "    batch_size: !ref <batch_size>\n",
        "    shuffle: !ref <shuffle>\n",
        "    num_workers: !ref <num_workers>\n",
        "\n",
        "# Functions\n",
        "compute_features: !new:speechbrain.lobes.features.Fbank\n",
        "    n_mels: !ref <n_mels>\n",
        "    left_frames: !ref <left_frames>\n",
        "    right_frames: !ref <right_frames>\n",
        "    deltas: !ref <deltas>\n",
        "\n",
        "embedding_model: !new:speechbrain.lobes.models.Xvector.Xvector\n",
        "    in_channels: !ref <n_mels>\n",
        "    activation: !name:torch.nn.LeakyReLU\n",
        "    tdnn_blocks: 5\n",
        "    tdnn_channels: [512, 512, 512, 512, 1500]\n",
        "    tdnn_kernel_sizes: [5, 3, 3, 1, 1]\n",
        "    tdnn_dilations: [1, 2, 3, 1, 1]\n",
        "    lin_neurons: 512\n",
        "\n",
        "freeze_ssl: False\n",
        "freeze_ssl_conv: True\n",
        "sslmodel_hub: facebook/hubert-base-ls960\n",
        "sslmodel_folder: !ref <save_folder>/ssl_checkpoint\n",
        "encoder_dim: 768\n",
        "\n",
        "# hubert\n",
        "ssl_model: !new:speechbrain.lobes.models.huggingface_transformers.hubert.HuBERT\n",
        "   source: !ref <sslmodel_hub>\n",
        "   output_norm: True\n",
        "   freeze: !ref <freeze_ssl>\n",
        "   freeze_feature_extractor: !ref <freeze_ssl_conv>\n",
        "   output_all_hiddens: False\n",
        "   save_path: !ref <sslmodel_folder>\n",
        "\n",
        "avg_pool: !new:speechbrain.nnet.pooling.StatisticsPooling\n",
        "    return_std: False\n",
        "\n",
        "output_mlp: !new:speechbrain.nnet.linear.Linear\n",
        "    input_size: !ref <encoder_dim>\n",
        "    n_neurons: !ref <out_n_neurons>\n",
        "    bias: False\n",
        "\n",
        "classifier: !new:speechbrain.lobes.models.Xvector.Classifier\n",
        "    input_shape: [null, null, 512]\n",
        "    activation: !name:torch.nn.LeakyReLU\n",
        "    lin_blocks: 1\n",
        "    lin_neurons: 512\n",
        "    out_neurons: !ref <out_n_neurons>\n",
        "\n",
        "softmax: !new:speechbrain.nnet.activations.Softmax\n",
        "    apply_log: True\n",
        "\n",
        "epoch_counter: !new:speechbrain.utils.epoch_loop.EpochCounter\n",
        "    limit: !ref <number_of_epochs>\n",
        "\n",
        "\n",
        "# Download and prepare the dataset of noisy sequences for augmentation\n",
        "prepare_noise_data: !name:speechbrain.augment.preparation.prepare_dataset_from_URL\n",
        "    URL: !ref <NOISE_DATASET_URL>\n",
        "    dest_folder: !ref <data_folder_noise>\n",
        "    ext: wav\n",
        "    csv_file: !ref <noise_annotation>\n",
        "\n",
        "# Add noise to input signal\n",
        "snr_low: 0  # Min SNR for noise augmentation\n",
        "snr_high: 15  # Max SNR for noise augmentation\n",
        "\n",
        "add_noise: !new:speechbrain.augment.time_domain.AddNoise\n",
        "    csv_file: !ref <noise_annotation>\n",
        "    snr_low: !ref <snr_low>\n",
        "    snr_high: !ref <snr_high>\n",
        "    noise_sample_rate: !ref <sample_rate>\n",
        "    clean_sample_rate: !ref <sample_rate>\n",
        "    num_workers: !ref <num_workers>\n",
        "\n",
        "\n",
        "# Download and prepare the dataset of room impulse responses for augmentation\n",
        "prepare_rir_data: !name:speechbrain.augment.preparation.prepare_dataset_from_URL\n",
        "    URL: !ref <RIR_DATASET_URL>\n",
        "    dest_folder: !ref <data_folder_rir>\n",
        "    ext: wav\n",
        "    csv_file: !ref <rir_annotation>\n",
        "\n",
        "# Add reverberation to input signal\n",
        "add_reverb: !new:speechbrain.augment.time_domain.AddReverb\n",
        "    csv_file: !ref <rir_annotation>\n",
        "    reverb_sample_rate: !ref <sample_rate>\n",
        "    clean_sample_rate: !ref <sample_rate>\n",
        "    num_workers: !ref <num_workers>\n",
        "\n",
        "# Frequency drop: randomly drops a number of frequency bands to zero.\n",
        "drop_freq_low: 0  # Min frequency band dropout probability\n",
        "drop_freq_high: 1  # Max frequency band dropout probability\n",
        "drop_freq_count_low: 1  # Min number of frequency bands to drop\n",
        "drop_freq_count_high: 3  # Max number of frequency bands to drop\n",
        "drop_freq_width: 0.05  # Width of frequency bands to drop\n",
        "\n",
        "drop_freq: !new:speechbrain.augment.time_domain.DropFreq\n",
        "    drop_freq_low: !ref <drop_freq_low>\n",
        "    drop_freq_high: !ref <drop_freq_high>\n",
        "    drop_freq_count_low: !ref <drop_freq_count_low>\n",
        "    drop_freq_count_high: !ref <drop_freq_count_high>\n",
        "    drop_freq_width: !ref <drop_freq_width>\n",
        "\n",
        "# Time drop: randomly drops a number of temporal chunks.\n",
        "drop_chunk_count_low: 1  # Min number of audio chunks to drop\n",
        "drop_chunk_count_high: 5  # Max number of audio chunks to drop\n",
        "drop_chunk_length_low: 1000  # Min length of audio chunks to drop\n",
        "drop_chunk_length_high: 2000  # Max length of audio chunks to drop\n",
        "\n",
        "drop_chunk: !new:speechbrain.augment.time_domain.DropChunk\n",
        "    drop_length_low: !ref <drop_chunk_length_low>\n",
        "    drop_length_high: !ref <drop_chunk_length_high>\n",
        "    drop_count_low: !ref <drop_chunk_count_low>\n",
        "    drop_count_high: !ref <drop_chunk_count_high>\n",
        "\n",
        "# Augmenter: Combines previously defined augmentations to perform data augmentation\n",
        "wav_augment: !new:speechbrain.augment.augmenter.Augmenter\n",
        "    parallel_augment: True\n",
        "    concat_original: True\n",
        "    repeat_augment: 1\n",
        "    shuffle_augmentations: False\n",
        "    min_augmentations: 4\n",
        "    max_augmentations: 4\n",
        "    augment_prob: 1.0\n",
        "    augmentations: [\n",
        "        !ref <add_noise>,\n",
        "        !ref <add_reverb>,\n",
        "        !ref <drop_freq>,\n",
        "        !ref <drop_chunk>]\n",
        "\n",
        "mean_var_norm: !new:speechbrain.processing.features.InputNormalization\n",
        "    norm_type: sentence\n",
        "    std_norm: False\n",
        "\n",
        "modules:\n",
        "    compute_features: !ref <compute_features>\n",
        "    embedding_model: !ref <embedding_model>\n",
        "    classifier: !ref <classifier>\n",
        "    softmax: !ref <softmax>\n",
        "    mean_var_norm: !ref <mean_var_norm>\n",
        "    ssl_model: !ref <ssl_model>\n",
        "    output_mlp: !ref <output_mlp>\n",
        "\n",
        "\n",
        "# Cost + optimization\n",
        "compute_cost: !name:speechbrain.nnet.losses.nll_loss\n",
        "# compute_error: !name:speechbrain.nnet.losses.classification_error\n",
        "\n",
        "opt_class: !name:torch.optim.Adam\n",
        "    lr: !ref <lr>\n",
        "    weight_decay: 0.000002\n",
        "\n",
        "lr_annealing: !new:speechbrain.nnet.schedulers.LinearScheduler\n",
        "    initial_value: !ref <lr>\n",
        "    final_value: !ref <lr_final>\n",
        "    epoch_count: !ref <number_of_epochs>\n",
        "\n",
        "# Logging + checkpoints\n",
        "train_logger: !new:speechbrain.utils.train_logger.FileTrainLogger\n",
        "    save_file: !ref <train_log>\n",
        "\n",
        "error_stats: !name:speechbrain.utils.metric_stats.MetricStats\n",
        "    metric: !name:speechbrain.nnet.losses.classification_error\n",
        "        reduction: batch\n",
        "\n",
        "checkpointer: !new:speechbrain.utils.checkpoints.Checkpointer\n",
        "    checkpoints_dir: !ref <save_folder>\n",
        "    recoverables:\n",
        "        embedding_model: !ref <embedding_model>\n",
        "        classifier: !ref <classifier>\n",
        "        normalizer: !ref <mean_var_norm>\n",
        "        counter: !ref <epoch_counter>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1_GScfuUe7Dg"
      },
      "source": [
        "Remove previously generated results for HuBert."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tmts7zFoR0cR"
      },
      "outputs": [],
      "source": [
        "# !rm -rf ./Keyword_Spotting/results/hubert/1986"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hs03nsMMe9si"
      },
      "source": [
        "Generate the results for HuBert."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rTn0TB77R0wq"
      },
      "outputs": [],
      "source": [
        "!python train_self_supervised.py hubert.yaml --data_folder=/content/Google_Speech_Command_Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mHcyaV6Uesfm"
      },
      "source": [
        "After running it, we get result like:\n",
        "```\n",
        "epoch: 1, lr: 1.00e-04 - train loss: 2.74e-01 - valid loss: 6.30e-02, valid ErrorRate: 1.69e-02\n",
        "epoch: 2, lr: 1.00e-05 - train loss: 1.05e-01 - valid loss: 3.98e-02, valid ErrorRate: 1.12e-02\n",
        "Epoch loaded: 2 - test loss: 7.90e-02, test ErrorRate: 2.19e-02\n",
        "\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1cnuA3s_el6w"
      },
      "source": [
        "YAML file with WavLM, avg pooling and a linear layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KexmCi9xWocZ",
        "outputId": "c758af16-1181-4632-9110-ada6fe0a2619"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting wavlm.yaml\n"
          ]
        }
      ],
      "source": [
        "%%file wavlm.yaml\n",
        "\n",
        "# Basic parameters\n",
        "seed: 1986\n",
        "__set_seed: !apply:torch.manual_seed [!ref <seed>]\n",
        "\n",
        "# Use 12 for V2 12 task and 35 for V2 35 task\n",
        "number_of_commands: 12\n",
        "output_folder: !ref /content/Keyword_Spotting/results/wavlm/<seed>\n",
        "save_folder: !ref <output_folder>/save\n",
        "train_log: !ref <output_folder>/train_log.txt\n",
        "\n",
        "# Data for augmentation\n",
        "NOISE_DATASET_URL: https://www.dropbox.com/scl/fi/a09pj97s5ifan81dqhi4n/noises.zip?rlkey=j8b0n9kdjdr32o1f06t0cw5b7&dl=1\n",
        "RIR_DATASET_URL: https://www.dropbox.com/scl/fi/linhy77c36mu10965a836/RIRs.zip?rlkey=pg9cu8vrpn2u173vhiqyu743u&dl=1\n",
        "\n",
        "# Data files\n",
        "data_folder: !PLACEHOLDER  # e.g. /path/to/GSC\n",
        "data_folder_noise: !ref <data_folder>/noise # The noisy sequences for data augmentation will automatically be downloaded here.\n",
        "data_folder_rir: !ref <data_folder>/rir # The impulse responses used for data augmentation will automatically be downloaded here.\n",
        "train_annotation: !ref <save_folder>/train.csv\n",
        "valid_annotation: !ref <save_folder>/valid.csv\n",
        "test_annotation: !ref <save_folder>/test.csv\n",
        "noise_annotation: !ref <save_folder>/noise.csv\n",
        "rir_annotation: !ref <save_folder>/rir.csv\n",
        "\n",
        "# Percentage of files used for validation and test\n",
        "validation_percentage: 5\n",
        "testing_percentage: 5\n",
        "\n",
        "# Percentage of unknown and silence examples\n",
        "# (relative to total of known word samples) to include\n",
        "percentage_unknown: 10 # Set this to 0 for the V2 35 task\n",
        "percentage_silence: 10 # Set this to 0 for the V2 35 task\n",
        "\n",
        "skip_prep: False\n",
        "ckpt_interval_minutes: 15 # save checkpoint every N min\n",
        "\n",
        "####################### Training Parameters ####################################\n",
        "number_of_epochs: 2\n",
        "batch_size: 16\n",
        "lr: 0.0001\n",
        "lr_final: 0.00001\n",
        "\n",
        "sample_rate: 16000\n",
        "shuffle: True\n",
        "\n",
        "\n",
        "# Feature parameters\n",
        "n_mels: 24\n",
        "left_frames: 0\n",
        "right_frames: 0\n",
        "deltas: False\n",
        "\n",
        "# Number of classes (i.e. different commands)\n",
        "out_n_neurons: !ref <number_of_commands>  #includes core commands & auxiliary words\n",
        "\n",
        "num_workers: 2\n",
        "dataloader_options:\n",
        "    batch_size: !ref <batch_size>\n",
        "    shuffle: !ref <shuffle>\n",
        "    num_workers: !ref <num_workers>\n",
        "\n",
        "# Functions\n",
        "compute_features: !new:speechbrain.lobes.features.Fbank\n",
        "    n_mels: !ref <n_mels>\n",
        "    left_frames: !ref <left_frames>\n",
        "    right_frames: !ref <right_frames>\n",
        "    deltas: !ref <deltas>\n",
        "\n",
        "embedding_model: !new:speechbrain.lobes.models.Xvector.Xvector\n",
        "    in_channels: !ref <n_mels>\n",
        "    activation: !name:torch.nn.LeakyReLU\n",
        "    tdnn_blocks: 5\n",
        "    tdnn_channels: [512, 512, 512, 512, 1500]\n",
        "    tdnn_kernel_sizes: [5, 3, 3, 1, 1]\n",
        "    tdnn_dilations: [1, 2, 3, 1, 1]\n",
        "    lin_neurons: 512\n",
        "\n",
        "freeze_ssl: False\n",
        "freeze_ssl_conv: True\n",
        "sslmodel_hub: microsoft/wavlm-base\n",
        "sslmodel_folder: !ref <save_folder>/ssl_checkpoint\n",
        "encoder_dim: 768\n",
        "\n",
        "# wavLM\n",
        "ssl_model: !new:speechbrain.lobes.models.huggingface_transformers.wavlm.WavLM\n",
        "   source: !ref <sslmodel_hub>\n",
        "   output_norm: True\n",
        "   freeze: !ref <freeze_ssl>\n",
        "   output_all_hiddens: False\n",
        "   save_path: !ref <sslmodel_folder>\n",
        "\n",
        "avg_pool: !new:speechbrain.nnet.pooling.StatisticsPooling\n",
        "    return_std: False\n",
        "\n",
        "output_mlp: !new:speechbrain.nnet.linear.Linear\n",
        "    input_size: !ref <encoder_dim>\n",
        "    n_neurons: !ref <out_n_neurons>\n",
        "    bias: False\n",
        "\n",
        "classifier: !new:speechbrain.lobes.models.Xvector.Classifier\n",
        "    input_shape: [null, null, 512]\n",
        "    activation: !name:torch.nn.LeakyReLU\n",
        "    lin_blocks: 1\n",
        "    lin_neurons: 512\n",
        "    out_neurons: !ref <out_n_neurons>\n",
        "\n",
        "softmax: !new:speechbrain.nnet.activations.Softmax\n",
        "    apply_log: True\n",
        "\n",
        "epoch_counter: !new:speechbrain.utils.epoch_loop.EpochCounter\n",
        "    limit: !ref <number_of_epochs>\n",
        "\n",
        "\n",
        "# Download and prepare the dataset of noisy sequences for augmentation\n",
        "prepare_noise_data: !name:speechbrain.augment.preparation.prepare_dataset_from_URL\n",
        "    URL: !ref <NOISE_DATASET_URL>\n",
        "    dest_folder: !ref <data_folder_noise>\n",
        "    ext: wav\n",
        "    csv_file: !ref <noise_annotation>\n",
        "\n",
        "# Add noise to input signal\n",
        "snr_low: 0  # Min SNR for noise augmentation\n",
        "snr_high: 15  # Max SNR for noise augmentation\n",
        "\n",
        "add_noise: !new:speechbrain.augment.time_domain.AddNoise\n",
        "    csv_file: !ref <noise_annotation>\n",
        "    snr_low: !ref <snr_low>\n",
        "    snr_high: !ref <snr_high>\n",
        "    noise_sample_rate: !ref <sample_rate>\n",
        "    clean_sample_rate: !ref <sample_rate>\n",
        "    num_workers: !ref <num_workers>\n",
        "\n",
        "\n",
        "# Download and prepare the dataset of room impulse responses for augmentation\n",
        "prepare_rir_data: !name:speechbrain.augment.preparation.prepare_dataset_from_URL\n",
        "    URL: !ref <RIR_DATASET_URL>\n",
        "    dest_folder: !ref <data_folder_rir>\n",
        "    ext: wav\n",
        "    csv_file: !ref <rir_annotation>\n",
        "\n",
        "# Add reverberation to input signal\n",
        "add_reverb: !new:speechbrain.augment.time_domain.AddReverb\n",
        "    csv_file: !ref <rir_annotation>\n",
        "    reverb_sample_rate: !ref <sample_rate>\n",
        "    clean_sample_rate: !ref <sample_rate>\n",
        "    num_workers: !ref <num_workers>\n",
        "\n",
        "# Frequency drop: randomly drops a number of frequency bands to zero.\n",
        "drop_freq_low: 0  # Min frequency band dropout probability\n",
        "drop_freq_high: 1  # Max frequency band dropout probability\n",
        "drop_freq_count_low: 1  # Min number of frequency bands to drop\n",
        "drop_freq_count_high: 3  # Max number of frequency bands to drop\n",
        "drop_freq_width: 0.05  # Width of frequency bands to drop\n",
        "\n",
        "drop_freq: !new:speechbrain.augment.time_domain.DropFreq\n",
        "    drop_freq_low: !ref <drop_freq_low>\n",
        "    drop_freq_high: !ref <drop_freq_high>\n",
        "    drop_freq_count_low: !ref <drop_freq_count_low>\n",
        "    drop_freq_count_high: !ref <drop_freq_count_high>\n",
        "    drop_freq_width: !ref <drop_freq_width>\n",
        "\n",
        "# Time drop: randomly drops a number of temporal chunks.\n",
        "drop_chunk_count_low: 1  # Min number of audio chunks to drop\n",
        "drop_chunk_count_high: 5  # Max number of audio chunks to drop\n",
        "drop_chunk_length_low: 1000  # Min length of audio chunks to drop\n",
        "drop_chunk_length_high: 2000  # Max length of audio chunks to drop\n",
        "\n",
        "drop_chunk: !new:speechbrain.augment.time_domain.DropChunk\n",
        "    drop_length_low: !ref <drop_chunk_length_low>\n",
        "    drop_length_high: !ref <drop_chunk_length_high>\n",
        "    drop_count_low: !ref <drop_chunk_count_low>\n",
        "    drop_count_high: !ref <drop_chunk_count_high>\n",
        "\n",
        "# Augmenter: Combines previously defined augmentations to perform data augmentation\n",
        "wav_augment: !new:speechbrain.augment.augmenter.Augmenter\n",
        "    parallel_augment: True\n",
        "    concat_original: True\n",
        "    repeat_augment: 1\n",
        "    shuffle_augmentations: False\n",
        "    min_augmentations: 4\n",
        "    max_augmentations: 4\n",
        "    augment_prob: 1.0\n",
        "    augmentations: [\n",
        "        !ref <add_noise>,\n",
        "        !ref <add_reverb>,\n",
        "        !ref <drop_freq>,\n",
        "        !ref <drop_chunk>]\n",
        "\n",
        "mean_var_norm: !new:speechbrain.processing.features.InputNormalization\n",
        "    norm_type: sentence\n",
        "    std_norm: False\n",
        "\n",
        "modules:\n",
        "    compute_features: !ref <compute_features>\n",
        "    embedding_model: !ref <embedding_model>\n",
        "    classifier: !ref <classifier>\n",
        "    softmax: !ref <softmax>\n",
        "    mean_var_norm: !ref <mean_var_norm>\n",
        "    ssl_model: !ref <ssl_model>\n",
        "    output_mlp: !ref <output_mlp>\n",
        "\n",
        "\n",
        "# Cost + optimization\n",
        "compute_cost: !name:speechbrain.nnet.losses.nll_loss\n",
        "# compute_error: !name:speechbrain.nnet.losses.classification_error\n",
        "\n",
        "opt_class: !name:torch.optim.Adam\n",
        "    lr: !ref <lr>\n",
        "    weight_decay: 0.000002\n",
        "\n",
        "lr_annealing: !new:speechbrain.nnet.schedulers.LinearScheduler\n",
        "    initial_value: !ref <lr>\n",
        "    final_value: !ref <lr_final>\n",
        "    epoch_count: !ref <number_of_epochs>\n",
        "\n",
        "# Logging + checkpoints\n",
        "train_logger: !new:speechbrain.utils.train_logger.FileTrainLogger\n",
        "    save_file: !ref <train_log>\n",
        "\n",
        "error_stats: !name:speechbrain.utils.metric_stats.MetricStats\n",
        "    metric: !name:speechbrain.nnet.losses.classification_error\n",
        "        reduction: batch\n",
        "\n",
        "checkpointer: !new:speechbrain.utils.checkpoints.Checkpointer\n",
        "    checkpoints_dir: !ref <save_folder>\n",
        "    recoverables:\n",
        "        embedding_model: !ref <embedding_model>\n",
        "        classifier: !ref <classifier>\n",
        "        normalizer: !ref <mean_var_norm>\n",
        "        counter: !ref <epoch_counter>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fQjlwt8OfQmz"
      },
      "source": [
        "Remove previously generated results for WavLM."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QYUF95I6XDXC"
      },
      "outputs": [],
      "source": [
        "# !rm -rf ./Keyword_Spotting/results/wavlm/1986"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rT13hUKDfGjr"
      },
      "source": [
        "Generate the results for WavLM."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6EoZm7YMXE5-"
      },
      "outputs": [],
      "source": [
        "!python train_self_supervised.py wavlm.yaml --data_folder=/content/Google_Speech_Command_Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NDoN32VQfMVP"
      },
      "source": [
        "After running it, we get result like:\n",
        "```\n",
        "epoch: 1, lr: 1.00e-04 - train loss: 2.19e-01 - valid loss: 5.27e-02, valid ErrorRate: 1.53e-02\n",
        "epoch: 2, lr: 1.00e-05 - train loss: 8.33e-02 - valid loss: 2.82e-02, valid ErrorRate: 7.88e-03\n",
        "Epoch loaded: 2 - test loss: 6.54e-02, test ErrorRate: 1.63e-02\n",
        "\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZcxVA9uxHR7k"
      },
      "source": [
        "YAML file for Discrete Audio Feature using Encodec and a MLP on top."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XbCU0QngHbti",
        "outputId": "ea8975fe-90de-4f79-d067-747fbaec1f26"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting encodac_mlp.yaml\n"
          ]
        }
      ],
      "source": [
        "%%file encodec_mlp.yaml\n",
        "\n",
        "# Basic parameters\n",
        "seed: 1986\n",
        "__set_seed: !apply:torch.manual_seed [!ref <seed>]\n",
        "\n",
        "# Use 12 for V2 12 task and 35 for V2 35 task\n",
        "number_of_commands: 12\n",
        "output_folder: !ref /content/Keyword_Spotting/results/encodec_mlp/<seed>\n",
        "save_folder: !ref <output_folder>/save\n",
        "train_log: !ref <output_folder>/train_log.txt\n",
        "\n",
        "# Data for augmentation\n",
        "NOISE_DATASET_URL: https://www.dropbox.com/scl/fi/a09pj97s5ifan81dqhi4n/noises.zip?rlkey=j8b0n9kdjdr32o1f06t0cw5b7&dl=1\n",
        "RIR_DATASET_URL: https://www.dropbox.com/scl/fi/linhy77c36mu10965a836/RIRs.zip?rlkey=pg9cu8vrpn2u173vhiqyu743u&dl=1\n",
        "\n",
        "# Data files\n",
        "data_folder: !PLACEHOLDER  # e.g. /path/to/GSC\n",
        "data_folder_noise: !ref <data_folder>/noise # The noisy sequences for data augmentation will automatically be downloaded here.\n",
        "data_folder_rir: !ref <data_folder>/rir # The impulse responses used for data augmentation will automatically be downloaded here.\n",
        "train_annotation: !ref <save_folder>/train.csv\n",
        "valid_annotation: !ref <save_folder>/valid.csv\n",
        "test_annotation: !ref <save_folder>/test.csv\n",
        "noise_annotation: !ref <save_folder>/noise.csv\n",
        "rir_annotation: !ref <save_folder>/rir.csv\n",
        "\n",
        "# Percentage of files used for validation and test\n",
        "validation_percentage: 5\n",
        "testing_percentage: 5\n",
        "\n",
        "# Percentage of unknown and silence examples\n",
        "# (relative to total of known word samples) to include\n",
        "percentage_unknown: 10 # Set this to 0 for the V2 35 task\n",
        "percentage_silence: 10 # Set this to 0 for the V2 35 task\n",
        "\n",
        "skip_prep: False\n",
        "ckpt_interval_minutes: 15 # save checkpoint every N min\n",
        "\n",
        "####################### Training Parameters ####################################\n",
        "number_of_epochs: 5\n",
        "batch_size: 16\n",
        "lr: 0.0001\n",
        "lr_final: 0.00001\n",
        "\n",
        "sample_rate: 16000\n",
        "shuffle: True\n",
        "\n",
        "\n",
        "# Feature parameters\n",
        "n_mels: 24\n",
        "left_frames: 0\n",
        "right_frames: 0\n",
        "deltas: False\n",
        "\n",
        "# Number of classes (i.e. different commands)\n",
        "out_n_neurons: !ref <number_of_commands>  #includes core commands & auxiliary words\n",
        "\n",
        "num_workers: 2\n",
        "dataloader_options:\n",
        "    batch_size: !ref <batch_size>\n",
        "    shuffle: !ref <shuffle>\n",
        "    num_workers: !ref <num_workers>\n",
        "\n",
        "# Functions\n",
        "compute_features: !new:speechbrain.lobes.features.Fbank\n",
        "    n_mels: !ref <n_mels>\n",
        "    left_frames: !ref <left_frames>\n",
        "    right_frames: !ref <right_frames>\n",
        "    deltas: !ref <deltas>\n",
        "\n",
        "embedding_model: !new:speechbrain.lobes.models.Xvector.Xvector\n",
        "    in_channels: !ref <n_mels>\n",
        "    activation: !name:torch.nn.LeakyReLU\n",
        "    tdnn_blocks: 5\n",
        "    tdnn_channels: [512, 512, 512, 512, 1500]\n",
        "    tdnn_kernel_sizes: [5, 3, 3, 1, 1]\n",
        "    tdnn_dilations: [1, 2, 3, 1, 1]\n",
        "    lin_neurons: 512\n",
        "\n",
        "freeze_feat: False\n",
        "feat_model_hub: facebook/encodec_24khz\n",
        "feat_model_folder: !ref <save_folder>/ssl_checkpoint\n",
        "encoder_dim: 256\n",
        "\n",
        "# Encodec\n",
        "feat_model: !new:speechbrain.lobes.models.huggingface_transformers.encodec.Encodec\n",
        "    source: !ref <feat_model_hub>\n",
        "    save_path: !ref <feat_model_folder>\n",
        "    sample_rate: !ref <sample_rate>\n",
        "    flat_embeddings : True\n",
        "    freeze: !ref <freeze_feat>\n",
        "    renorm_embeddings : False\n",
        "\n",
        "avg_pool: !new:speechbrain.nnet.pooling.StatisticsPooling\n",
        "    return_std: False\n",
        "\n",
        "output_mlp: !new:speechbrain.nnet.linear.Linear\n",
        "    input_size: !ref <encoder_dim>\n",
        "    n_neurons: !ref <out_n_neurons>\n",
        "    bias: False\n",
        "\n",
        "classifier: !new:speechbrain.lobes.models.Xvector.Classifier\n",
        "    input_shape: [null, null, 512]\n",
        "    activation: !name:torch.nn.LeakyReLU\n",
        "    lin_blocks: 1\n",
        "    lin_neurons: 512\n",
        "    out_neurons: !ref <out_n_neurons>\n",
        "\n",
        "softmax: !new:speechbrain.nnet.activations.Softmax\n",
        "    apply_log: True\n",
        "\n",
        "epoch_counter: !new:speechbrain.utils.epoch_loop.EpochCounter\n",
        "    limit: !ref <number_of_epochs>\n",
        "\n",
        "\n",
        "# Download and prepare the dataset of noisy sequences for augmentation\n",
        "prepare_noise_data: !name:speechbrain.augment.preparation.prepare_dataset_from_URL\n",
        "    URL: !ref <NOISE_DATASET_URL>\n",
        "    dest_folder: !ref <data_folder_noise>\n",
        "    ext: wav\n",
        "    csv_file: !ref <noise_annotation>\n",
        "\n",
        "# Add noise to input signal\n",
        "snr_low: 0  # Min SNR for noise augmentation\n",
        "snr_high: 15  # Max SNR for noise augmentation\n",
        "\n",
        "add_noise: !new:speechbrain.augment.time_domain.AddNoise\n",
        "    csv_file: !ref <noise_annotation>\n",
        "    snr_low: !ref <snr_low>\n",
        "    snr_high: !ref <snr_high>\n",
        "    noise_sample_rate: !ref <sample_rate>\n",
        "    clean_sample_rate: !ref <sample_rate>\n",
        "    num_workers: !ref <num_workers>\n",
        "\n",
        "\n",
        "# Download and prepare the dataset of room impulse responses for augmentation\n",
        "prepare_rir_data: !name:speechbrain.augment.preparation.prepare_dataset_from_URL\n",
        "    URL: !ref <RIR_DATASET_URL>\n",
        "    dest_folder: !ref <data_folder_rir>\n",
        "    ext: wav\n",
        "    csv_file: !ref <rir_annotation>\n",
        "\n",
        "# Add reverberation to input signal\n",
        "add_reverb: !new:speechbrain.augment.time_domain.AddReverb\n",
        "    csv_file: !ref <rir_annotation>\n",
        "    reverb_sample_rate: !ref <sample_rate>\n",
        "    clean_sample_rate: !ref <sample_rate>\n",
        "    num_workers: !ref <num_workers>\n",
        "\n",
        "# Frequency drop: randomly drops a number of frequency bands to zero.\n",
        "drop_freq_low: 0  # Min frequency band dropout probability\n",
        "drop_freq_high: 1  # Max frequency band dropout probability\n",
        "drop_freq_count_low: 1  # Min number of frequency bands to drop\n",
        "drop_freq_count_high: 3  # Max number of frequency bands to drop\n",
        "drop_freq_width: 0.05  # Width of frequency bands to drop\n",
        "\n",
        "drop_freq: !new:speechbrain.augment.time_domain.DropFreq\n",
        "    drop_freq_low: !ref <drop_freq_low>\n",
        "    drop_freq_high: !ref <drop_freq_high>\n",
        "    drop_freq_count_low: !ref <drop_freq_count_low>\n",
        "    drop_freq_count_high: !ref <drop_freq_count_high>\n",
        "    drop_freq_width: !ref <drop_freq_width>\n",
        "\n",
        "# Time drop: randomly drops a number of temporal chunks.\n",
        "drop_chunk_count_low: 1  # Min number of audio chunks to drop\n",
        "drop_chunk_count_high: 5  # Max number of audio chunks to drop\n",
        "drop_chunk_length_low: 1000  # Min length of audio chunks to drop\n",
        "drop_chunk_length_high: 2000  # Max length of audio chunks to drop\n",
        "\n",
        "drop_chunk: !new:speechbrain.augment.time_domain.DropChunk\n",
        "    drop_length_low: !ref <drop_chunk_length_low>\n",
        "    drop_length_high: !ref <drop_chunk_length_high>\n",
        "    drop_count_low: !ref <drop_chunk_count_low>\n",
        "    drop_count_high: !ref <drop_chunk_count_high>\n",
        "\n",
        "# Augmenter: Combines previously defined augmentations to perform data augmentation\n",
        "wav_augment: !new:speechbrain.augment.augmenter.Augmenter\n",
        "    parallel_augment: True\n",
        "    concat_original: True\n",
        "    repeat_augment: 1\n",
        "    shuffle_augmentations: False\n",
        "    min_augmentations: 4\n",
        "    max_augmentations: 4\n",
        "    augment_prob: 1.0\n",
        "    augmentations: [\n",
        "        !ref <add_noise>,\n",
        "        !ref <add_reverb>,\n",
        "        !ref <drop_freq>,\n",
        "        !ref <drop_chunk>]\n",
        "\n",
        "mean_var_norm: !new:speechbrain.processing.features.InputNormalization\n",
        "    norm_type: sentence\n",
        "    std_norm: False\n",
        "\n",
        "modules:\n",
        "    compute_features: !ref <compute_features>\n",
        "    embedding_model: !ref <embedding_model>\n",
        "    classifier: !ref <classifier>\n",
        "    softmax: !ref <softmax>\n",
        "    mean_var_norm: !ref <mean_var_norm>\n",
        "    feat_model: !ref <feat_model>\n",
        "    output_mlp: !ref <output_mlp>\n",
        "\n",
        "# Cost + optimization\n",
        "compute_cost: !name:speechbrain.nnet.losses.nll_loss\n",
        "# compute_error: !name:speechbrain.nnet.losses.classification_error\n",
        "\n",
        "opt_class: !name:torch.optim.Adam\n",
        "    lr: !ref <lr>\n",
        "    weight_decay: 0.000002\n",
        "\n",
        "lr_annealing: !new:speechbrain.nnet.schedulers.LinearScheduler\n",
        "    initial_value: !ref <lr>\n",
        "    final_value: !ref <lr_final>\n",
        "    epoch_count: !ref <number_of_epochs>\n",
        "\n",
        "# Logging + checkpoints\n",
        "train_logger: !new:speechbrain.utils.train_logger.FileTrainLogger\n",
        "    save_file: !ref <train_log>\n",
        "\n",
        "error_stats: !name:speechbrain.utils.metric_stats.MetricStats\n",
        "    metric: !name:speechbrain.nnet.losses.classification_error\n",
        "        reduction: batch\n",
        "\n",
        "checkpointer: !new:speechbrain.utils.checkpoints.Checkpointer\n",
        "    checkpoints_dir: !ref <save_folder>\n",
        "    recoverables:\n",
        "        embedding_model: !ref <embedding_model>\n",
        "        classifier: !ref <classifier>\n",
        "        normalizer: !ref <mean_var_norm>\n",
        "        counter: !ref <epoch_counter>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EUXPy6FDMI37"
      },
      "source": [
        "Train file with discrete audio features and a MLP on top."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KUBzFC2EMyop",
        "outputId": "6a326df4-f88d-43be-914e-d4a61dab8297"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting train_discrete_mlp.py\n"
          ]
        }
      ],
      "source": [
        "%%file train_discrete_mlp.py\n",
        "\n",
        "#!/usr/bin/python3\n",
        "import os\n",
        "import sys\n",
        "import torch\n",
        "import torchaudio\n",
        "import speechbrain as sb\n",
        "from hyperpyyaml import load_hyperpyyaml\n",
        "\n",
        "import speechbrain.nnet.CNN\n",
        "from speechbrain.utils.distributed import run_on_main\n",
        "\n",
        "\n",
        "class SpeakerBrain(sb.core.Brain):\n",
        "    \"\"\"Class for GSC training\" \"\"\"\n",
        "\n",
        "    def compute_forward(self, batch, stage):\n",
        "        \"\"\"Computation pipeline based on a discrete audio features(Encodec)\n",
        "        + command classifier (MLP). Data augmentation and environmental corruption\n",
        "        are applied to the input speech.\n",
        "        \"\"\"\n",
        "        batch = batch.to(self.device)\n",
        "        wavs, lens = batch.sig\n",
        "\n",
        "        # Add waveform augmentation if specified.\n",
        "        if stage == sb.Stage.TRAIN and hasattr(self.hparams, \"wav_augment\"):\n",
        "            wavs, lens = self.hparams.wav_augment(wavs, lens)\n",
        "\n",
        "        # Embeddings + classifier\n",
        "        wavs = wavs.unsqueeze(1)\n",
        "        tokens, embeddings = self.modules.feat_model(wavs)\n",
        "\n",
        "        embeddings = self.hparams.avg_pool(embeddings, lens)\n",
        "\n",
        "        embeddings = embeddings.view(embeddings.shape[0], -1)\n",
        "\n",
        "        outputs = self.modules.output_mlp(embeddings)\n",
        "        # Ecapa model uses softmax outside of its classifier\n",
        "        if \"softmax\" in self.modules.keys():\n",
        "            outputs = self.modules.softmax(outputs)\n",
        "\n",
        "        return outputs, lens\n",
        "\n",
        "    def compute_objectives(self, predictions, batch, stage):\n",
        "        \"\"\"Computes the loss using command-id as label.\"\"\"\n",
        "        predictions, lens = predictions\n",
        "        uttid = batch.id\n",
        "        command, _ = batch.command_encoded\n",
        "\n",
        "        # Concatenate labels (due to data augmentation)\n",
        "        if stage == sb.Stage.TRAIN and hasattr(self.hparams, \"wav_augment\"):\n",
        "            command = self.hparams.wav_augment.replicate_labels(command)\n",
        "\n",
        "        \"\"\"to meet the input form of nll loss\"\"\"\n",
        "        command = command.squeeze(1)\n",
        "\n",
        "        # compute the cost function\n",
        "        loss = self.hparams.compute_cost(predictions, command)\n",
        "        # loss = sb.nnet.losses.nll_loss(predictions, command, lens)\n",
        "\n",
        "        # if hasattr(self.hparams.lr_annealing, \"on_batch_end\"):\n",
        "        #     self.hparams.lr_annealing.on_batch_end(self.optimizer)\n",
        "\n",
        "        if stage != sb.Stage.TRAIN:\n",
        "            self.error_metrics.append(uttid, predictions, command)\n",
        "\n",
        "        return loss\n",
        "\n",
        "    def on_stage_start(self, stage, epoch=None):\n",
        "        \"\"\"Gets called at the beginning of an epoch.\"\"\"\n",
        "        if stage != sb.Stage.TRAIN:\n",
        "            self.error_metrics = self.hparams.error_stats()\n",
        "\n",
        "    def on_stage_end(self, stage, stage_loss, epoch=None):\n",
        "        \"\"\"Gets called at the end of an epoch.\"\"\"\n",
        "        # Compute/store important stats\n",
        "        stage_stats = {\"loss\": stage_loss}\n",
        "        if stage == sb.Stage.TRAIN:\n",
        "            self.train_stats = stage_stats\n",
        "        else:\n",
        "            stage_stats[\"ErrorRate\"] = self.error_metrics.summarize(\"average\")\n",
        "\n",
        "        # Perform end-of-iteration things, like annealing, logging, etc.\n",
        "        if stage == sb.Stage.VALID:\n",
        "            old_lr, new_lr = self.hparams.lr_annealing(epoch)\n",
        "            sb.nnet.schedulers.update_learning_rate(self.optimizer, new_lr)\n",
        "\n",
        "            self.hparams.train_logger.log_stats(\n",
        "                stats_meta={\"epoch\": epoch, \"lr\": old_lr},\n",
        "                train_stats=self.train_stats,\n",
        "                valid_stats=stage_stats,\n",
        "            )\n",
        "            self.checkpointer.save_and_keep_only(\n",
        "                meta={\"ErrorRate\": stage_stats[\"ErrorRate\"]},\n",
        "                min_keys=[\"ErrorRate\"],\n",
        "            )\n",
        "\n",
        "        # We also write statistics about test data to stdout and to the logfile.\n",
        "        if stage == sb.Stage.TEST:\n",
        "            self.hparams.train_logger.log_stats(\n",
        "                {\"Epoch loaded\": self.hparams.epoch_counter.current},\n",
        "                test_stats=stage_stats,\n",
        "            )\n",
        "\n",
        "\n",
        "def dataio_prep(hparams):\n",
        "    \"Creates the datasets and their data processing pipelines.\"\n",
        "\n",
        "    data_folder = hparams[\"data_folder\"]\n",
        "\n",
        "    # 1. Declarations:\n",
        "    train_data = sb.dataio.dataset.DynamicItemDataset.from_csv(\n",
        "        csv_path=hparams[\"train_annotation\"],\n",
        "        replacements={\"data_root\": data_folder},\n",
        "    )\n",
        "\n",
        "    valid_data = sb.dataio.dataset.DynamicItemDataset.from_csv(\n",
        "        csv_path=hparams[\"valid_annotation\"],\n",
        "        replacements={\"data_root\": data_folder},\n",
        "    )\n",
        "\n",
        "    test_data = sb.dataio.dataset.DynamicItemDataset.from_csv(\n",
        "        csv_path=hparams[\"test_annotation\"],\n",
        "        replacements={\"data_root\": data_folder},\n",
        "    )\n",
        "\n",
        "    datasets = [train_data, valid_data, test_data]\n",
        "    label_encoder = sb.dataio.encoder.CategoricalEncoder()\n",
        "\n",
        "    # 2. Define audio pipeline:\n",
        "    @sb.utils.data_pipeline.takes(\"wav\", \"start\", \"stop\", \"duration\")\n",
        "    @sb.utils.data_pipeline.provides(\"sig\")\n",
        "    def audio_pipeline(wav, start, stop, duration):\n",
        "        start = int(start)\n",
        "        stop = int(stop)\n",
        "        num_frames = stop - start\n",
        "        sig, fs = torchaudio.load(\n",
        "            wav, num_frames=num_frames, frame_offset=start\n",
        "        )\n",
        "        sig = sig.transpose(0, 1).squeeze(1)\n",
        "        return sig\n",
        "\n",
        "    sb.dataio.dataset.add_dynamic_item(datasets, audio_pipeline)\n",
        "\n",
        "    # 3. Define text pipeline:\n",
        "    @sb.utils.data_pipeline.takes(\"command\")\n",
        "    @sb.utils.data_pipeline.provides(\"command\", \"command_encoded\")\n",
        "    def label_pipeline(command):\n",
        "        yield command\n",
        "        command_encoded = label_encoder.encode_sequence_torch([command])\n",
        "        yield command_encoded\n",
        "\n",
        "    sb.dataio.dataset.add_dynamic_item(datasets, label_pipeline)\n",
        "\n",
        "    # 3. Fit encoder:\n",
        "    # Load or compute the label encoder (with multi-GPU DDP support)\n",
        "    lab_enc_file = os.path.join(hparams[\"save_folder\"], \"label_encoder.txt\")\n",
        "    label_encoder.load_or_create(\n",
        "        path=lab_enc_file,\n",
        "        from_didatasets=[train_data],\n",
        "        output_key=\"command\",\n",
        "    )\n",
        "\n",
        "    # 4. Set output:\n",
        "    sb.dataio.dataset.set_output_keys(\n",
        "        datasets, [\"id\", \"sig\", \"command_encoded\"]\n",
        "    )\n",
        "\n",
        "    return train_data, valid_data, test_data, label_encoder\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # This flag enables the inbuilt cudnn auto-tuner\n",
        "    torch.backends.cudnn.benchmark = True\n",
        "\n",
        "    # CLI:\n",
        "    hparams_file, run_opts, overrides = sb.parse_arguments(sys.argv[1:])\n",
        "\n",
        "    # Initialize ddp (useful only for multi-GPU DDP training)\n",
        "    sb.utils.distributed.ddp_init_group(run_opts)\n",
        "\n",
        "    # Load hyperparameters file with command-line overrides\n",
        "    with open(hparams_file) as fin:\n",
        "        hparams = load_hyperpyyaml(fin, overrides)\n",
        "\n",
        "    # Create experiment directory\n",
        "    sb.core.create_experiment_directory(\n",
        "        experiment_directory=hparams[\"output_folder\"],\n",
        "        hyperparams_to_save=hparams_file,\n",
        "        overrides=overrides,\n",
        "    )\n",
        "\n",
        "    # Dataset prep (parsing GSC and annotation into csv files)\n",
        "    from prepare_GSC import prepare_GSC\n",
        "\n",
        "    # Known words for V2 12 and V2 35 sets\n",
        "    if hparams[\"number_of_commands\"] == 12:\n",
        "        words_wanted = [\n",
        "            \"yes\",\n",
        "            \"no\",\n",
        "            \"up\",\n",
        "            \"down\",\n",
        "            \"left\",\n",
        "            \"right\",\n",
        "            \"on\",\n",
        "            \"off\",\n",
        "            \"stop\",\n",
        "            \"go\",\n",
        "        ]\n",
        "    elif hparams[\"number_of_commands\"] == 35:\n",
        "        words_wanted = [\n",
        "            \"yes\",\n",
        "            \"no\",\n",
        "            \"up\",\n",
        "            \"down\",\n",
        "            \"left\",\n",
        "            \"right\",\n",
        "            \"on\",\n",
        "            \"off\",\n",
        "            \"stop\",\n",
        "            \"go\",\n",
        "            \"zero\",\n",
        "            \"one\",\n",
        "            \"two\",\n",
        "            \"three\",\n",
        "            \"four\",\n",
        "            \"five\",\n",
        "            \"six\",\n",
        "            \"seven\",\n",
        "            \"eight\",\n",
        "            \"nine\",\n",
        "            \"bed\",\n",
        "            \"bird\",\n",
        "            \"cat\",\n",
        "            \"dog\",\n",
        "            \"happy\",\n",
        "            \"house\",\n",
        "            \"marvin\",\n",
        "            \"sheila\",\n",
        "            \"tree\",\n",
        "            \"wow\",\n",
        "            \"backward\",\n",
        "            \"forward\",\n",
        "            \"follow\",\n",
        "            \"learn\",\n",
        "            \"visual\",\n",
        "        ]\n",
        "    else:\n",
        "        raise ValueError(\"number_of_commands must be 12 or 35\")\n",
        "\n",
        "    # Data preparation\n",
        "    run_on_main(\n",
        "        prepare_GSC,\n",
        "        kwargs={\n",
        "            \"data_folder\": hparams[\"data_folder\"],\n",
        "            \"save_folder\": hparams[\"save_folder\"],\n",
        "            \"validation_percentage\": hparams[\"validation_percentage\"],\n",
        "            \"testing_percentage\": hparams[\"testing_percentage\"],\n",
        "            \"percentage_unknown\": hparams[\"percentage_unknown\"],\n",
        "            \"percentage_silence\": hparams[\"percentage_silence\"],\n",
        "            \"words_wanted\": words_wanted,\n",
        "            \"skip_prep\": hparams[\"skip_prep\"],\n",
        "        },\n",
        "    )\n",
        "    sb.utils.distributed.run_on_main(hparams[\"prepare_noise_data\"])\n",
        "    sb.utils.distributed.run_on_main(hparams[\"prepare_rir_data\"])\n",
        "\n",
        "    # Dataset IO prep: creating Dataset objects and proper encodings for phones\n",
        "    train_data, valid_data, test_data, label_encoder = dataio_prep(hparams)\n",
        "\n",
        "    # Brain class initialization\n",
        "    speaker_brain = SpeakerBrain(\n",
        "        modules=hparams[\"modules\"],\n",
        "        opt_class=hparams[\"opt_class\"],\n",
        "        hparams=hparams,\n",
        "        run_opts=run_opts,\n",
        "        checkpointer=hparams[\"checkpointer\"],\n",
        "    )\n",
        "\n",
        "    # with torch.autograd.detect_anomaly():\n",
        "    # Training\n",
        "    speaker_brain.fit(\n",
        "        speaker_brain.hparams.epoch_counter,\n",
        "        train_data,\n",
        "        valid_data,\n",
        "        train_loader_kwargs=hparams[\"dataloader_options\"],\n",
        "        valid_loader_kwargs=hparams[\"dataloader_options\"],\n",
        "    )\n",
        "\n",
        "    # Load the best checkpoint for evaluation\n",
        "    test_stats = speaker_brain.evaluate(\n",
        "        test_set=test_data,\n",
        "        min_key=\"ErrorRate\",\n",
        "        test_loader_kwargs=hparams[\"dataloader_options\"],\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D4QVG43dT6hG"
      },
      "source": [
        "Remove previously generated results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fCnpoi5mT7TP"
      },
      "outputs": [],
      "source": [
        "# !rm -rf ./Keyword_Spotting/results/encodec_mlp/1986"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0hDxXe12Tw5w"
      },
      "source": [
        "Run Encodac with MLP"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uFTDQD40T1LZ"
      },
      "outputs": [],
      "source": [
        "!python train_discrete_mlp.py encodec_mlp.yaml --data_folder=/content/Google_Speech_Command_Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "After running it, we get result like:\n",
        "```\n",
        "epoch: 1, lr: 1.00e-03 - train loss: 2.09 - valid loss: 1.85, valid ErrorRate: 6.29e-01\n",
        "epoch: 2, lr: 7.75e-04 - train loss: 1.97 - valid loss: 1.77, valid ErrorRate: 5.96e-01\n",
        "epoch: 3, lr: 5.50e-04 - train loss: 1.93 - valid loss: 1.74, valid ErrorRate: 5.93e-01\n",
        "epoch: 4, lr: 3.25e-04 - train loss: 1.91 - valid loss: 1.72, valid ErrorRate: 5.64e-01\n",
        "epoch: 5, lr: 1.00e-04 - train loss: 1.89 - valid loss: 1.71, valid ErrorRate: 5.58e-01\n",
        "Epoch loaded: 5 - test loss: 1.75, test ErrorRate: 5.60e-01\n",
        "```"
      ],
      "metadata": {
        "id": "fVSaTgh-SKaP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "YAML file for Discrete Audio Feature using Encodec and a RNN on top."
      ],
      "metadata": {
        "id": "M0MQjpVjSgK2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%file encodec_rnn.yaml\n",
        "\n",
        "# Basic parameters\n",
        "seed: 1986\n",
        "__set_seed: !apply:torch.manual_seed [!ref <seed>]\n",
        "\n",
        "# Use 12 for V2 12 task and 35 for V2 35 task\n",
        "number_of_commands: 12\n",
        "output_folder: !ref /content/Keyword_Spotting/results/encodec_rnn/<seed>\n",
        "save_folder: !ref <output_folder>/save\n",
        "train_log: !ref <output_folder>/train_log.txt\n",
        "\n",
        "# Data for augmentation\n",
        "NOISE_DATASET_URL: https://www.dropbox.com/scl/fi/a09pj97s5ifan81dqhi4n/noises.zip?rlkey=j8b0n9kdjdr32o1f06t0cw5b7&dl=1\n",
        "RIR_DATASET_URL: https://www.dropbox.com/scl/fi/linhy77c36mu10965a836/RIRs.zip?rlkey=pg9cu8vrpn2u173vhiqyu743u&dl=1\n",
        "\n",
        "# Data files\n",
        "data_folder: !PLACEHOLDER  # e.g. /path/to/GSC\n",
        "data_folder_noise: !ref <data_folder>/noise # The noisy sequences for data augmentation will automatically be downloaded here.\n",
        "data_folder_rir: !ref <data_folder>/rir # The impulse responses used for data augmentation will automatically be downloaded here.\n",
        "train_annotation: !ref <save_folder>/train.csv\n",
        "valid_annotation: !ref <save_folder>/valid.csv\n",
        "test_annotation: !ref <save_folder>/test.csv\n",
        "noise_annotation: !ref <save_folder>/noise.csv\n",
        "rir_annotation: !ref <save_folder>/rir.csv\n",
        "\n",
        "# Percentage of files used for validation and test\n",
        "validation_percentage: 5\n",
        "testing_percentage: 5\n",
        "\n",
        "# Percentage of unknown and silence examples\n",
        "# (relative to total of known word samples) to include\n",
        "percentage_unknown: 10 # Set this to 0 for the V2 35 task\n",
        "percentage_silence: 10 # Set this to 0 for the V2 35 task\n",
        "\n",
        "skip_prep: False\n",
        "ckpt_interval_minutes: 15 # save checkpoint every N min\n",
        "\n",
        "####################### Training Parameters ####################################\n",
        "number_of_epochs: 5\n",
        "batch_size: 16\n",
        "lr: 0.0001\n",
        "lr_final: 0.00001\n",
        "\n",
        "sample_rate: 16000\n",
        "shuffle: True\n",
        "\n",
        "\n",
        "# Feature parameters\n",
        "n_mels: 24\n",
        "left_frames: 0\n",
        "right_frames: 0\n",
        "deltas: False\n",
        "\n",
        "# Number of classes (i.e. different commands)\n",
        "out_n_neurons: !ref <number_of_commands>  #includes core commands & auxiliary words\n",
        "\n",
        "num_workers: 2\n",
        "dataloader_options:\n",
        "    batch_size: !ref <batch_size>\n",
        "    shuffle: !ref <shuffle>\n",
        "    num_workers: !ref <num_workers>\n",
        "\n",
        "# Functions\n",
        "compute_features: !new:speechbrain.lobes.features.Fbank\n",
        "    n_mels: !ref <n_mels>\n",
        "    left_frames: !ref <left_frames>\n",
        "    right_frames: !ref <right_frames>\n",
        "    deltas: !ref <deltas>\n",
        "\n",
        "embedding_model: !new:speechbrain.lobes.models.Xvector.Xvector\n",
        "    in_channels: !ref <n_mels>\n",
        "    activation: !name:torch.nn.LeakyReLU\n",
        "    tdnn_blocks: 5\n",
        "    tdnn_channels: [512, 512, 512, 512, 1500]\n",
        "    tdnn_kernel_sizes: [5, 3, 3, 1, 1]\n",
        "    tdnn_dilations: [1, 2, 3, 1, 1]\n",
        "    lin_neurons: 512\n",
        "\n",
        "freeze_feat: False\n",
        "feat_model_hub: facebook/encodec_24khz\n",
        "feat_model_folder: !ref <save_folder>/ssl_checkpoint\n",
        "encoder_dim: 256\n",
        "\n",
        "# Encodec\n",
        "feat_model: !new:speechbrain.lobes.models.huggingface_transformers.encodec.Encodec\n",
        "    source: !ref <feat_model_hub>\n",
        "    save_path: !ref <feat_model_folder>\n",
        "    sample_rate: !ref <sample_rate>\n",
        "    flat_embeddings : True\n",
        "    freeze: !ref <freeze_feat>\n",
        "    renorm_embeddings : False\n",
        "\n",
        "avg_pool: !new:speechbrain.nnet.pooling.StatisticsPooling\n",
        "    return_std: False\n",
        "\n",
        "output_mlp: !new:speechbrain.nnet.RNN.RNN\n",
        "    input_size: 256\n",
        "    hidden_size: 40\n",
        "    bidirectional: True\n",
        "    dropout: 0.15\n",
        "    num_layers: 4\n",
        "\n",
        "classifier: !new:speechbrain.lobes.models.Xvector.Classifier\n",
        "    input_shape: [null, null, 512]\n",
        "    activation: !name:torch.nn.LeakyReLU\n",
        "    lin_blocks: 1\n",
        "    lin_neurons: 512\n",
        "    out_neurons: !ref <out_n_neurons>\n",
        "\n",
        "softmax: !new:speechbrain.nnet.activations.Softmax\n",
        "    apply_log: True\n",
        "\n",
        "epoch_counter: !new:speechbrain.utils.epoch_loop.EpochCounter\n",
        "    limit: !ref <number_of_epochs>\n",
        "\n",
        "\n",
        "# Download and prepare the dataset of noisy sequences for augmentation\n",
        "prepare_noise_data: !name:speechbrain.augment.preparation.prepare_dataset_from_URL\n",
        "    URL: !ref <NOISE_DATASET_URL>\n",
        "    dest_folder: !ref <data_folder_noise>\n",
        "    ext: wav\n",
        "    csv_file: !ref <noise_annotation>\n",
        "\n",
        "# Add noise to input signal\n",
        "snr_low: 0  # Min SNR for noise augmentation\n",
        "snr_high: 15  # Max SNR for noise augmentation\n",
        "\n",
        "add_noise: !new:speechbrain.augment.time_domain.AddNoise\n",
        "    csv_file: !ref <noise_annotation>\n",
        "    snr_low: !ref <snr_low>\n",
        "    snr_high: !ref <snr_high>\n",
        "    noise_sample_rate: !ref <sample_rate>\n",
        "    clean_sample_rate: !ref <sample_rate>\n",
        "    num_workers: !ref <num_workers>\n",
        "\n",
        "\n",
        "# Download and prepare the dataset of room impulse responses for augmentation\n",
        "prepare_rir_data: !name:speechbrain.augment.preparation.prepare_dataset_from_URL\n",
        "    URL: !ref <RIR_DATASET_URL>\n",
        "    dest_folder: !ref <data_folder_rir>\n",
        "    ext: wav\n",
        "    csv_file: !ref <rir_annotation>\n",
        "\n",
        "# Add reverberation to input signal\n",
        "add_reverb: !new:speechbrain.augment.time_domain.AddReverb\n",
        "    csv_file: !ref <rir_annotation>\n",
        "    reverb_sample_rate: !ref <sample_rate>\n",
        "    clean_sample_rate: !ref <sample_rate>\n",
        "    num_workers: !ref <num_workers>\n",
        "\n",
        "# Frequency drop: randomly drops a number of frequency bands to zero.\n",
        "drop_freq_low: 0  # Min frequency band dropout probability\n",
        "drop_freq_high: 1  # Max frequency band dropout probability\n",
        "drop_freq_count_low: 1  # Min number of frequency bands to drop\n",
        "drop_freq_count_high: 3  # Max number of frequency bands to drop\n",
        "drop_freq_width: 0.05  # Width of frequency bands to drop\n",
        "\n",
        "drop_freq: !new:speechbrain.augment.time_domain.DropFreq\n",
        "    drop_freq_low: !ref <drop_freq_low>\n",
        "    drop_freq_high: !ref <drop_freq_high>\n",
        "    drop_freq_count_low: !ref <drop_freq_count_low>\n",
        "    drop_freq_count_high: !ref <drop_freq_count_high>\n",
        "    drop_freq_width: !ref <drop_freq_width>\n",
        "\n",
        "# Time drop: randomly drops a number of temporal chunks.\n",
        "drop_chunk_count_low: 1  # Min number of audio chunks to drop\n",
        "drop_chunk_count_high: 5  # Max number of audio chunks to drop\n",
        "drop_chunk_length_low: 1000  # Min length of audio chunks to drop\n",
        "drop_chunk_length_high: 2000  # Max length of audio chunks to drop\n",
        "\n",
        "drop_chunk: !new:speechbrain.augment.time_domain.DropChunk\n",
        "    drop_length_low: !ref <drop_chunk_length_low>\n",
        "    drop_length_high: !ref <drop_chunk_length_high>\n",
        "    drop_count_low: !ref <drop_chunk_count_low>\n",
        "    drop_count_high: !ref <drop_chunk_count_high>\n",
        "\n",
        "# Augmenter: Combines previously defined augmentations to perform data augmentation\n",
        "wav_augment: !new:speechbrain.augment.augmenter.Augmenter\n",
        "    parallel_augment: True\n",
        "    concat_original: True\n",
        "    repeat_augment: 1\n",
        "    shuffle_augmentations: False\n",
        "    min_augmentations: 4\n",
        "    max_augmentations: 4\n",
        "    augment_prob: 1.0\n",
        "    augmentations: [\n",
        "        !ref <add_noise>,\n",
        "        !ref <add_reverb>,\n",
        "        !ref <drop_freq>,\n",
        "        !ref <drop_chunk>]\n",
        "\n",
        "mean_var_norm: !new:speechbrain.processing.features.InputNormalization\n",
        "    norm_type: sentence\n",
        "    std_norm: False\n",
        "\n",
        "modules:\n",
        "    compute_features: !ref <compute_features>\n",
        "    embedding_model: !ref <embedding_model>\n",
        "    classifier: !ref <classifier>\n",
        "    softmax: !ref <softmax>\n",
        "    mean_var_norm: !ref <mean_var_norm>\n",
        "    feat_model: !ref <feat_model>\n",
        "    output_mlp: !ref <output_mlp>\n",
        "\n",
        "# Cost + optimization\n",
        "compute_cost: !name:speechbrain.nnet.losses.nll_loss\n",
        "# compute_error: !name:speechbrain.nnet.losses.classification_error\n",
        "\n",
        "opt_class: !name:torch.optim.Adam\n",
        "    lr: !ref <lr>\n",
        "    weight_decay: 0.000002\n",
        "\n",
        "lr_annealing: !new:speechbrain.nnet.schedulers.LinearScheduler\n",
        "    initial_value: !ref <lr>\n",
        "    final_value: !ref <lr_final>\n",
        "    epoch_count: !ref <number_of_epochs>\n",
        "\n",
        "# Logging + checkpoints\n",
        "train_logger: !new:speechbrain.utils.train_logger.FileTrainLogger\n",
        "    save_file: !ref <train_log>\n",
        "\n",
        "error_stats: !name:speechbrain.utils.metric_stats.MetricStats\n",
        "    metric: !name:speechbrain.nnet.losses.classification_error\n",
        "        reduction: batch\n",
        "\n",
        "checkpointer: !new:speechbrain.utils.checkpoints.Checkpointer\n",
        "    checkpoints_dir: !ref <save_folder>\n",
        "    recoverables:\n",
        "        embedding_model: !ref <embedding_model>\n",
        "        classifier: !ref <classifier>\n",
        "        normalizer: !ref <mean_var_norm>\n",
        "        counter: !ref <epoch_counter>"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2j89J8XxQbLp",
        "outputId": "a8410c62-7a58-4ca3-cf8f-4d0d5de381b8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting encodac_rnn.yaml\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train file with discrete audio features and a RNN on top."
      ],
      "metadata": {
        "id": "52fqxByiSnjT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%file train_discrete_rnn.py\n",
        "\n",
        "#!/usr/bin/python3\n",
        "import os\n",
        "import sys\n",
        "import torch\n",
        "import torchaudio\n",
        "import speechbrain as sb\n",
        "from hyperpyyaml import load_hyperpyyaml\n",
        "\n",
        "import speechbrain.nnet.CNN\n",
        "from speechbrain.utils.distributed import run_on_main\n",
        "\n",
        "\n",
        "class SpeakerBrain(sb.core.Brain):\n",
        "    \"\"\"Class for GSC training\" \"\"\"\n",
        "\n",
        "    def compute_forward(self, batch, stage):\n",
        "        \"\"\"Computation pipeline based on a discrete audio features(Encodec)\n",
        "        + command classifier (RNN). Data augmentation and environmental corruption\n",
        "        are applied to the input speech.\n",
        "        \"\"\"\n",
        "        batch = batch.to(self.device)\n",
        "        wavs, lens = batch.sig\n",
        "\n",
        "        # Add waveform augmentation if specified.\n",
        "        if stage == sb.Stage.TRAIN and hasattr(self.hparams, \"wav_augment\"):\n",
        "            wavs, lens = self.hparams.wav_augment(wavs, lens)\n",
        "\n",
        "        # Embeddings + classifier\n",
        "        tokens, embeddings = self.modules.feat_model(wavs, lens)\n",
        "        embeddings = self.hparams.avg_pool(embeddings, lens)\n",
        "        embeddings = embeddings.view(embeddings.shape[0], -1)\n",
        "        outputs, hidden_state = self.modules.output_mlp(embeddings)\n",
        "        # Ecapa model uses softmax outside of its classifier\n",
        "        if \"softmax\" in self.modules.keys():\n",
        "            outputs = self.modules.softmax(outputs)\n",
        "\n",
        "        return outputs, lens\n",
        "\n",
        "    def compute_objectives(self, predictions, batch, stage):\n",
        "        \"\"\"Computes the loss using command-id as label.\"\"\"\n",
        "        predictions, lens = predictions\n",
        "        uttid = batch.id\n",
        "        command, _ = batch.command_encoded\n",
        "\n",
        "        # Concatenate labels (due to data augmentation)\n",
        "        if stage == sb.Stage.TRAIN and hasattr(self.hparams, \"wav_augment\"):\n",
        "            command = self.hparams.wav_augment.replicate_labels(command)\n",
        "\n",
        "        \"\"\"to meet the input form of nll loss\"\"\"\n",
        "        command = command.squeeze(1)\n",
        "\n",
        "        # compute the cost function\n",
        "        loss = self.hparams.compute_cost(predictions, command)\n",
        "        # loss = sb.nnet.losses.nll_loss(predictions, command, lens)\n",
        "\n",
        "        # if hasattr(self.hparams.lr_annealing, \"on_batch_end\"):\n",
        "        #     self.hparams.lr_annealing.on_batch_end(self.optimizer)\n",
        "\n",
        "        if stage != sb.Stage.TRAIN:\n",
        "            self.error_metrics.append(uttid, predictions, command)\n",
        "\n",
        "        return loss\n",
        "\n",
        "    def on_stage_start(self, stage, epoch=None):\n",
        "        \"\"\"Gets called at the beginning of an epoch.\"\"\"\n",
        "        if stage != sb.Stage.TRAIN:\n",
        "            self.error_metrics = self.hparams.error_stats()\n",
        "\n",
        "    def on_stage_end(self, stage, stage_loss, epoch=None):\n",
        "        \"\"\"Gets called at the end of an epoch.\"\"\"\n",
        "        # Compute/store important stats\n",
        "        stage_stats = {\"loss\": stage_loss}\n",
        "        if stage == sb.Stage.TRAIN:\n",
        "            self.train_stats = stage_stats\n",
        "        else:\n",
        "            stage_stats[\"ErrorRate\"] = self.error_metrics.summarize(\"average\")\n",
        "\n",
        "        # Perform end-of-iteration things, like annealing, logging, etc.\n",
        "        if stage == sb.Stage.VALID:\n",
        "            old_lr, new_lr = self.hparams.lr_annealing(epoch)\n",
        "            sb.nnet.schedulers.update_learning_rate(self.optimizer, new_lr)\n",
        "\n",
        "            self.hparams.train_logger.log_stats(\n",
        "                stats_meta={\"epoch\": epoch, \"lr\": old_lr},\n",
        "                train_stats=self.train_stats,\n",
        "                valid_stats=stage_stats,\n",
        "            )\n",
        "            self.checkpointer.save_and_keep_only(\n",
        "                meta={\"ErrorRate\": stage_stats[\"ErrorRate\"]},\n",
        "                min_keys=[\"ErrorRate\"],\n",
        "            )\n",
        "\n",
        "        # We also write statistics about test data to stdout and to the logfile.\n",
        "        if stage == sb.Stage.TEST:\n",
        "            self.hparams.train_logger.log_stats(\n",
        "                {\"Epoch loaded\": self.hparams.epoch_counter.current},\n",
        "                test_stats=stage_stats,\n",
        "            )\n",
        "\n",
        "\n",
        "def dataio_prep(hparams):\n",
        "    \"Creates the datasets and their data processing pipelines.\"\n",
        "\n",
        "    data_folder = hparams[\"data_folder\"]\n",
        "\n",
        "    # 1. Declarations:\n",
        "    train_data = sb.dataio.dataset.DynamicItemDataset.from_csv(\n",
        "        csv_path=hparams[\"train_annotation\"],\n",
        "        replacements={\"data_root\": data_folder},\n",
        "    )\n",
        "\n",
        "    valid_data = sb.dataio.dataset.DynamicItemDataset.from_csv(\n",
        "        csv_path=hparams[\"valid_annotation\"],\n",
        "        replacements={\"data_root\": data_folder},\n",
        "    )\n",
        "\n",
        "    test_data = sb.dataio.dataset.DynamicItemDataset.from_csv(\n",
        "        csv_path=hparams[\"test_annotation\"],\n",
        "        replacements={\"data_root\": data_folder},\n",
        "    )\n",
        "\n",
        "    datasets = [train_data, valid_data, test_data]\n",
        "    label_encoder = sb.dataio.encoder.CategoricalEncoder()\n",
        "\n",
        "    # 2. Define audio pipeline:\n",
        "    @sb.utils.data_pipeline.takes(\"wav\", \"start\", \"stop\", \"duration\")\n",
        "    @sb.utils.data_pipeline.provides(\"sig\")\n",
        "    def audio_pipeline(wav, start, stop, duration):\n",
        "        start = int(start)\n",
        "        stop = int(stop)\n",
        "        num_frames = stop - start\n",
        "        sig, fs = torchaudio.load(\n",
        "            wav, num_frames=num_frames, frame_offset=start\n",
        "        )\n",
        "        sig = sig.transpose(0, 1).squeeze(1)\n",
        "        return sig\n",
        "\n",
        "    sb.dataio.dataset.add_dynamic_item(datasets, audio_pipeline)\n",
        "\n",
        "    # 3. Define text pipeline:\n",
        "    @sb.utils.data_pipeline.takes(\"command\")\n",
        "    @sb.utils.data_pipeline.provides(\"command\", \"command_encoded\")\n",
        "    def label_pipeline(command):\n",
        "        yield command\n",
        "        command_encoded = label_encoder.encode_sequence_torch([command])\n",
        "        yield command_encoded\n",
        "\n",
        "    sb.dataio.dataset.add_dynamic_item(datasets, label_pipeline)\n",
        "\n",
        "    # 3. Fit encoder:\n",
        "    # Load or compute the label encoder (with multi-GPU DDP support)\n",
        "    lab_enc_file = os.path.join(hparams[\"save_folder\"], \"label_encoder.txt\")\n",
        "    label_encoder.load_or_create(\n",
        "        path=lab_enc_file,\n",
        "        from_didatasets=[train_data],\n",
        "        output_key=\"command\",\n",
        "    )\n",
        "\n",
        "    # 4. Set output:\n",
        "    sb.dataio.dataset.set_output_keys(\n",
        "        datasets, [\"id\", \"sig\", \"command_encoded\"]\n",
        "    )\n",
        "\n",
        "    return train_data, valid_data, test_data, label_encoder\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # This flag enables the inbuilt cudnn auto-tuner\n",
        "    torch.backends.cudnn.benchmark = True\n",
        "\n",
        "    # CLI:\n",
        "    hparams_file, run_opts, overrides = sb.parse_arguments(sys.argv[1:])\n",
        "\n",
        "    # Initialize ddp (useful only for multi-GPU DDP training)\n",
        "    sb.utils.distributed.ddp_init_group(run_opts)\n",
        "\n",
        "    # Load hyperparameters file with command-line overrides\n",
        "    with open(hparams_file) as fin:\n",
        "        hparams = load_hyperpyyaml(fin, overrides)\n",
        "\n",
        "    # Create experiment directory\n",
        "    sb.core.create_experiment_directory(\n",
        "        experiment_directory=hparams[\"output_folder\"],\n",
        "        hyperparams_to_save=hparams_file,\n",
        "        overrides=overrides,\n",
        "    )\n",
        "\n",
        "    # Dataset prep (parsing GSC and annotation into csv files)\n",
        "    from prepare_GSC import prepare_GSC\n",
        "\n",
        "    # Known words for V2 12 and V2 35 sets\n",
        "    if hparams[\"number_of_commands\"] == 12:\n",
        "        words_wanted = [\n",
        "            \"yes\",\n",
        "            \"no\",\n",
        "            \"up\",\n",
        "            \"down\",\n",
        "            \"left\",\n",
        "            \"right\",\n",
        "            \"on\",\n",
        "            \"off\",\n",
        "            \"stop\",\n",
        "            \"go\",\n",
        "        ]\n",
        "    elif hparams[\"number_of_commands\"] == 35:\n",
        "        words_wanted = [\n",
        "            \"yes\",\n",
        "            \"no\",\n",
        "            \"up\",\n",
        "            \"down\",\n",
        "            \"left\",\n",
        "            \"right\",\n",
        "            \"on\",\n",
        "            \"off\",\n",
        "            \"stop\",\n",
        "            \"go\",\n",
        "            \"zero\",\n",
        "            \"one\",\n",
        "            \"two\",\n",
        "            \"three\",\n",
        "            \"four\",\n",
        "            \"five\",\n",
        "            \"six\",\n",
        "            \"seven\",\n",
        "            \"eight\",\n",
        "            \"nine\",\n",
        "            \"bed\",\n",
        "            \"bird\",\n",
        "            \"cat\",\n",
        "            \"dog\",\n",
        "            \"happy\",\n",
        "            \"house\",\n",
        "            \"marvin\",\n",
        "            \"sheila\",\n",
        "            \"tree\",\n",
        "            \"wow\",\n",
        "            \"backward\",\n",
        "            \"forward\",\n",
        "            \"follow\",\n",
        "            \"learn\",\n",
        "            \"visual\",\n",
        "        ]\n",
        "    else:\n",
        "        raise ValueError(\"number_of_commands must be 12 or 35\")\n",
        "\n",
        "    # Data preparation\n",
        "    run_on_main(\n",
        "        prepare_GSC,\n",
        "        kwargs={\n",
        "            \"data_folder\": hparams[\"data_folder\"],\n",
        "            \"save_folder\": hparams[\"save_folder\"],\n",
        "            \"validation_percentage\": hparams[\"validation_percentage\"],\n",
        "            \"testing_percentage\": hparams[\"testing_percentage\"],\n",
        "            \"percentage_unknown\": hparams[\"percentage_unknown\"],\n",
        "            \"percentage_silence\": hparams[\"percentage_silence\"],\n",
        "            \"words_wanted\": words_wanted,\n",
        "            \"skip_prep\": hparams[\"skip_prep\"],\n",
        "        },\n",
        "    )\n",
        "    sb.utils.distributed.run_on_main(hparams[\"prepare_noise_data\"])\n",
        "    sb.utils.distributed.run_on_main(hparams[\"prepare_rir_data\"])\n",
        "\n",
        "    # Dataset IO prep: creating Dataset objects and proper encodings for phones\n",
        "    train_data, valid_data, test_data, label_encoder = dataio_prep(hparams)\n",
        "\n",
        "    # Brain class initialization\n",
        "    speaker_brain = SpeakerBrain(\n",
        "        modules=hparams[\"modules\"],\n",
        "        opt_class=hparams[\"opt_class\"],\n",
        "        hparams=hparams,\n",
        "        run_opts=run_opts,\n",
        "        checkpointer=hparams[\"checkpointer\"],\n",
        "    )\n",
        "\n",
        "    # with torch.autograd.detect_anomaly():\n",
        "    # Training\n",
        "    speaker_brain.fit(\n",
        "        speaker_brain.hparams.epoch_counter,\n",
        "        train_data,\n",
        "        valid_data,\n",
        "        train_loader_kwargs=hparams[\"dataloader_options\"],\n",
        "        valid_loader_kwargs=hparams[\"dataloader_options\"],\n",
        "    )\n",
        "\n",
        "    # Load the best checkpoint for evaluation\n",
        "    test_stats = speaker_brain.evaluate(\n",
        "        test_set=test_data,\n",
        "        min_key=\"ErrorRate\",\n",
        "        test_loader_kwargs=hparams[\"dataloader_options\"],\n",
        "    )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ra3qVa9-krRR",
        "outputId": "9e5b5bdc-759d-4c1f-fcc6-ee128a456d6f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing train_discrete_rnn.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !rm -rf ./Keyword_Spotting/results/encodec_rnn/1986"
      ],
      "metadata": {
        "id": "CFw9AWImZ992"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Run Encodec with RNN"
      ],
      "metadata": {
        "id": "rwsWOXeyG9pO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python train_discrete_rnn.py encodec_rnn.yaml --data_folder=/content/Google_Speech_Command_Dataset"
      ],
      "metadata": {
        "id": "wJOt9TQEZ-ft"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "After running it, we get result like:\n",
        "```\n",
        "epoch: 1, lr: 1.00e-04 - train loss: 2.83 - valid loss: 2.38, valid ErrorRate: 7.97e-01\n",
        "epoch: 2, lr: 7.75e-05 - train loss: 2.32 - valid loss: 2.17, valid ErrorRate: 7.34e-01\n",
        "epoch: 3, lr: 5.50e-05 - train loss: 2.19 - valid loss: 2.06, valid ErrorRate: 7.04e-01\n",
        "epoch: 4, lr: 3.25e-05 - train loss: 2.13 - valid loss: 2.01, valid ErrorRate: 6.93e-01\n",
        "epoch: 5, lr: 1.00e-05 - train loss: 2.10 - valid loss: 1.99, valid ErrorRate: 6.81e-01\n",
        "Epoch loaded: 5 - test loss: 2.02, test ErrorRate: 6.85e-01\n",
        "```"
      ],
      "metadata": {
        "id": "YeWoFUsaSSuI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "YAML file for Discrete Audio Feature using Encodec and a Transformer on top."
      ],
      "metadata": {
        "id": "UeyWMMsbMqxV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%file encodec_transformer.yaml\n",
        "# Basic parameters\n",
        "seed: 1986\n",
        "__set_seed: !apply:torch.manual_seed [!ref <seed>]\n",
        "\n",
        "# Use 12 for V2 12 task and 35 for V2 35 task\n",
        "number_of_commands: 12\n",
        "output_folder: !ref /content/Keyword_Spotting/results/encodec_transformer/<seed>\n",
        "save_folder: !ref <output_folder>/save\n",
        "train_log: !ref <output_folder>/train_log.txt\n",
        "\n",
        "# Data for augmentation\n",
        "NOISE_DATASET_URL: https://www.dropbox.com/scl/fi/a09pj97s5ifan81dqhi4n/noises.zip?rlkey=j8b0n9kdjdr32o1f06t0cw5b7&dl=1\n",
        "RIR_DATASET_URL: https://www.dropbox.com/scl/fi/linhy77c36mu10965a836/RIRs.zip?rlkey=pg9cu8vrpn2u173vhiqyu743u&dl=1\n",
        "\n",
        "# Data files\n",
        "data_folder: !PLACEHOLDER  # e.g. /path/to/GSC\n",
        "data_folder_noise: !ref <data_folder>/noise # The noisy sequences for data augmentation will automatically be downloaded here.\n",
        "data_folder_rir: !ref <data_folder>/rir # The impulse responses used for data augmentation will automatically be downloaded here.\n",
        "train_annotation: !ref <save_folder>/train.csv\n",
        "valid_annotation: !ref <save_folder>/valid.csv\n",
        "test_annotation: !ref <save_folder>/test.csv\n",
        "noise_annotation: !ref <save_folder>/noise.csv\n",
        "rir_annotation: !ref <save_folder>/rir.csv\n",
        "\n",
        "# Percentage of files used for validation and test\n",
        "validation_percentage: 5\n",
        "testing_percentage: 5\n",
        "\n",
        "# Percentage of unknown and silence examples\n",
        "# (relative to total of known word samples) to include\n",
        "percentage_unknown: 10 # Set this to 0 for the V2 35 task\n",
        "percentage_silence: 10 # Set this to 0 for the V2 35 task\n",
        "\n",
        "skip_prep: False\n",
        "ckpt_interval_minutes: 15 # save checkpoint every N min\n",
        "\n",
        "####################### Training Parameters ####################################\n",
        "number_of_epochs: 5\n",
        "batch_size: 16\n",
        "lr: 0.001\n",
        "lr_final: 0.0001\n",
        "\n",
        "sample_rate: 16000\n",
        "shuffle: True\n",
        "\n",
        "\n",
        "# Feature parameters\n",
        "n_mels: 24\n",
        "left_frames: 0\n",
        "right_frames: 0\n",
        "deltas: False\n",
        "\n",
        "# Number of classes (i.e. different commands)\n",
        "out_n_neurons: !ref <number_of_commands>  #includes core commands & auxiliary words\n",
        "\n",
        "num_workers: 2\n",
        "dataloader_options:\n",
        "    batch_size: !ref <batch_size>\n",
        "    shuffle: !ref <shuffle>\n",
        "    num_workers: !ref <num_workers>\n",
        "\n",
        "# Functions\n",
        "compute_features: !new:speechbrain.lobes.features.Fbank\n",
        "    n_mels: !ref <n_mels>\n",
        "    left_frames: !ref <left_frames>\n",
        "    right_frames: !ref <right_frames>\n",
        "    deltas: !ref <deltas>\n",
        "\n",
        "embedding_model: !new:speechbrain.lobes.models.Xvector.Xvector\n",
        "    in_channels: !ref <n_mels>\n",
        "    activation: !name:torch.nn.LeakyReLU\n",
        "    tdnn_blocks: 5\n",
        "    tdnn_channels: [512, 512, 512, 512, 1500]\n",
        "    tdnn_kernel_sizes: [5, 3, 3, 1, 1]\n",
        "    tdnn_dilations: [1, 2, 3, 1, 1]\n",
        "    lin_neurons: 512\n",
        "\n",
        "freeze_feat: False\n",
        "feat_model_hub: facebook/encodec_24khz\n",
        "feat_model_folder: !ref <save_folder>/ssl_checkpoint\n",
        "encoder_dim: 256\n",
        "\n",
        "# Encodec\n",
        "feat_model: !new:speechbrain.lobes.models.huggingface_transformers.encodec.Encodec\n",
        "    source: !ref <feat_model_hub>\n",
        "    save_path: !ref <feat_model_folder>\n",
        "    sample_rate: !ref <sample_rate>\n",
        "    flat_embeddings : True\n",
        "    freeze: !ref <freeze_feat>\n",
        "    renorm_embeddings : False\n",
        "\n",
        "avg_pool: !new:speechbrain.nnet.pooling.StatisticsPooling\n",
        "    return_std: False\n",
        "\n",
        "output_mlp: !new:speechbrain.lobes.models.transformer.Transformer.TransformerEncoder\n",
        "   d_model: 256\n",
        "   num_layers: 12\n",
        "   nhead: 8\n",
        "   d_ffn: 3072\n",
        "   dropout: 0.15\n",
        "   layerdrop_prob: 0.0\n",
        "   normalize_before: True\n",
        "   activation: !name:torch.nn.LeakyReLU\n",
        "\n",
        "classifier: !new:speechbrain.lobes.models.Xvector.Classifier\n",
        "    input_shape: [null, null, 512]\n",
        "    activation: !name:torch.nn.LeakyReLU\n",
        "    lin_blocks: 1\n",
        "    lin_neurons: 512\n",
        "    out_neurons: !ref <out_n_neurons>\n",
        "\n",
        "softmax: !new:speechbrain.nnet.activations.Softmax\n",
        "    apply_log: True\n",
        "\n",
        "epoch_counter: !new:speechbrain.utils.epoch_loop.EpochCounter\n",
        "    limit: !ref <number_of_epochs>\n",
        "\n",
        "\n",
        "# Download and prepare the dataset of noisy sequences for augmentation\n",
        "prepare_noise_data: !name:speechbrain.augment.preparation.prepare_dataset_from_URL\n",
        "    URL: !ref <NOISE_DATASET_URL>\n",
        "    dest_folder: !ref <data_folder_noise>\n",
        "    ext: wav\n",
        "    csv_file: !ref <noise_annotation>\n",
        "\n",
        "# Add noise to input signal\n",
        "snr_low: 0  # Min SNR for noise augmentation\n",
        "snr_high: 15  # Max SNR for noise augmentation\n",
        "\n",
        "add_noise: !new:speechbrain.augment.time_domain.AddNoise\n",
        "    csv_file: !ref <noise_annotation>\n",
        "    snr_low: !ref <snr_low>\n",
        "    snr_high: !ref <snr_high>\n",
        "    noise_sample_rate: !ref <sample_rate>\n",
        "    clean_sample_rate: !ref <sample_rate>\n",
        "    num_workers: !ref <num_workers>\n",
        "\n",
        "\n",
        "# Download and prepare the dataset of room impulse responses for augmentation\n",
        "prepare_rir_data: !name:speechbrain.augment.preparation.prepare_dataset_from_URL\n",
        "    URL: !ref <RIR_DATASET_URL>\n",
        "    dest_folder: !ref <data_folder_rir>\n",
        "    ext: wav\n",
        "    csv_file: !ref <rir_annotation>\n",
        "\n",
        "# Add reverberation to input signal\n",
        "add_reverb: !new:speechbrain.augment.time_domain.AddReverb\n",
        "    csv_file: !ref <rir_annotation>\n",
        "    reverb_sample_rate: !ref <sample_rate>\n",
        "    clean_sample_rate: !ref <sample_rate>\n",
        "    num_workers: !ref <num_workers>\n",
        "\n",
        "# Frequency drop: randomly drops a number of frequency bands to zero.\n",
        "drop_freq_low: 0  # Min frequency band dropout probability\n",
        "drop_freq_high: 1  # Max frequency band dropout probability\n",
        "drop_freq_count_low: 1  # Min number of frequency bands to drop\n",
        "drop_freq_count_high: 3  # Max number of frequency bands to drop\n",
        "drop_freq_width: 0.05  # Width of frequency bands to drop\n",
        "\n",
        "drop_freq: !new:speechbrain.augment.time_domain.DropFreq\n",
        "    drop_freq_low: !ref <drop_freq_low>\n",
        "    drop_freq_high: !ref <drop_freq_high>\n",
        "    drop_freq_count_low: !ref <drop_freq_count_low>\n",
        "    drop_freq_count_high: !ref <drop_freq_count_high>\n",
        "    drop_freq_width: !ref <drop_freq_width>\n",
        "\n",
        "# Time drop: randomly drops a number of temporal chunks.\n",
        "drop_chunk_count_low: 1  # Min number of audio chunks to drop\n",
        "drop_chunk_count_high: 5  # Max number of audio chunks to drop\n",
        "drop_chunk_length_low: 1000  # Min length of audio chunks to drop\n",
        "drop_chunk_length_high: 2000  # Max length of audio chunks to drop\n",
        "\n",
        "drop_chunk: !new:speechbrain.augment.time_domain.DropChunk\n",
        "    drop_length_low: !ref <drop_chunk_length_low>\n",
        "    drop_length_high: !ref <drop_chunk_length_high>\n",
        "    drop_count_low: !ref <drop_chunk_count_low>\n",
        "    drop_count_high: !ref <drop_chunk_count_high>\n",
        "\n",
        "# Augmenter: Combines previously defined augmentations to perform data augmentation\n",
        "wav_augment: !new:speechbrain.augment.augmenter.Augmenter\n",
        "    parallel_augment: True\n",
        "    concat_original: True\n",
        "    repeat_augment: 1\n",
        "    shuffle_augmentations: False\n",
        "    min_augmentations: 4\n",
        "    max_augmentations: 4\n",
        "    augment_prob: 1.0\n",
        "    augmentations: [\n",
        "        !ref <add_noise>,\n",
        "        !ref <add_reverb>,\n",
        "        !ref <drop_freq>,\n",
        "        !ref <drop_chunk>]\n",
        "\n",
        "mean_var_norm: !new:speechbrain.processing.features.InputNormalization\n",
        "    norm_type: sentence\n",
        "    std_norm: False\n",
        "\n",
        "modules:\n",
        "    compute_features: !ref <compute_features>\n",
        "    embedding_model: !ref <embedding_model>\n",
        "    classifier: !ref <classifier>\n",
        "    softmax: !ref <softmax>\n",
        "    mean_var_norm: !ref <mean_var_norm>\n",
        "    feat_model: !ref <feat_model>\n",
        "    output_mlp: !ref <output_mlp>\n",
        "\n",
        "# Cost + optimization\n",
        "compute_cost: !name:speechbrain.nnet.losses.nll_loss\n",
        "# compute_error: !name:speechbrain.nnet.losses.classification_error\n",
        "\n",
        "opt_class: !name:torch.optim.Adam\n",
        "    lr: !ref <lr>\n",
        "    weight_decay: 0.000002\n",
        "\n",
        "lr_annealing: !new:speechbrain.nnet.schedulers.LinearScheduler\n",
        "    initial_value: !ref <lr>\n",
        "    final_value: !ref <lr_final>\n",
        "    epoch_count: !ref <number_of_epochs>\n",
        "\n",
        "# Logging + checkpoints\n",
        "train_logger: !new:speechbrain.utils.train_logger.FileTrainLogger\n",
        "    save_file: !ref <train_log>\n",
        "\n",
        "error_stats: !name:speechbrain.utils.metric_stats.MetricStats\n",
        "    metric: !name:speechbrain.nnet.losses.classification_error\n",
        "        reduction: batch\n",
        "\n",
        "checkpointer: !new:speechbrain.utils.checkpoints.Checkpointer\n",
        "    checkpoints_dir: !ref <save_folder>\n",
        "    recoverables:\n",
        "        embedding_model: !ref <embedding_model>\n",
        "        classifier: !ref <classifier>\n",
        "        normalizer: !ref <mean_var_norm>\n",
        "        counter: !ref <epoch_counter>"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K48AlIyeMqEP",
        "outputId": "a987680e-3783-4cef-c0fc-f17b52a850db"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing encodec_transformer.yaml\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train file for Encodec with a transformer on top."
      ],
      "metadata": {
        "id": "IvizFf2oM_GQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%file train_discrete_transformer.py\n",
        "\n",
        "#!/usr/bin/python3\n",
        "import os\n",
        "import sys\n",
        "import torch\n",
        "import torchaudio\n",
        "import speechbrain as sb\n",
        "from hyperpyyaml import load_hyperpyyaml\n",
        "\n",
        "import speechbrain.nnet.CNN\n",
        "from speechbrain.utils.distributed import run_on_main\n",
        "\n",
        "\n",
        "class SpeakerBrain(sb.core.Brain):\n",
        "    \"\"\"Class for GSC training\" \"\"\"\n",
        "\n",
        "    def compute_forward(self, batch, stage):\n",
        "        \"\"\"Computation pipeline based on a discrete audio features(Encodec)\n",
        "        + command classifier (Transformer). Data augmentation and environmental corruption\n",
        "        are applied to the input speech.\n",
        "        \"\"\"\n",
        "        batch = batch.to(self.device)\n",
        "        wavs, lens = batch.sig\n",
        "\n",
        "        # Add waveform augmentation if specified.\n",
        "        if stage == sb.Stage.TRAIN and hasattr(self.hparams, \"wav_augment\"):\n",
        "            wavs, lens = self.hparams.wav_augment(wavs, lens)\n",
        "\n",
        "        # Embeddings + classifier\n",
        "        tokens, embeddings = self.modules.feat_model(wavs, lens)\n",
        "        embeddings = self.hparams.avg_pool(embeddings, lens)\n",
        "        outputs, hidden_state = self.modules.output_mlp(embeddings)\n",
        "        outputs = outputs.view(outputs.shape[0], -1)\n",
        "        # Ecapa model uses softmax outside of its classifier\n",
        "        if \"softmax\" in self.modules.keys():\n",
        "            outputs = self.modules.softmax(outputs)\n",
        "\n",
        "        return outputs, lens\n",
        "\n",
        "    def compute_objectives(self, predictions, batch, stage):\n",
        "        \"\"\"Computes the loss using command-id as label.\"\"\"\n",
        "        predictions, lens = predictions\n",
        "        uttid = batch.id\n",
        "        command, _ = batch.command_encoded\n",
        "\n",
        "        # Concatenate labels (due to data augmentation)\n",
        "        if stage == sb.Stage.TRAIN and hasattr(self.hparams, \"wav_augment\"):\n",
        "            command = self.hparams.wav_augment.replicate_labels(command)\n",
        "\n",
        "        \"\"\"to meet the input form of nll loss\"\"\"\n",
        "        command = command.squeeze(1)\n",
        "\n",
        "        # compute the cost function\n",
        "        loss = self.hparams.compute_cost(predictions, command)\n",
        "        # loss = sb.nnet.losses.nll_loss(predictions, command, lens)\n",
        "\n",
        "        # if hasattr(self.hparams.lr_annealing, \"on_batch_end\"):\n",
        "        #     self.hparams.lr_annealing.on_batch_end(self.optimizer)\n",
        "\n",
        "        if stage != sb.Stage.TRAIN:\n",
        "            self.error_metrics.append(uttid, predictions, command)\n",
        "\n",
        "        return loss\n",
        "\n",
        "    def on_stage_start(self, stage, epoch=None):\n",
        "        \"\"\"Gets called at the beginning of an epoch.\"\"\"\n",
        "        if stage != sb.Stage.TRAIN:\n",
        "            self.error_metrics = self.hparams.error_stats()\n",
        "\n",
        "    def on_stage_end(self, stage, stage_loss, epoch=None):\n",
        "        \"\"\"Gets called at the end of an epoch.\"\"\"\n",
        "        # Compute/store important stats\n",
        "        stage_stats = {\"loss\": stage_loss}\n",
        "        if stage == sb.Stage.TRAIN:\n",
        "            self.train_stats = stage_stats\n",
        "        else:\n",
        "            stage_stats[\"ErrorRate\"] = self.error_metrics.summarize(\"average\")\n",
        "\n",
        "        # Perform end-of-iteration things, like annealing, logging, etc.\n",
        "        if stage == sb.Stage.VALID:\n",
        "            old_lr, new_lr = self.hparams.lr_annealing(epoch)\n",
        "            sb.nnet.schedulers.update_learning_rate(self.optimizer, new_lr)\n",
        "\n",
        "            self.hparams.train_logger.log_stats(\n",
        "                stats_meta={\"epoch\": epoch, \"lr\": old_lr},\n",
        "                train_stats=self.train_stats,\n",
        "                valid_stats=stage_stats,\n",
        "            )\n",
        "            self.checkpointer.save_and_keep_only(\n",
        "                meta={\"ErrorRate\": stage_stats[\"ErrorRate\"]},\n",
        "                min_keys=[\"ErrorRate\"],\n",
        "            )\n",
        "\n",
        "        # We also write statistics about test data to stdout and to the logfile.\n",
        "        if stage == sb.Stage.TEST:\n",
        "            self.hparams.train_logger.log_stats(\n",
        "                {\"Epoch loaded\": self.hparams.epoch_counter.current},\n",
        "                test_stats=stage_stats,\n",
        "            )\n",
        "\n",
        "\n",
        "def dataio_prep(hparams):\n",
        "    \"Creates the datasets and their data processing pipelines.\"\n",
        "\n",
        "    data_folder = hparams[\"data_folder\"]\n",
        "\n",
        "    # 1. Declarations:\n",
        "    train_data = sb.dataio.dataset.DynamicItemDataset.from_csv(\n",
        "        csv_path=hparams[\"train_annotation\"],\n",
        "        replacements={\"data_root\": data_folder},\n",
        "    )\n",
        "\n",
        "    valid_data = sb.dataio.dataset.DynamicItemDataset.from_csv(\n",
        "        csv_path=hparams[\"valid_annotation\"],\n",
        "        replacements={\"data_root\": data_folder},\n",
        "    )\n",
        "\n",
        "    test_data = sb.dataio.dataset.DynamicItemDataset.from_csv(\n",
        "        csv_path=hparams[\"test_annotation\"],\n",
        "        replacements={\"data_root\": data_folder},\n",
        "    )\n",
        "\n",
        "    datasets = [train_data, valid_data, test_data]\n",
        "    label_encoder = sb.dataio.encoder.CategoricalEncoder()\n",
        "\n",
        "    # 2. Define audio pipeline:\n",
        "    @sb.utils.data_pipeline.takes(\"wav\", \"start\", \"stop\", \"duration\")\n",
        "    @sb.utils.data_pipeline.provides(\"sig\")\n",
        "    def audio_pipeline(wav, start, stop, duration):\n",
        "        start = int(start)\n",
        "        stop = int(stop)\n",
        "        num_frames = stop - start\n",
        "        sig, fs = torchaudio.load(\n",
        "            wav, num_frames=num_frames, frame_offset=start\n",
        "        )\n",
        "        sig = sig.transpose(0, 1).squeeze(1)\n",
        "        return sig\n",
        "\n",
        "    sb.dataio.dataset.add_dynamic_item(datasets, audio_pipeline)\n",
        "\n",
        "    # 3. Define text pipeline:\n",
        "    @sb.utils.data_pipeline.takes(\"command\")\n",
        "    @sb.utils.data_pipeline.provides(\"command\", \"command_encoded\")\n",
        "    def label_pipeline(command):\n",
        "        yield command\n",
        "        command_encoded = label_encoder.encode_sequence_torch([command])\n",
        "        yield command_encoded\n",
        "\n",
        "    sb.dataio.dataset.add_dynamic_item(datasets, label_pipeline)\n",
        "\n",
        "    # 3. Fit encoder:\n",
        "    # Load or compute the label encoder (with multi-GPU DDP support)\n",
        "    lab_enc_file = os.path.join(hparams[\"save_folder\"], \"label_encoder.txt\")\n",
        "    label_encoder.load_or_create(\n",
        "        path=lab_enc_file,\n",
        "        from_didatasets=[train_data],\n",
        "        output_key=\"command\",\n",
        "    )\n",
        "\n",
        "    # 4. Set output:\n",
        "    sb.dataio.dataset.set_output_keys(\n",
        "        datasets, [\"id\", \"sig\", \"command_encoded\"]\n",
        "    )\n",
        "\n",
        "    return train_data, valid_data, test_data, label_encoder\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # This flag enables the inbuilt cudnn auto-tuner\n",
        "    torch.backends.cudnn.benchmark = True\n",
        "\n",
        "    # CLI:\n",
        "    hparams_file, run_opts, overrides = sb.parse_arguments(sys.argv[1:])\n",
        "\n",
        "    # Initialize ddp (useful only for multi-GPU DDP training)\n",
        "    sb.utils.distributed.ddp_init_group(run_opts)\n",
        "\n",
        "    # Load hyperparameters file with command-line overrides\n",
        "    with open(hparams_file) as fin:\n",
        "        hparams = load_hyperpyyaml(fin, overrides)\n",
        "\n",
        "    # Create experiment directory\n",
        "    sb.core.create_experiment_directory(\n",
        "        experiment_directory=hparams[\"output_folder\"],\n",
        "        hyperparams_to_save=hparams_file,\n",
        "        overrides=overrides,\n",
        "    )\n",
        "\n",
        "    # Dataset prep (parsing GSC and annotation into csv files)\n",
        "    from prepare_GSC import prepare_GSC\n",
        "\n",
        "    # Known words for V2 12 and V2 35 sets\n",
        "    if hparams[\"number_of_commands\"] == 12:\n",
        "        words_wanted = [\n",
        "            \"yes\",\n",
        "            \"no\",\n",
        "            \"up\",\n",
        "            \"down\",\n",
        "            \"left\",\n",
        "            \"right\",\n",
        "            \"on\",\n",
        "            \"off\",\n",
        "            \"stop\",\n",
        "            \"go\",\n",
        "        ]\n",
        "    elif hparams[\"number_of_commands\"] == 35:\n",
        "        words_wanted = [\n",
        "            \"yes\",\n",
        "            \"no\",\n",
        "            \"up\",\n",
        "            \"down\",\n",
        "            \"left\",\n",
        "            \"right\",\n",
        "            \"on\",\n",
        "            \"off\",\n",
        "            \"stop\",\n",
        "            \"go\",\n",
        "            \"zero\",\n",
        "            \"one\",\n",
        "            \"two\",\n",
        "            \"three\",\n",
        "            \"four\",\n",
        "            \"five\",\n",
        "            \"six\",\n",
        "            \"seven\",\n",
        "            \"eight\",\n",
        "            \"nine\",\n",
        "            \"bed\",\n",
        "            \"bird\",\n",
        "            \"cat\",\n",
        "            \"dog\",\n",
        "            \"happy\",\n",
        "            \"house\",\n",
        "            \"marvin\",\n",
        "            \"sheila\",\n",
        "            \"tree\",\n",
        "            \"wow\",\n",
        "            \"backward\",\n",
        "            \"forward\",\n",
        "            \"follow\",\n",
        "            \"learn\",\n",
        "            \"visual\",\n",
        "        ]\n",
        "    else:\n",
        "        raise ValueError(\"number_of_commands must be 12 or 35\")\n",
        "\n",
        "    # Data preparation\n",
        "    run_on_main(\n",
        "        prepare_GSC,\n",
        "        kwargs={\n",
        "            \"data_folder\": hparams[\"data_folder\"],\n",
        "            \"save_folder\": hparams[\"save_folder\"],\n",
        "            \"validation_percentage\": hparams[\"validation_percentage\"],\n",
        "            \"testing_percentage\": hparams[\"testing_percentage\"],\n",
        "            \"percentage_unknown\": hparams[\"percentage_unknown\"],\n",
        "            \"percentage_silence\": hparams[\"percentage_silence\"],\n",
        "            \"words_wanted\": words_wanted,\n",
        "            \"skip_prep\": hparams[\"skip_prep\"],\n",
        "        },\n",
        "    )\n",
        "    sb.utils.distributed.run_on_main(hparams[\"prepare_noise_data\"])\n",
        "    sb.utils.distributed.run_on_main(hparams[\"prepare_rir_data\"])\n",
        "\n",
        "    # Dataset IO prep: creating Dataset objects and proper encodings for phones\n",
        "    train_data, valid_data, test_data, label_encoder = dataio_prep(hparams)\n",
        "\n",
        "    # Brain class initialization\n",
        "    speaker_brain = SpeakerBrain(\n",
        "        modules=hparams[\"modules\"],\n",
        "        opt_class=hparams[\"opt_class\"],\n",
        "        hparams=hparams,\n",
        "        run_opts=run_opts,\n",
        "        checkpointer=hparams[\"checkpointer\"],\n",
        "    )\n",
        "\n",
        "    # with torch.autograd.detect_anomaly():\n",
        "    # Training\n",
        "    speaker_brain.fit(\n",
        "        speaker_brain.hparams.epoch_counter,\n",
        "        train_data,\n",
        "        valid_data,\n",
        "        train_loader_kwargs=hparams[\"dataloader_options\"],\n",
        "        valid_loader_kwargs=hparams[\"dataloader_options\"],\n",
        "    )\n",
        "\n",
        "    # Load the best checkpoint for evaluation\n",
        "    test_stats = speaker_brain.evaluate(\n",
        "        test_set=test_data,\n",
        "        min_key=\"ErrorRate\",\n",
        "        test_loader_kwargs=hparams[\"dataloader_options\"],\n",
        "    )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NOz0rSUMNEpe",
        "outputId": "a2a96a34-dfd4-4c92-bf0f-70a50703df1e"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting train_discrete_transformer.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !rm -rf ./Keyword_Spotting/results/encodec_transformer/1986"
      ],
      "metadata": {
        "id": "NSJ6aVQlOZNB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Run Encodec with RNN"
      ],
      "metadata": {
        "id": "OW8bcoKUOYc5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python train_discrete_transformer.py encodec_transformer.yaml --data_folder=/content/Google_Speech_Command_Dataset"
      ],
      "metadata": {
        "id": "VLxszw-sOgig"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "After running it, we get result like:\n",
        "```\n",
        "epoch: 1, lr: 1.00e-03 - train loss: 2.20 - valid loss: 1.75, valid ErrorRate: 6.35e-01\n",
        "epoch: 2, lr: 7.75e-04 - train loss: 1.91 - valid loss: 1.65, valid ErrorRate: 6.04e-01\n",
        "epoch: 3, lr: 5.50e-04 - train loss: 1.82 - valid loss: 1.60, valid ErrorRate: 5.80e-01\n",
        "epoch: 4, lr: 3.25e-04 - train loss: 1.75 - valid loss: 1.51, valid ErrorRate: 5.42e-01\n",
        "epoch: 5, lr: 1.00e-04 - train loss: 1.71 - valid loss: 1.50, valid ErrorRate: 5.43e-01\n",
        "Epoch loaded: 4 - test loss: 1.51, test ErrorRate: 5.26e-01\n",
        "```"
      ],
      "metadata": {
        "id": "Betig86JOq_8"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1wXt9EqIknih"
      },
      "source": [
        "YAML file for Discrete Audio Feature using Dac and a MLP on top."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d50deeb1-3782-476a-adcb-bd3d3cc9001c",
        "id": "guKtQa3Pknio"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting dac_mlp.yaml\n"
          ]
        }
      ],
      "source": [
        "%%file dac_mlp.yaml\n",
        "\n",
        "# Basic parameters\n",
        "seed: 1986\n",
        "__set_seed: !apply:torch.manual_seed [!ref <seed>]\n",
        "\n",
        "# Use 12 for V2 12 task and 35 for V2 35 task\n",
        "number_of_commands: 12\n",
        "output_folder: !ref /content/Keyword_Spotting/results/dac_mlp/<seed>\n",
        "save_folder: !ref <output_folder>/save\n",
        "train_log: !ref <output_folder>/train_log.txt\n",
        "\n",
        "# Data for augmentation\n",
        "NOISE_DATASET_URL: https://www.dropbox.com/scl/fi/a09pj97s5ifan81dqhi4n/noises.zip?rlkey=j8b0n9kdjdr32o1f06t0cw5b7&dl=1\n",
        "RIR_DATASET_URL: https://www.dropbox.com/scl/fi/linhy77c36mu10965a836/RIRs.zip?rlkey=pg9cu8vrpn2u173vhiqyu743u&dl=1\n",
        "\n",
        "# Data files\n",
        "data_folder: !PLACEHOLDER  # e.g. /path/to/GSC\n",
        "data_folder_noise: !ref <data_folder>/noise # The noisy sequences for data augmentation will automatically be downloaded here.\n",
        "data_folder_rir: !ref <data_folder>/rir # The impulse responses used for data augmentation will automatically be downloaded here.\n",
        "train_annotation: !ref <save_folder>/train.csv\n",
        "valid_annotation: !ref <save_folder>/valid.csv\n",
        "test_annotation: !ref <save_folder>/test.csv\n",
        "noise_annotation: !ref <save_folder>/noise.csv\n",
        "rir_annotation: !ref <save_folder>/rir.csv\n",
        "\n",
        "# Percentage of files used for validation and test\n",
        "validation_percentage: 5\n",
        "testing_percentage: 5\n",
        "\n",
        "# Percentage of unknown and silence examples\n",
        "# (relative to total of known word samples) to include\n",
        "percentage_unknown: 10 # Set this to 0 for the V2 35 task\n",
        "percentage_silence: 10 # Set this to 0 for the V2 35 task\n",
        "\n",
        "skip_prep: False\n",
        "ckpt_interval_minutes: 15 # save checkpoint every N min\n",
        "\n",
        "####################### Training Parameters ####################################\n",
        "number_of_epochs: 5\n",
        "batch_size: 16\n",
        "lr: 0.0001\n",
        "lr_final: 0.00001\n",
        "\n",
        "sample_rate: 16000\n",
        "shuffle: True\n",
        "\n",
        "\n",
        "# Feature parameters\n",
        "n_mels: 24\n",
        "left_frames: 0\n",
        "right_frames: 0\n",
        "deltas: False\n",
        "\n",
        "# Number of classes (i.e. different commands)\n",
        "out_n_neurons: !ref <number_of_commands>  #includes core commands & auxiliary words\n",
        "\n",
        "num_workers: 2\n",
        "dataloader_options:\n",
        "    batch_size: !ref <batch_size>\n",
        "    shuffle: !ref <shuffle>\n",
        "    num_workers: !ref <num_workers>\n",
        "\n",
        "# Functions\n",
        "compute_features: !new:speechbrain.lobes.features.Fbank\n",
        "    n_mels: !ref <n_mels>\n",
        "    left_frames: !ref <left_frames>\n",
        "    right_frames: !ref <right_frames>\n",
        "    deltas: !ref <deltas>\n",
        "\n",
        "embedding_model: !new:speechbrain.lobes.models.Xvector.Xvector\n",
        "    in_channels: !ref <n_mels>\n",
        "    activation: !name:torch.nn.LeakyReLU\n",
        "    tdnn_blocks: 5\n",
        "    tdnn_channels: [512, 512, 512, 512, 1500]\n",
        "    tdnn_kernel_sizes: [5, 3, 3, 1, 1]\n",
        "    tdnn_dilations: [1, 2, 3, 1, 1]\n",
        "    lin_neurons: 512\n",
        "\n",
        "freeze_feat: False\n",
        "feat_model_hub: facebook/encodec_24khz\n",
        "feat_model_folder: !ref <save_folder>/ssl_checkpoint\n",
        "encoder_dim: 256\n",
        "model_type: 16khz\n",
        "\n",
        "# Dac\n",
        "feat_model: !new:speechbrain.lobes.models.discrete.dac.DAC\n",
        "    load_pretrained: True\n",
        "    model_type: !ref <model_type>\n",
        "\n",
        "avg_pool: !new:speechbrain.nnet.pooling.StatisticsPooling\n",
        "    return_std: False\n",
        "\n",
        "output_mlp: !new:speechbrain.nnet.linear.Linear\n",
        "    input_size: !ref <encoder_dim>\n",
        "    n_neurons: !ref <out_n_neurons>\n",
        "    bias: False\n",
        "\n",
        "classifier: !new:speechbrain.lobes.models.Xvector.Classifier\n",
        "    input_shape: [null, null, 512]\n",
        "    activation: !name:torch.nn.LeakyReLU\n",
        "    lin_blocks: 1\n",
        "    lin_neurons: 512\n",
        "    out_neurons: !ref <out_n_neurons>\n",
        "\n",
        "softmax: !new:speechbrain.nnet.activations.Softmax\n",
        "    apply_log: True\n",
        "\n",
        "epoch_counter: !new:speechbrain.utils.epoch_loop.EpochCounter\n",
        "    limit: !ref <number_of_epochs>\n",
        "\n",
        "\n",
        "# Download and prepare the dataset of noisy sequences for augmentation\n",
        "prepare_noise_data: !name:speechbrain.augment.preparation.prepare_dataset_from_URL\n",
        "    URL: !ref <NOISE_DATASET_URL>\n",
        "    dest_folder: !ref <data_folder_noise>\n",
        "    ext: wav\n",
        "    csv_file: !ref <noise_annotation>\n",
        "\n",
        "# Add noise to input signal\n",
        "snr_low: 0  # Min SNR for noise augmentation\n",
        "snr_high: 15  # Max SNR for noise augmentation\n",
        "\n",
        "add_noise: !new:speechbrain.augment.time_domain.AddNoise\n",
        "    csv_file: !ref <noise_annotation>\n",
        "    snr_low: !ref <snr_low>\n",
        "    snr_high: !ref <snr_high>\n",
        "    noise_sample_rate: !ref <sample_rate>\n",
        "    clean_sample_rate: !ref <sample_rate>\n",
        "    num_workers: !ref <num_workers>\n",
        "\n",
        "\n",
        "# Download and prepare the dataset of room impulse responses for augmentation\n",
        "prepare_rir_data: !name:speechbrain.augment.preparation.prepare_dataset_from_URL\n",
        "    URL: !ref <RIR_DATASET_URL>\n",
        "    dest_folder: !ref <data_folder_rir>\n",
        "    ext: wav\n",
        "    csv_file: !ref <rir_annotation>\n",
        "\n",
        "# Add reverberation to input signal\n",
        "add_reverb: !new:speechbrain.augment.time_domain.AddReverb\n",
        "    csv_file: !ref <rir_annotation>\n",
        "    reverb_sample_rate: !ref <sample_rate>\n",
        "    clean_sample_rate: !ref <sample_rate>\n",
        "    num_workers: !ref <num_workers>\n",
        "\n",
        "# Frequency drop: randomly drops a number of frequency bands to zero.\n",
        "drop_freq_low: 0  # Min frequency band dropout probability\n",
        "drop_freq_high: 1  # Max frequency band dropout probability\n",
        "drop_freq_count_low: 1  # Min number of frequency bands to drop\n",
        "drop_freq_count_high: 3  # Max number of frequency bands to drop\n",
        "drop_freq_width: 0.05  # Width of frequency bands to drop\n",
        "\n",
        "drop_freq: !new:speechbrain.augment.time_domain.DropFreq\n",
        "    drop_freq_low: !ref <drop_freq_low>\n",
        "    drop_freq_high: !ref <drop_freq_high>\n",
        "    drop_freq_count_low: !ref <drop_freq_count_low>\n",
        "    drop_freq_count_high: !ref <drop_freq_count_high>\n",
        "    drop_freq_width: !ref <drop_freq_width>\n",
        "\n",
        "# Time drop: randomly drops a number of temporal chunks.\n",
        "drop_chunk_count_low: 1  # Min number of audio chunks to drop\n",
        "drop_chunk_count_high: 5  # Max number of audio chunks to drop\n",
        "drop_chunk_length_low: 1000  # Min length of audio chunks to drop\n",
        "drop_chunk_length_high: 2000  # Max length of audio chunks to drop\n",
        "\n",
        "drop_chunk: !new:speechbrain.augment.time_domain.DropChunk\n",
        "    drop_length_low: !ref <drop_chunk_length_low>\n",
        "    drop_length_high: !ref <drop_chunk_length_high>\n",
        "    drop_count_low: !ref <drop_chunk_count_low>\n",
        "    drop_count_high: !ref <drop_chunk_count_high>\n",
        "\n",
        "# Augmenter: Combines previously defined augmentations to perform data augmentation\n",
        "wav_augment: !new:speechbrain.augment.augmenter.Augmenter\n",
        "    parallel_augment: True\n",
        "    concat_original: True\n",
        "    repeat_augment: 1\n",
        "    shuffle_augmentations: False\n",
        "    min_augmentations: 4\n",
        "    max_augmentations: 4\n",
        "    augment_prob: 1.0\n",
        "    augmentations: [\n",
        "        !ref <add_noise>,\n",
        "        !ref <add_reverb>,\n",
        "        !ref <drop_freq>,\n",
        "        !ref <drop_chunk>]\n",
        "\n",
        "mean_var_norm: !new:speechbrain.processing.features.InputNormalization\n",
        "    norm_type: sentence\n",
        "    std_norm: False\n",
        "\n",
        "modules:\n",
        "    compute_features: !ref <compute_features>\n",
        "    embedding_model: !ref <embedding_model>\n",
        "    classifier: !ref <classifier>\n",
        "    softmax: !ref <softmax>\n",
        "    mean_var_norm: !ref <mean_var_norm>\n",
        "    feat_model: !ref <feat_model>\n",
        "    output_mlp: !ref <output_mlp>\n",
        "\n",
        "# Cost + optimization\n",
        "compute_cost: !name:speechbrain.nnet.losses.nll_loss\n",
        "# compute_error: !name:speechbrain.nnet.losses.classification_error\n",
        "\n",
        "opt_class: !name:torch.optim.Adam\n",
        "    lr: !ref <lr>\n",
        "    weight_decay: 0.000002\n",
        "\n",
        "lr_annealing: !new:speechbrain.nnet.schedulers.LinearScheduler\n",
        "    initial_value: !ref <lr>\n",
        "    final_value: !ref <lr_final>\n",
        "    epoch_count: !ref <number_of_epochs>\n",
        "\n",
        "# Logging + checkpoints\n",
        "train_logger: !new:speechbrain.utils.train_logger.FileTrainLogger\n",
        "    save_file: !ref <train_log>\n",
        "\n",
        "error_stats: !name:speechbrain.utils.metric_stats.MetricStats\n",
        "    metric: !name:speechbrain.nnet.losses.classification_error\n",
        "        reduction: batch\n",
        "\n",
        "checkpointer: !new:speechbrain.utils.checkpoints.Checkpointer\n",
        "    checkpoints_dir: !ref <save_folder>\n",
        "    recoverables:\n",
        "        embedding_model: !ref <embedding_model>\n",
        "        classifier: !ref <classifier>\n",
        "        normalizer: !ref <mean_var_norm>\n",
        "        counter: !ref <epoch_counter>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GaOVeIS_knip"
      },
      "source": [
        "Remove previously generated results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cYsIkkBwknip"
      },
      "outputs": [],
      "source": [
        "# !rm -rf ./Keyword_Spotting/results/dac_mlp/1986"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kD728zTkknip"
      },
      "source": [
        "Run Encodac with MLP"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d1HKEjRuknip"
      },
      "outputs": [],
      "source": [
        "!python train_discrete_mlp.py dac_mlp.yaml --data_folder=/content/Google_Speech_Command_Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Findings are reported here:\n",
        "\n",
        "1. Could not consider DAC features the colab memory was not enoungh for the exceeded even for the lowest sized encoder of DAC (model_type: 16khz)\n",
        "2. Because of time limitation of GPU usage daily, could not run more than 2-2.5 hours daily. So, could not run for enough epoch.\n",
        "3. All the experiments were done with 12 commands.\n",
        "\n",
        "(With noise and reverbaration added)\n",
        "Results with Encodec\n",
        "1. With Encodec + MLP ->           error rate 56% (5 epochs due to GPU Time limitation)\n",
        "2. With Encodec + RNN ->           error rate 68.5% (5 epochs due to GPU Time limitation)\n",
        "3. With Encodec + Transformer ->   error rate 52.6% (5 epochs due to GPU Time limitation)\n",
        "\n",
        "Results with SSL Models\n",
        "1. Wav2Vec2 -> error rate 2.42% (2 epochs)\n",
        "2. HuBERT -> error rate 2.19% (2 epochs)\n",
        "3. WavLM -> error rate 1.63% (2 epochs)\n",
        "\n",
        "Results with FBANK + XVect (Speechbrain recipe) -> error rate 3.75% (5 epochs due to GPU Time limitation)\n",
        "\n",
        "References:\n",
        "1. Speechbrain recipe https://github.com/speechbrain/speechbrain/tree/develop/recipes/Google-speech-commands"
      ],
      "metadata": {
        "id": "s2DiR8TQknip"
      }
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}